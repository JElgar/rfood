% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the academic year, say 2013/14 say).
                 

\documentclass[ oneside,% the name of the author
                    author={James Elgar},
                % the degree programme: BSc, MEng, MSci or MSc.
                    degree={MEng},
                % the dissertation    title (which cannot be blank)
                     title={Bidirectional transformer between functional and \\ object-oriented programming in Rust},
                % the dissertation subtitle (which can    be blank)
                  subtitle={}]{dissertation}
                  
% Commands
\usepackage{hyperref}
\newcommand{\weixin}{Zhang et al }

\usepackage{listings}
\usepackage{xcolor}

\usepackage{mathtools}

% \usepackage{minted}
\usepackage[outputdir=../]{minted}
\newcommand{\rust}[1]{\mintinline{rust}{#1}}

\newcommand{\codefile}[2]{\inputminted[xleftmargin=20pt,linenos, breaklines]{#1}{#2}}
\newcommand{\codefileLN}[4]{\inputminted[xleftmargin=20pt,linenos, breaklines, firstline=#3, lastline=#4]{#1}{#2}}
\newcommand{\rustsnippet}[1]{\codefile{rust}{snippets/#1.rs}}
\newcommand{\rustfile}[1]{\codefile{rust}{../#1.rs}}
\newcommand{\rustexample}[1]{\rustfile{examples/src/#1}}
\newcommand{\rustoutput}[1]{\rustfile{outputs/src/#1}}

\newcommand{\rustfileLN}[3]{\codefileLN{rust}{../#1.rs}{#2}{#3}}
\newcommand{\rustexampleLN}[3]{\rustfileLN{examples/src/#1}{#2}{#3}}

\usepackage{xcolor} % to access the named colour LightGray
\definecolor{LightGray}{gray}{0.9}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{soul}

\usepackage{xspace}
\usepackage{listings}
\usepackage{xypic}
\usepackage{mathpartir}
\usepackage{stmaryrd}
\usepackage{longtable}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{enumitem}
\usepackage{mathtools}

\input{report/syntax}

\begin{document}

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter


%\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Abstract}

% TODO I think is should really be in the intro, focus more on what actually happens
There are benefits to using both object orient and functional programming. With modern programming languages adopting traditional functional feature, developer often have the choice between the two paradigms. However once a paradigm is selected switching can be a time consuming process. In \cite{food} \weixin proposes a framework for transforming between these decomposition styles. This paper explorers the generality of this approach through an implementation of the transformer in rust. 

The key contributions of this paper are:

\begin{quote}
\noindent
\begin{itemize}
    \item Created an implementation of the \cite{food} transformer in rust
    \item Extend the \cite{food} transformer to support a basic implementation of the rust type system
    \item Explored extensions to the transformer including generics and mutability
\end{itemize}
\end{quote}

% {\bf A compulsory section, of at most 300 words} 
% \vspace{1cm} 

% ---- Reference/Example ---- %
% \noindent
% This section should pr\'{e}cis the project context, aims and objectives,
% and main contributions (e.g., deliverables) and achievements; the same 
% section may be called an abstract elsewhere.  The goal is to ensure the 
% reader is clear about what the topic is, what you have done within this 
% topic, {\em and} what your view of the outcome is.

% The former aspects should be guided by your specification: essentially 
% this section is a (very) short version of what is typically the first 
% chapter. If your project is experimental in nature, this should include 
% a clear research hypothesis.  This will obviously differ significantly
% for each project, but an example might be as follows:

% \begin{quote}
% My research hypothesis is that a suitable genetic algorithm will yield
% more accurate results (when applied to the standard ACME data set) than 
% the algorithm proposed by Jones and Smith, while also executing in less
% time.
% \end{quote}

% \noindent
% The latter aspects should (ideally) be presented as a concise, factual 
% bullet point list.  Again the points will differ for each project, but 
% an might be as follows:

% \begin{quote}
% \noindent
% \begin{itemize}
% \item I spent $120$ hours collecting material on and learning about the 
%       Java garbage-collection sub-system. 
% \item I wrote a total of $5000$ lines of source code, comprising a Linux 
%       device driver for a robot (in C) and a GUI (in Java) that is 
%       used to control it.
% \item I designed a new algorithm for computing the non-linear mapping 
%       from A-space to B-space using a genetic algorithm, see page $17$.
% \item I implemented a version of the algorithm proposed by Jones and 
%       Smith in [6], see page $12$, corrected a mistake in it, and 
%       compared the results with several alternatives.
% \end{itemize}
% \end{quote}

% -----------------------------------------------------------------------------


\chapter*{Dedication and Acknowledgements}

{\bf A compulsory section}
\vspace{1cm} 

\noindent
It is common practice (although totally optional) to acknowledge any
third-party advice, contribution or influence you have found useful
during your work.  Examples include support from friends or family, 
the input of your Supervisor and/or Advisor, external organisations 
or persons who  have supplied resources of some kind (e.g., funding, 
advice or time), and so on.

% -----------------------------------------------------------------------------

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl



% -----------------------------------------------------------------------------

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures and tables.  These are all compulsory parts of the dissertation.

\tableofcontents
\listoffigures
\listoftables

% -----------------------------------------------------------------------------



\chapter*{Ethics Statement}

This project did not require ethical review, as determined by my supervisor, [fill in name].

% -----------------------------------------------------------------------------

% \noindent
% This section should present a detailed summary, in bullet point form, 
% of any third-party resources (e.g., hardware and software components) 
% used during the project.  Use of such resources is always perfectly 
% acceptable: the goal of this section is simply to be clear about how
% and where they are used, so that a clear assessment of your work can
% result.  The content can focus on the project topic itself (rather,
% for example, than including ``I used \mbox{\LaTeX} to prepare my 
% dissertation''); an example is as follows:

\chapter*{Supporting Technologies}

\noindent
To implement the transformer I used the following technologies:

\begin{quote}
\noindent
\begin{itemize}
\item I used the Rust programming language to implement the transformer. \url{https://www.rust-lang.org/}
\item I used the syn package to create and parse the AST. \url{https://docs.rs/syn/latest/syn/}
\item I used the quote package to covert the transformed AST back to rust code. \url{https://docs.rs/quote/latest/quote/}
\item I used rustfmt (a component of the Rust language) to format the transformed output code.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

{\bf An optional section}
\vspace{1cm} 

\noindent
% Any well written document will introduce notation and acronyms before
% their use, {\em even if} they are standard in some way: this ensures 
% any reader can understand the resulting self-contained content.  

% Said introduction can exist within the dissertation itself, wherever 
% that is appropriate.  For an acronym, this is typically achieved at 
% the first point of use via ``Advanced Encryption Standard (AES)'' or 
% similar, noting the capitalisation of relevant letters.  However, it 
% can be useful to include an additional, dedicated list at the start 
% of the dissertation; the advantage of doing so is that you cannot 
% mistakenly use an acronym before defining it.  A limited example is 
% as follows:

\begin{quote}
\noindent
\begin{tabular}{lcl}
AST                 &:     & Abstract Syntax Tree                                        \\
OOP                 &:     & Object Oriented Programming                                 \\
FP                  &:     & Functional Programming                                      \\
FOOD                &:     & Functional and Object Oriented Decomposition                \\
\end{tabular}
\end{quote}


% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter


\chapter{Introduction}
\label{chap:context}

% {\bf Unlike the frontmatter up to and including the Summary of Changes, which you should not deviate from, Chapters 1--5 represent a suggested outline only. This outline will only be appropriate for a specific type of project. You should talk with your supervisor about the best way to structure your own dissertation, but ultimately the choice is yours. However, almost every project will want to include the content discussed in these chapters in some way. For more advice on structuring your dissertation, see the unit handbook.}
% \vspace{1cm} 

% \noindent
% This chapter should introduce the project context and motivate each of the proposed aims and objectives.  Ideally, it is written at a fairly high-level, and easily understood by a reader who is technically competent but not an expert in the topic itself.

% In short, the goal is to answer three questions for the reader.  First, what is the project topic, or problem being investigated?  Second, why is the topic important, or rather why should the reader care about it?  For example, why there is a need for this project (e.g., lack of similar software or deficiency in existing software), who will benefit from the project and in what way (e.g., end-users, or software developers) what work does the project build on and why is the selected approach either important and/or interesting (e.g., fills a gap in literature, applies results from another field to a new problem).  Finally, what are the central challenges involved and why are they significant? 
 
% The chapter should conclude with a concise bullet point list that summarises the aims and objectives.  For example:

% \begin{quote}
% \noindent
% The high-level objective of this project is to reduce the performance 
% gap between hardware and software implementations of modular arithmetic.  
% More specifically, the concrete aims are:

% \begin{enumerate}
% \item Research and survey literature on public-key cryptography and
%       identify the state of the art in exponentiation algorithms.
% \item Improve the state of the art algorithm so that it can be used
%       in an effective and flexible way on constrained devices.
% \item Implement a framework for describing exponentiation algorithms
%       and populate it with suitable examples from the literature on 
%       an ARM7 platform.
% \item Use the framework to perform a study of algorithm performance
%       in terms of time and space, and show the proposed improvements
%       are worthwhile.
% \end{enumerate}
% \end{quote}

\noindent
With the rapid adoption of functional and object oriented features in modern programming languages, developers now have the choice of which paradigm to use in different situations. Both paradigms come with distinct advantages and disadvantages, and such the correct choice for a given feature can often be a tricky decision. In any case a "correct" decision may not be possible as both paradigms could have different values in the same feature. If a developer makes a choice they later regret, refactoring to another paradigm can be an extremely slow process.

Therefore, to truly harness the value of both paradigms, in \cite{food} \weixin explores the idea of a transformer, which allows developer to automatically transform between these two paradigms. This allows developers to instantly switch the style of there code depending on the task.

\weixin proposes a framework to transform between these two decomposition styles and create an implementation in Scala to demo its functionality.

The goal of this paper is to replicate this implementation in Rust. Rust was created by the Mozilla foundation with the intention of replacing C++ in their stack. It has a strong focus on performance and safety which it achieves partly through its borrow checker. With Rust being a low level language it exposes concepts to the developer which are not available in Scala, such as references and borrows. This requires extension to the existing transformer to allow it to transform with these concepts. 



% -----------------------------------------------------------------------------

% \noindent
% This chapter is intended to describe the background on which execution of the project depends. This may be a technical or a contextual background, or both. The goal is to provide a detailed explanation of the specific problem at hand, and existing work that is relevant (e.g., an existing algorithm that you use, alternative solutions proposed, supporting technologies).  

% Per the same advice in the handbook, note there is a subtly difference from this and a full-blown literature review (or survey).  The latter might try to capture and organise (e.g., categorise somehow) \emph{all} related work, potentially offering meta-analysis, whereas here the goal is simple to ensure the dissertation is self-contained.  Put another way, after reading this chapter a non-expert reader should have obtained enough background to understand what \emph{you} have done (by reading subsequent sections), then accurately assess your work against existing relevant related work.  You might view an additional goal as giving the reader confidence that you are able to absorb, understand and clearly communicate highly technical material and to situate your work within existing literature.

\chapter{Background}
\label{chap:technical}

In \cite{warburton}, Warburton discusses the on going evolution of modern languages focusing on their adoptions of functional and object oriented features. He states, object oriented and functional features were both introduced as improvements to more traditional procedural (imperative) languages in the 1980s. In these early days languages inherently had a combination of both paradigms. For example LISP, a functional language, had the CLOS (common LISP object system), and Smalltalk, an object oriented language, had lambda expression and operations equivalent to map, reduce and filter.

Between the 1990s and the 2000s this mulit-paradigm trend died down as Object-oriented programming became the dominant style with popular languages such as Java and C++.

However, since then the popularity of multi-paradigm features has risen again. Traditionally object oriented languages (including C++ and Java) are adding in functional features, whilst new languages such as Rust and Go are created with multi-paradigm features.

Warburton concludes with "The future is hybrid: pick the best features and ideas from both
functional and object-oriented approaches in order to solve the
problem at hand." \cite{warburton}. This statement is the motivation for this paper. Programmers are increasingly working within multi-paradigm projects and are expected to pick the "best" features from a paradigm depending on their task at hand. Selecting the "wrong" style or if requirements change, can be a costly as refactoring between styles is a time consuming process.

With this it is clear there is value in a tool which can automatically switch between features in these paradigms. The work of \weixin \cite{food}, demonstrates, that practical equivalence can be drawn between certain features and as such an automatic transformer, in some capacity, can be created. The goal of this paper, is to both replicate the functionality of that paper in a different language, but also to extend the transformer to support a wider range of input programs.

\section{Decomposition}

% "There are important design tradeoffs between functional and object-oriented decom-
% positions in terms of extensibility and expressiveness. As acknowledged by the notorious
% Expression Problem [18, 7, 23], these two decomposition styles are complementary in terms
% of extensibility."

Both object oriented and functional programming are approaches to decomposition, which is the process of breaking a complex problem down into less complex sub problems. The primary difference between these two paradigms, is how data abstraction is achieved.

\subsection{Object Oriented Decomposition}

Object oriented decomposition achieves data abstraction through the creation of interfaces. This is an "operation first" \cite{food} approach as these interfaces describe the operations that can be applied to the type but not the data within the types.
Throughout this paper these operations are referred to as "destructors" as they "tear down" an object into another type.
Throughout \cite{food} \weixin uses Pure OOP from \cite{cook} as the style of object oriented decomposition. In Pure OOP, these interfaces are types rather than classes.
In this style, classes are "object generators" \cite{food} meaning they create objects which satisfy a specified interface.

Although Rust's OOP features do not map directly to Pure OOP style, equivalence between the styles can be drawn. In Rust interfaces are defined as \rust{trait}s. Lines 3-6 of \autoref{fig:dec-ex-oop} demonstrate the definition of a trait called \rust{Shape}. \rust{Shape} defines two destructors, \rust{area} and \rust{perimeter}. 

In Rust the concept of classes are split into two components, \rust{struct} and \rust{impl}. A \rust{struct} defines a collection of attributes (the data), whilst the \rust{impl} block can implement a given trait on the struct (the operations). In Rust a \rust{struct} can implement multiple traits. \rust{struct}s are also treated a types them selves, meaning methods can be defined directly on them (using an \rust{impl} block with no \rust{for} trait). Despite this a \rust{struct} which implements a single \rust{trait} is equivalent to a Pure OOP class. 
Lines 8-19 demonstrte how a Pure OOP style class, which implements the \rust{Shape} interface, can be created in Rust. 
Lines 8-10 create the \rust{Circle} \rust{struct} with a single attribute \rust{radius}. Lines 11-19, then implement the \rust{Circle} \rust{trait}. In the implementation a definition for each of the required methods in included. 
Lines 21-32 demonstrate another implementation of a Pure OOP class. Therefore in this example the shape interface has two generators, \rust{Circle} and \rust{Square}. These generators can then be used to create an instance of a \rust{Shape} with the following syntax \rust{Circle{radius: 12.0}}.

% This section will describe the key components of this style and their equivalence in Rust. \autoref{fig:dec-ex-oop} will be used throughout as an example of a basic object oriented program in Rust. 

\subsubsection{Inheritance}

Inheritance is one of the central concepts in object oriented programming \cite{cook_palsberg_1989}. It allows developers to share logic between linked classes, preventing duplication. 
However Rust has very limited support for traditional inheritance. Instead Rust opts for composition, which enables structs to reference other objects (structs and traits).
Rust also has support for default implementations. These are definitions of destructors directly in the trait, meaning structs that then implement the method do not have to provide a definition. In this case if a definition is provided in an impl block, that definition is used other wise the default implementation is used.
An example is provided in \autoref{fig:inheritance-example-oop}. In this example the \rust{Shape} trait, defines two operations \rust{side_count} with no default implementation and \rust{internal_angle}, which provides a default implementation.
Two structs then implement this trait, \rust{Circle} and \rust{Square}. Both provide an implementation for the \rust{side_count}, method as required (as there is no default), whilst only \rust{Circle} provides an implementation for the \rust{internal_angle} method.
As a result, calling \rust{internal_angle} on a \rust{Circle} object uses the overridden definition (returning 0) whilst calling that method on \rust{Triangle} will use the default implementation.

\subsubsection{Self Reference}

When defining a destructors on a trait, an optional argument \rust{self} can be included as the first argument in the signature. 
This argument has type \rust{Self} and allows self-reference within the destructor implementation.
With in an \rust{impl} block for a \rust{struct}, the \rust{Self} type is equivalent to the struct, this therefore allows the method to access both attributes and operations on the object the method is called on.
However, in default implementations (on traits) \rust{Self} is equivalent to the trait. Therefore, the method will not be able to access the fields of \rust{self} and instead only be able to access the operations.  

In Rust if the self argument is not included in the method this is actually called an "associated function" as it no longer has access to an instance of the type.


\subsection{Functional Decomposition}

In contrast to object oriented decomposition, the first stage of data abstraction for functional decomposition is the creation of datatypes.
These data types specify the attributes of each variant, hence functional decomposition is described as "data first" \cite{food}. 
Similarly to the OOP style, \weixin uses a very restricted form of functional programming for defining the transformer.  
In this style consumers of datatypes are defined with pattern matches on the type.

In Rust \rust{enum}s are equivalent to datatypes. Lines 3-6 of \autoref{fig:dec-ex-fp} demonstrate the creation of an \rust{enum} with two variants \rust{Circle} and \rust{Square}. Note how the attributes of these variants (radius for circle and side for square) are defined directly in the datatype.

In Rust consumers are top level functions, which take a datatype as the first argument. They then contain a single match statement on the datatype, with an arm for each of its variants. In \autoref{fig:dec-ex-fp} both \rust{area} and \rust{perimeter} are consumers of the \rust{Shape} datatype. 
They both pattern match on the parameter of type shape, defining responses for each variant. Each arm of the match statement, such \rust{Shape::Square { side } => side * side}, contains a pattern which destruct the type of the parameter, giving access to the attributes of the provided \rust{Shape} variant.

\subsubsection{Wildcard}

In a similar vein to inheritance in OOP, match statements support the wildcard pattern, \rust{_ =>}. This will match any pattern that has not yet been matched by a match arm. This is demonstrated on line 11 in \autoref{fig:inheritance-example-fp}. Here the shape will match circles for the first arm of the match statement, but for any other shape variant will use the wildcard expression.
Similarly if all variants of the datatype have the same implementation, (i.e. the match statement only requires a single wild card match) no match statement is required. 

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustexample{shape/oop}
    \caption{Object Oriented Programming - Implementation of trait for structs}
    \label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustexample{shape/fp}
    \caption{Functional Programming - Datatype with two consumers}
    \label{fig:dec-ex-fp}
\end{subfigure}
\caption{Rust Decomposition Examples}
\label{fig:dec-ex}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustexample{shape2/oop}
    \caption{Object Oriented Programming - Default implementation}
    \label{fig:inheritance-example-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustexample{shape2/fp}
    \caption{Functional Programming - Wildcard pattern}
    \label{fig:inheritance-example-fp}
\end{subfigure}
\caption{Rust Decomposition Examples}
\label{fig:inheritance-example}
\end{figure}

\section{Decomposition without Regrets}
% Following on from this dudes work with rust

In "Decomposition without Regrets" \cite{food}, \weixin proposes a framework for a bidirectional transformer between these two decomposition styles. \weixin introduces a syntax called "FOOD calculus", which is heavily based on Scala. Using this syntax, \weixin defines a set of type-directed transformation rules, to formalise transformations between Cook's pure OOP decomposition style, outlined in \cite{food} and functional decomposition style.

These rules outline the fundamental transformations required to convert between the two styles within FOOD calculus, including transforming the top level types (datatype and traits) and their associated functions (generators, constructors, destructors and consumers).

% \weixin uses type directed rules to formalize the transformation to perform.

\weixin creates an implementation of this transformer in Scala and demonstrates the mapping between the theoretical rules in FOOD, and practical examples in Scala.
\weixin claims that the transformation rules are not Scala specific and can therefore be applied to other multi-paradigm languages. The primary goal of this paper is to replicate the work of \weixin's Scala transformer in another programming language, Rust, in order to validate this claim.



\section{Rust}

% Both Rust and Scala offer high level safety guarantees using the typing system.
% Rust 
Unlike Scala, Rust's API offers direct control over resource management, much like low-level languages such as C. However in order to maintain the safety guarantees, Rust approaches this control using a type system based on the principles of ownership and borrowing. For features that cannot be implemented with in Rust's strict type system, rust uses the concept of \rust{unsafe} code. This allows developer to directly use raw pointers and thus make use of shared mutable state. For these cases Rust's embraces the concept of "unsafe code encapsulated within safe APIs" \cite{rustsafesystems}.
The unique and strong type system of Rust presents additional challenges when creating a transformer for the rust programming language.

\subsection{Ownership, Borrows and References}

In system level programming languages such as C, developers have direct control over memory management. With this developers can create shared mutable state, using pointers to reference objects in memory. With this control, there is an inherently lack of safety. Developers are able to create references to objects which are then later mutated and such the pointer becomes invalid.

In order to maintain safety guarantees, programming languages have developed ways to obfuscate this memory management. High level languages such as Scala, often implement a garbage collector. This is a separate process which periodically frees any memory that can no longer be accessed by the program. This approach, however, comes at a performance cost as the garbage collector is repeatedly having to run through the program in order to detect memory which can be freed.

\subsubsection{Ownership}

In order to maintain control for the developers, instead of using a garbage collector, Rust opts for the principal of ownership. Rust wraps the use of raw pointers in \rust{unsafe} blocks. Outside of these blocks Rust enforces the strict "ownership" system.

The key principal of ownership is every variable in Rust has a single owner. Once the owner of a variable goes out of scope, the drop operation is run, which generally means the memory is freed. Ownership of a variable can be transferred by either, reassigning to a new variable, returning from a function or passing directly into a function. Note for types which implement the Copy trait (there are type which have a fixed size such as primitive types) these operations actually copy the values instead of changing ownership.

\subsubsection{References}

References provide a way to refer to a value without taking ownership of that value. A reference can be created with the \rust{&} symbol. For example:

\begin{minted}{rust}
let a = String::from("Hello world");
let b = &a;
\end{minted}

By default references are immutable, so in this example \rust{b} is an immutable reference of the variable \rust{a}.
There is no limit on the number of immutable references which can be created for a value. Mutable references however allow direct mutation of the underlying value and can be created with \rust{&mut}. Therefore for a value, only a single mutable reference can be created at one time. Moreover if a mutable reference exits, no immutable references can exits. This approach allows Rust to prevent the creation of dangling references as compile time and thus without impact on performance.

\subsubsection{Smart Pointers}

Rust also has a set of Smart pointers, these are data structures which "act like pointers" but include additional functionality. This paper currently only support the Box smart pointer. This pointer, allows the developer to assign a value on the heap. The Box is stored on the stack and stores a pointer to the value on the heap. Note the box is the owner of the data on the heap.
% Talk aobut smart pointers

\subsubsection{Dereferencing}

In Rust the "indirection", \rust{*}, operator can be used to access the value within a reference or pointer. 

A convenient feature in rust is the \rust{.} operator will automatically reference or dereference a value where possible. For example when using method with signature \rust{fn double(&self)} or \rust{fn double(self: Box<Self>)} an owned values of type Self can be called and the reference/box will be created.
% Talk about dereferencing
% Talk about clone 
% Talk about how the dot operator auto references or dereferences a thing

% DYN%

\subsection{\rust{dyn} and Recursive types}
\label{sec:dyn}

In order to create a type in Rust, the compiler must know its size. When types have a another value within them, which is/contains a value of the same type, they are called "recursive types". These kinds of types inherently do not have fixes sizes, as there is no limit to the nesting of the type. 
An example of a recursive type in Scala is shown in \autoref{fig:recursive-typedef-scala}. In this case Cons, contains a List within it. In Scala this issue of recursive types is obfuscated from the programmer.
In Rust, however, this construction is not permitted. Instead a pointer must be used and throughout this paper the Box smart pointer will be used. The Box datatype has a fixed sized and contains a pointer to the location of the nested value. Therefore this smart pointer can be used to create recursive types as shown in \autoref{fig:recursive-typedef-rust}.

Notice also how the \rust{dyn} prefix is used in the \rust{Box}. The \rust{dyn} prefix allows dynamic dispatch for a trait, by storing a pointer to the object data as well as a pointer to a map of the methods for the concrete type. Therefore without \rust{dyn} the concrete type of the attribute would be unknown and the methods could not be called (this will result in a compile time error). This \rust{dyn} prefix is required each type a generic trait is reference rather than a concrete type, for example in parameter arguments or return types. These limitation can be overcome with trait bounds, but these are beyond the scope of this transformer.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \codefile{scala}{../scala/examples/src/main/scala/RecursiveType.scala}
    \caption{Scala recursive class}
    \label{fig:recursive-typedef-scala}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustexample{list/oop}
    \caption{Rust recursive struct}
    \label{fig:recursive-typedef-rust}
\end{subfigure}
\caption{Recursive Type Definitions}
\label{fig:recursive-typedef}
\end{figure}


% -----------------------------------------------------------------------------

% \noindent
% This chapter is intended to describe what you did: the goal is to explain
% the main activity or activities, of any type, which constituted your work 
% during the project.  The content is highly topic-specific, but for many 
% projects it will make sense to split the chapter into two sections: one 
% will discuss the design of something (e.g., some hardware or software, or 
% an algorithm, or experiment), including any rationale or decisions made, 
% and the other will discuss how this design was realised via some form of 
% implementation.

% This is, of course, far from ideal for {\em many} project topics.  Some
% situations which clearly require a different approach include:

% \begin{itemize}
% \item In a project where asymptotic analysis of some algorithm is the goal,
%       there is no real ``design and implementation'' in a traditional sense
%       even though the activity of analysis is clearly within the remit of
%       this chapter.
% \item In a project where analysis of some results is as major, or a more
%       major goal than the implementation that produced them, it might be
%       sensible to merge this chapter with the next one: the main activity 
%       is such that discussion of the results cannot be viewed separately.
% \end{itemize}

% \noindent
% Note that it is common to include evidence of ``best practice'' project 
% management (e.g., use of version control, choice of programming language 
% and so on).  Rather than simply a rote list, make sure any such content 
% is useful and/or informative in some way: for example, if there was a 
% decision to be made then explain the trade-offs and implications 
% involved.


\chapter{Project Execution}
\label{chap:execution}

\section{Setup}

The transformation rules are applied to an abstract syntax tree, AST. 
Although rust exposes the native compiler parser through the \verb|rustc| create, the APIs are unstable and there is limited documentation. Instead the \verb|syn| package is used to parse the provided source code file into an AST. 
The \verb|syn| package is designed for parsing macro inputs and is therefor limited in its support for creating AST objects. A minor fork (\url{https://github.com/jelgar/syn}) of the package was created to expose an additional module which enabled the creation of a required set of types.

Once the transformations are completed, the \verb|quote| package is used to covert the AST back into Rust source code tokens. The \verb|rustfmt| package is then used to format the output in a consistent style.

The transformer packaged is exposes as a command line interface, CLI, which takes the path to a file and the desired transformation type (OOP to FP / FP to OOP). 

The project is split into three modules, the core, examples and outputs. The examples and outputs have been separated from the core project to enable them to be tested independently. The core and examples have a limited set of unit tests. An end to end test has then been setup which runs the transformer on a set of examples, and runs a set of unit tests on the output files to validate the functionality of the output matches that of the input.

% Before any transformations take place, the provided code is transformed into an abstract syntax tree (AST). This provides a concrete format to represent the code syntax, keeping the transformer separate from the code parsing stage. Rust provides access to the \verb|rustc| package, which is its compiler. This contains the \verb|rustc_ast| and \verb|rustc_parse| package which are responsible for this parsing.

% However as Rust is a relatively new language many of these features have unstable APIs. This means they are only available with the nightly compiler and the interface is likely to change. To avoid this, the syn package is used for parsing the rust code into a syntax tree \cite{syn}which provides a stable interface.
% The main issue with Syn, is it geared toward helping parse code in Rust procedural macros. This means it has a stronger focus on understanding an AST rather than manually manipulating it and creating items. For this reason a fork of the Syn project was created to allow access to an additional library, which made creating more AST objects possible. The fork can be found at .

% Another requirement, is once the AST has be transformed, it must be converted back into Rust code. To achieve this the \verb|quote| and \verb|rustfmt|. The \verb|quote|  package converts the AST back into tokens of source code, whilst the \verb|rustfmt| package formats the outputted code in a consistent way.

% Finally a CLI was created to allow developers to parse in files they want to transform and to specity the direction (OOP to FP or FP to OOP).

\section{High level overview}

In Rust, much like in Scala, a user defined datatype can be used directly as a type. However unlike in Scala, a trait (interface) cannot. In Rust, as discussed in \autoref{sec:dyn}, either the type of the struct must be used (which violates Pure OOP principals, as interfaces are types), or a dynamic reference to the trait. For this reason when converting a type, which is a datatype in FP, to OOP, a dynamic reference of the transformed trait is used. This fundamentally changes the type. These changes can be applied every time a variable, attribute or parameter is explicitly typed, including in function signatures, attributes of types and let assignments.
In order to accommodate these additional transformations of types the transformer has been split into 3 stages. 

The first stage, matches the pre-processing stage of the transformer outlined in \cite{food}. In this stage the "global context" is collected which stores items that are regularly used throughout the transformation. This prevents repeated look ups of the same items, both improving efficiency and reducing the complexity of the transformer.

% A high level algorithm for each of the transform types is given in \autoref{alg:high-level}. In each case the transformer operates by performing three distinct passes of the AST.

The second stage is responsible for the transformations of the user defined types and their associated functions. This primarily matches the transformations outline by \weixin in \cite{food} however also includes the transformations of the types. For this reason the only transformation applied to the bodies of functions at this stage is a rename operation, whilst other transformations are handled in stage 3. This means at the end of this stage the program may be invalid as types may not match what is required. For example, if a function return type is updated to a dynamic pointer from a datatype, the body will still only return the datatype. This is addressed in the third stage.

The third stage is responsible for updating any references to the user defined types and their functions. Again this closely aligns with the work from \cite{food}. However this stage also updates the types of each expression. It does this by recursively parsing the expected return type to each expression which is transformed. This outlines the type which the expression should return after the transformation is completed. Expressions which do not match the expected return types are updated using the reference ($\&$), pointer ($\sim$) and dereference ($*$) operators. 

This approach allows the decoupling of the transformation of the user defined types which results in the translation of types, from the third stage which fixes any changed type and updates the uses of any transformed typed. 

Throughout the following sections, the output of the discussed stage will be shown for the example input programs in \autoref{fig:ex-set}.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustexampleLN{exp/oop}{1}{22}
    \caption{OOP decomposition style}
    \label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustexampleLN{exp/fp}{1}{11}
    \caption{FP decomposition style}
    \label{fig:dec-ex-fp}
\end{subfigure}
\caption{Example input to transformer}
\label{fig:ex-exp}
\end{figure}

% for enujm in gamma.enums...
\begin{figure}
\begin{minipage}{0.46\textwidth}
    \begin{algorithm}[H]
    delta $\leftarrow$ collect delta \\
    \For{int {\bf in} delta.interfaces}{
        create enum\\
        \For{gen {\bf in} int.generators}{
            create enum variant\\
        }
        
        \For{dest {\bf in} int.destructors} {
            create consumer function\\
            \For{gen {\bf in} int.generators}{
                create match arm\\
            }
            \uIf{required} {
                create wildcard arm\\
            }
        }
    }
    \caption{OOP Transformation}
    \label{alg:high-level-oop}
    \end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.46\textwidth}
    \begin{algorithm}[H]
    \For{$i=0$ {\bf upto} $n$}{
      $t_i \leftarrow 0$\;
    }
    \caption{FP Transformations}
    \label{alg:high-level-fp}
    \end{algorithm}
\end{minipage}
\caption{High level overview of transformations}
\label{alg:high-level}
\end{figure}

% In each case the transformations consist of three distinct stages. The first stage is to collect the global context, delta. This is used throughout the transformations to prevent repeated looks ups for items.

% The seconds stage is the transformation of the user defined types (either datatypes or traits). This stage closely maps to the transformation rules outline by \weixin in \cite{food}.
% Using these the user type as well as their associated destructors/consumers transformed. At this stage the  Implementations of destructors are converted into a match arm in a consumers, whilst consumers' match arms are converted into  destructor implementations.

% The final stage is to transform all expressions, including the output of the type transformations. This transforms any references to transformed function and methods, and updates the types of expressions which require updating as a result of the transformations.

\section{Formalization}

The RFOOD syntax, introduced in \autoref{fig:syntax}, is used to formalize the Rust transformations. This syntax is based on the FOOD syntax created by \weixin in \cite{food}, however with some modifications to support the Rust typing system. Primarily three additional types have been added, $\sim T$, $\sim_{dyn} T$ and $\& T$ where T is a type. These represent a pointer, a dynamic pointer and a borrow respectively. Note for this report only the Box pointer is considered, however most other smart pointer types should be analogous. Raw pointers (and all unsafe concepts) have not been included. The $\sim e$ and $\& e$ expression are added in order to create pointers ($\sim$ creates both pointers and dynamic pointers) and references of expressions. The additional $*e$ expression can then be used to dereference expressions, allowing access to a value within a pointer/reference. This extended syntax has been called RFOOD and is given in full in \autoref{fig:syntax}. 

% A fundamental difference between the transformer from \cite{food} and the transformer proposed in this paper, is that types are required to change in Rust. This comes about as in Rust, much like in Scala, datatypes (enums) can be used as types directly. However once transformed to a trait, as discussed in \autoref{sec:dyn} they can not longer be used as types. Instead a dynamic reference of the trait must be used. This fundamentally changes the type. This can have impacts across the whole program and therefore additional considerations are required throughout the transformations.

% The places types will change during the translation are: 
% 
% \begin{itemize}
    % \item Any dyn reference in OOP, this is handled differently in destructors to the rest of the code.
    % \item Anything which has the type of a Datatype directly (this does not explicitly include constructors). % TODO why, talk about how rules pretend a DT is returned from a constructro but its actually the struct type itself. Instead of transorming this directly we have left it (to not break other code) but results in more complicated translations
% \end{itemize}

% In \cite{food} \weixin denotes a type directed translation of the program $L$ of type $T$, under the context of $\Gamma$ and $\Delta$ with $\oldtrans L T L'$, however with the potential for types to change, this is no longer sufficient. Instead translations are represented as $\trans L T {L'} {T'}$ where $T'$ represents the output type after the translation. Moreover, the $\vdash^{\sigma}$ symbol is used to represent a translation which should result in an expression of type $\sigma$, so in full $\transSigma \sigma L T {L'}$.

% Note the changing in type, never effects the base type, but only the references/pointers. For example, the transformation may convert the result type of a function from \rust{Box<dyn T>} to \rust{T} but it may not change \rust{T} itself.

\begin{figure}[t]
  \setstcolor{red}
  \begin{displaymath}
    \begin{array}{l}
      \begin{array}{llcl}
        \text{Program}
        & L & \Coloneqq & Def ; L \mid e\\
        \text{Definition}
        % & Def & \Coloneqq & Dt \mid It \mid Gen \mid Csm \\
        & Def & \Coloneqq & Dt \mid It \mid Gen \mid  Dec \\
        \text{Datatype}
         & Dt & \Coloneqq & \datatype {\dtn} {Ctr}\\
        \text{Interface}
         & It & \Coloneqq & \interface {\itn} {Dtr}\\
        \text{Destructor}
         & Dtr & \Coloneqq & \fnDstr {\fnArg{self}{\&\kwself}} {x: T} {T} \mid \fnDstr {\fnArg{self}{\sim \kwself}} {x: T} {T}\\
        \text{Constructor}
        & Ctr & \Coloneqq & \ctr {\ctrn} {\fnArgs{x}{T}} \\
        \text{Generator}
        %  & gen & \Coloneqq & \class C {C(\overline{e})} {fun}\\
         & Gen & \Coloneqq & \class{\genn}{x}{T}{Itn}{Fun}\\
         
        \text{Consumer}
        & Csm & \Coloneqq & \csmcase {d: \&Dtn} {x: T} T {d} {P} {e} \\
        &     &           & \mid \csmcase{d: Dtn}{x: T}{T}{d} {P} {e}\\
        &     &           & \mid \csmcase{d: \sim Dtn}{x: T}{T}{d} {P} {e}\\
        
         \text{Declaration}
         & Dec & \Coloneqq & \dec {x: T} T\\
         \text{Function}
         & \text{Fun} & \text{$\Coloneqq$} & \text{Dec} = \overline{e} \\
        \text{Expression}
         & e & \Coloneqq & x %\mid \overline{(x: T)} \Rightarrow t 
          \mid e_1.f(\overline{e_2})  \mid f(\overline{e}) \mid \genn(\overline{e}) \mid \ctrCall{\dtn}{\ctrn}{e} \mid \sim e\mid \&e \mid *e\\
         &   &           & \ifelseexpr{e_1}{e_2}{e_3} \mid \eqexpr{e_1}{e_2} \mid \macroexpr{m}{e} \\
         \text{Pattern}
         & P & \Coloneqq & \genn(\overline{x}) \mid \ctrCall{\dtn}{\ctrn}{x} \mid \_ \\
        %  \text{Modifier}
        %  & m & \Coloneqq & \kwsealed \mid \kwabstract \mid \kwcase\\
        \text{Boolean}
        & bool & \Coloneqq & true \mid false  \\
        \text{Types}
        & T & \Coloneqq & \dtn \mid \overline{T} \rightarrow T \mid \sim  T \mid \& T \mid \sim_{dyn} \itn \mid bool \mid \bot \\
        & \dtn & \Coloneqq & \text{Datatype name} \\
        & \itn & \Coloneqq & \text{Interface name} \\
        & \ctrn & \Coloneqq & \text{Constructor name} \\
        & \genn & \Coloneqq & \text{Generator name} \\
        & f & \Coloneqq & \text{Function or destructor name} \\
        & Self & \Coloneqq & \text{Alias for type of trait within trait or impl block} \\
        % & \blockO{e} & \Coloneqq & \text{Block of expressions} \\
        
        % & D & \Coloneqq & Dtn \mid Itn \\
        % & C & \Coloneqq & \text{constructor or generator names} \\
        % \text{Names}
        % & D & datatype or interface name \\
      \end{array}
    \end{array}
  \end{displaymath}
  \caption{Syntax of RFOOD}
  \label{fig:syntax}
\end{figure}

\section{Context}

\subsection{Global Context}

The first stage of the transformation is the collection of the "global context", $\Delta$. This collects all user defined types and the associated functions as outlined in \cite{food}. It exposes a set of methods that enables, checking if a type name is a datatype or interface, getting all constructors/generators and destructors/consumers for a user defined type, getting the signatures for both constructors/generators and destructors/consumers and finally getting the definition of any of the above. Note in the Rust implementation, top level functions are also collect in gamma and their signatures can be accessed using $\sig$. Given the Rust implementation is based on named fields in types, the $\attrs$ method has also been added, in order to access the attributes of a generator/constructor. 
The set of attributes and methods exposed by gamma in the formalization, are shown in \autoref{fig:global-context}.

The gamma collected for the examples from \autoref{fig:ex-exp} are:

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
    \begin{minted}[escapeinside=||,mathescape=true]{text}
    {
        IT: {Set}
        DTR: {
            Set: {eval}
        }
        GEN: {
            Set: {Lit, Sub}
        }
        SIG: {
            Lit: i32  -> Lit 
            Sub: |$\sim_{dyn}$ Set|, |$\sim_{dyn}$ Set|  -> Lit 
            eval: &Self -> i32 
        }
        ATTR: {
            Sub: [n]
            Lit: [l, r]
        }
    }
    \end{minted}
    \caption{OOP decomposition style}
    \label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \begin{minted}[escapeinside=||,mathescape=true]{text}
    {
        DT: {Set}
        CSM: {
            Set: {eval}
        }
        CTR: {
            Set: {Lit, Sub}
        }
        SIG: {
            Lit: i32  -> Lit 
            Sub: |$\sim$ Set|, |$\sim$ Set|  -> Lit 
            eval: &Exp -> i32 
        }
        ATTR: {
            Sub: [n]
            Lit: [l, r]
        }
    }
    \end{minted}
    \caption{OOP decomposition style}
    \label{fig:dec-ex-oop}
\end{subfigure}
\caption{Gamma collect for example from \autoref{fig:ex-exp}}
\label{fig:ex-exp-gamma}
\end{figure}

\begin{figure}[t]
\begin{tabular}{ll}
\dt & Set of Datatypes\\
\ct & Set of Interfaces\\
$\ctrs{\dtn}$ & Constructors of datatype $\dtn$\\ 
$\dtr{\itn}$ & Destructors of interface $\itn$\\  
$\generator{\itn}$ & Generators of interface $\itn$\\
$\consumer{\dtn}$ &  Consumers of datatype $\dtn$\\
$\sig(N)$ & Signature of constructor/generator/consumer indexed by name $N$ \\
% $\csmBody(f,C)$ & Case clause for constructor $C$ in consumer $f$\\
% $\dtrBody(f,C)$ & Body of destructor $f$ in generator $C$\\
$\dtrType(f,\itn)$ & Signature of destructor $f$ for interface $\itn$\\
$\dtrType(f,\itn)$ & Signature of destructor $f$ for interface $\itn$\\
$\defs(N)$ & Definition of function/datatype/interface/constructor/generator/consumer by name $N$\\
$\defs(f, N)$ & Definition of destructor $f$ for interface $\itn$\\
$\attrs(\genn)$ & Return list of attribute names for generator $\genn$\\
$\attrs(\ctrn, \dtn)$ & Return list of attribute names for constructor $\ctrn$ of datatype $\dtn$\\
% Caseof & $\caseof f C$ & = & if $\kwdef \ f(x : D)(\overline{x : T}): T = \overline{\kwcase\ C(\overline{x}) \Rightarrow e}\ \land$ \\
\end{tabular}
\caption{Stage 1 - Global context, $\Delta$, attributes and methods. Based on \cite{food}.}
\label{fig:global-context}
\end{figure}


% In \cite{food} \weixin uses the name "global context", represented by $\Delta$, to represent the store with  the following attributes:

% \paragraph{Datatypes.} Datatypes are equivalent to Enums in Rust.
% \paragraph{Interfaces.} Interfaces are equivalent to Traits in Rust.
% \paragraph{Constructor.} Constructors create an instance of a given datatype. In Rust the equivalent is an Enum Variant.
% % TODO update this if it can handle multiple inheritance
% \paragraph{Generator.} Generators create an instance of a given interface. In Rust the equivalent is a struct which has an implementation of a trait (the interface). Note in this implementation a struct cannot be transformed if it implements multiple traits. 
% \paragraph{Consumer.} Consumers are functions that operate on a datatype. In Rust these are functions, with the first argument being a Datatype (or some reference to a datatype).
% \paragraph{Destructor.} A destructor is a function that operates on an interface. In Rust these are methods defined on the traits, which must either have a default implementation (in the trait) or an implementation in the generators impl of the trait.

% \vspace{10pt}

% In the current implementation of the transformer "global context" is an accurate name, as the Datatypes are collected at the beginning of the transformation. However, this does oversimplify the reality as it is possible to define Datatypes within scopes. In this case it would be more accurate to generate Gamma recursively (along side other transformations) to ensure all scoping is correct. Note, however, the top level types must be collected before any other types are transformed, as types defined after a function can still be referenced in that function.

% % TODO did the change?
% For the Rust implementation the Visitor pattern was used. Provided by the \verb|syn| package these traits (\verb|visit| and \verb|visit_mut|) allow the developer to view/mutate selected types of values in the AST. The default implementation of the visitor trait methods, recursively visits all expression. Using this technique ignores the scope of the current expression meaning all types and methods are collected and therefore assumes the developer has not renamed any types/function in a nested scope. This technique could be improved by a recursive approach to collecting this context along side other transformations, to ensure the correct versions of all the attributes are collected.

\subsection{Environment}

The other context variable is called the "Environment", $\Gamma$, and is used throughout the third stage of the transformation. The environment stores the types of variables currently on the stack. 
In the Scala implementation types are encoded as strings, however this is insufficient for Rust as reference types must also be kept track of. For this reason in the Rust implementation types are encoded as \verb|DeltaType| struct. 
This stores both the type name stored as an Ident (a simple wrapper around String provided by \verb|syn|) and the reference type.

The reference type is implemented as a recursive Enum called the \verb|ReferenceType|, with a variant for \verb|Box| (pointer) and \verb|Reference| which both contain a nested \verb|ReferenceType| and a variant for \verb|None|. This allows the encoding of arbitrarily nested references. For example the following type signature in Rust \rust{&Box<&i32>}, is represented as a \verb|DeltaType| with the name "i32" and the reference type \verb|Ref(Box(Ref(None)))|.

% TODO sort this out
% The key limitation of the current implementation is these are not stored as recursive types. This means nested references such as reference of boxes cannot be encoded. 

% \subsection{Type of Expression}

% With these two contexts it is possible to perform basic type inference on expressions. The following rules are used to perform this inference:

% \begin{itemize}
%     \item For literals, for example \rust{1} or \rust{true}, a basic type of the literal is returned. In the current implementation only a subset of literals are supported (\rust{i32}, \rust{f32}, \rust{bool}) and they are assumed to be a specific type (i.e. \rust{i32} rather than a generic \verb|Int|).
%     \item For paths, which are expressions referring to some variable in the environment, the type of the variable from the environment is returned 
%     \item For struct/enum instatioation, the struct or enum type is returned.
%     \item For method or function calls, the return type of the function is returned
%     \item For field expressions, for example \rust{self.a} or \rust{item.field}, the type of receiver (the thing the field is being accessed from) is determined using gamma. The type of the field which is being accessed is then returned.
% \end{itemize}

% % TODO talk about casting
% There are a few key limitations to the current type inference:

% \begin{itemize}
%     \item  Currently the transformer only support single file transformations. This means it cannot perform type inference on imported or built in functions. Supported has been added for a small subset of built in functions such as clone but this is currently a manual process rather than the importing definitions.
%     \item  Currently a limited subset of expressions are supported. With time the types of expressions that are supported could be extended.
%     \item Currently type names are encoded as strings and as such no casting is supported. For example when if a literal is marked as an \rust{i8} the transformer is unable to cast that to an \rust{i32} if required.
% \end{itemize}

% One approach to resolving these issues is to use the built in type inference in the rust compiler. The rust compiler parses the source code (including all dependencies) and converts it to an AST, then to a HIR (High-Level Intermediate Representation) and then finally a THIR (Typed High-Level Intermediate Representation), where the typing information is added. It collects (and checks) these types using the \verb|rustc_typecheck| package which can be exposed using the nightly compiler. Currently the documentation for this package is very limited and the API for all of rustc remains unstable. For this reason this approach was not developed for this version of the compiler, but could create some tangible improvements to the type checking in the future. Another improvement of using this crate is the type checking could automatically track with the version of rust the developer is using. This means as rust add or changes features in the langauge the type inference of the transformer would not have to be updated directly.

\section{Type Transformations}

\begin{figure}[t]

\begin{mathpar}
\inferrule[It2Dt]
{  \overline{Gen} = \generator{\itn} \\ \overline{\transtwoNT{\itn}{Gen}{Ctr}} \\ \overline{\transtwoNT{}{Dtr}{Csm}} \\ \overline{} \\ \transtwoNT {} L {L'} \\ \dtn = \itn}
{ \transtwoNT {} {\interface{\itn}{Dtr}; L} {\datatype {\dtn} {Ctr}; \overline{Csm}; L'}}
\end{mathpar}

\begin{mathpar}
\inferrule[Gen2Ctr]
{ \overline{\transType {\itn} T {T'}} \\  } %  
{ \transtwoNT {}{\class {C} {x} {T} {\itn} {\overline{Fun}}} {\ctr{C}{\overline{x: T'}}}}
\end{mathpar}

\begin{mathpar}
\inferrule[Dec2Csm]
{\fresh{d} \\ \overline{\genn} = \generator{\itn} \\ \transType{\itn}{T_{s}}{T_{s}'} \\ \overline{\transType{\itn}{T_{x}}{T_{x}'}} \\ \transType{\itn}{T}{T'} \\ \sig(G) = T_{g}
}
{\transtwoNTB{\itn}{\decD{self: T_{s}}{x: T_{x}}{T}}{
    \csmcase{\fnArg{d}{T_{s}'}} {\fnArg{x}{T_{x}'}} {T} {d}  {\ctrPat{\itn}{\genn}(\attrs(G))} {[self \mapsto d]\gdefinition{f}{\genn}}}
}
\end{mathpar}

% \begin{mathpar}
% \inferrule[Fun2Csm]
% { \dtrToCsm {\decD {self: T_{s}} {xs: T_{x}} {T}} {\csmcase {d: T_{s}'} {xs: T_{x}} {T'} {e}} }
% { \dtrToCsm {\decD {self: T_{s}} {xs: T_{x}}{T} = e} \csmcase{d: T_{s}'} {xs: T_{x}'} {T'} {e}; \kwcase\; \_ \Rightarrow [self \mapsto d]e}
% \end{mathpar}

% \begin{mathpar}
% \inferrule[Fun2Case] % reuse
% { \decD {self} x T = e \in \overline{Fun} }
% {  \dtrToCase {\classf {C} {y} {S} {D} {\overline{Fun}}} {\case {C(\overline{y})} {[self \mapsto d]e}}}
% \end{mathpar}

% \inferrule[Csm]
% {\overline{\trans {e} T {e'}} \\ \trans L T L'}
% { \trans {\csmcase e; L} {T}  {\csmcase {e'};L'}}

% \begin{mathpar}
% \inferrule[DecRef2Csm]
% { \overline{C} = \generator{D} \\ \overline{\dtrToCase{\defs(C)} {\case P e}}}
% { \dtrToCsm {\destr \& x} {\csmcase d \& e}}
% \end{mathpar}

% \begin{mathpar}
% \inferrule[DecRef2Csm]
% { \overline{C} = \generator{D} \\ \overline{\dtrToCase{\defs(C)} {\case P e}}}
% { \dtrToCsm {\destr \& x} {\csmcase d \& e}}
% \end{mathpar}

% \begin{mathpar}
% \inferrule[DecPointer2Csm]
% { \overline{C} = \generator{D} \\ \overline{\dtrToCase{\defs(C)} {\case P e}}}
% { \dtrToCsm {\destr \sim x} {\csmcase{d}{}{e}}}
% \end{mathpar}

\caption{Stage 2 - User defined types transformation rules}
\label{fig:stage2-trans-oop}
\end{figure}

\begin{figure}[t]

% \begin{mathpar}

% \inferrule[CreateRef]
% {}
% {\transTypeVal{Dtn}{\& T}}

% \inferrule[CreateRef]
% {}
% {\transTypeVal{Dtn}{T}}

% \inferrule[CreateRef]
% {}
% {:Dtn val}

% \end{mathpar}

\begin{mathpar}
% \inferrule[CreateRef]
% {\transType{Dtn}{\&Dtn}{T}}
% { \transType{Dtn}{\& Self}{T}}

\inferrule[CreateRef]
{}
{ \transType{Dtn}{\& Self}{\&Dtn}}

\inferrule[CreateRef]
{}
{ \transType{Dtn}{\sim Self}{Dtn}}

\inferrule[CreateRef]
{}
{ \transType{Dtn}{\sim_{dyn}Dtn}{Dtn}}

\inferrule[CreateRef]
{}
{ \transType{Dtn}{\sim_{dyn}D}{\sim D}}

\end{mathpar}

\begin{mathpar}

\inferrule[CreateRef]
{}
{ \transType{}{\sim_{dyn} Dtn}{\sim Dtn}}

\end{mathpar}
\caption{Stage 2 - Type transformation rules}
\label{fig:trans-fp}
\end{figure}


\begin{figure}

% \begin{mathpar}
% \inferrule[App2Sel]
% { \transSigma {D}{e_1} D {e_1'} \\ f \in \consumer{D} \\ \sig(f) = D, \overline{T} \rightarrow T \in \Gamma  \\ \transType{D}{T}{T'}\\ \overline{\transType{}{T}{T'}  \qquad \transSigma {T'} {e_2} T {e_2'}}}
% { \trans{f(e_1, \overline{e_2})}{T}{e_1'.f(\overline{e_2'})}{T'}}
% \end{mathpar}

\begin{mathpar}
\inferrule[Fn]
{\transSigma{T}{e}{e'} } %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transThreeStmt{\fnDec{xs: T_{x}}{T}{e}} {\fnDec{xs: T_{x}}{T}{e'}}}
\end{mathpar}

\begin{mathpar}
\inferrule[Block]
{\transSigma{\sigma}{e}{e'} \\ \overline{\transThreeStmt{e}{e'}}} %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transSigma{\sigma}{\block{\overline{e}; e}} {\block{\overline{e'}; e'}}}
\end{mathpar}

\begin{mathpar}
\inferrule[Class]
{\overline{\transThreeStmt{Fun}{Fun'}}} %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transThreeStmt{\class{\genn}{x}{T}{\itn}{\overline{Fun}}} {\class{\genn}{x}{T}{\itn}{\overline{Fun'}}}}
\end{mathpar}

\begin{mathpar}
\inferrule[New2Obj]
{ C \in \generator{D} \\ \sig(C) = \overline{T} \rightarrow D \\ \overline{\transSigma {T} e {e'}} } %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{ \trans{C(\overline{e})}{D}{D::C (\overline{e'})}{D}}
\end{mathpar}

\begin{mathpar}
\inferrule[Sel2App]
{ f \in Csm \\ SIG(f) = \fntype{T_{1}, \overline{T_{s}}}{T} \\ \transSigma{T_1}{e_1}{e_1'} \\ \overline{\transSigma{T_s}{e_2}{e_2'}}} %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transSigma {\sigma} {\methodcall{e_1}{\overline{e_2}}} {\fncall{e_1', \overline{e_2'}}}}
\end{mathpar}

\begin{mathpar}
\inferrule[Sel]
{ f \notin Csm \\ SIG(f) = \fntype{\overline{T_{s}}}{T} \\ \transThreeStmt{e_1}{e_1'} \\ \overline{\transSigma{T_s}{e_2}{e_2'}}} %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transSigma {\sigma} {\methodcall{e_1}{\overline{e_2}}} {\methodcall{e_1'}{\overline{e_2'}}}}
\end{mathpar}

% \begin{mathpar}
% \inferrule[Sel2App]
% { d \in Dtr } %\\ {\trans t {\overline{T} \arrow T} {t'}} }
% { \transTypeExprE {\fnDstr{d: T_d}{b}{c}} {} {}}
% \end{mathpar}

\begin{mathpar}
\inferrule[If]
{ \transSigma{bool}{e_1}{e_1'} \\ \transSigma{\sigma}{e_2}{e_2'} \\ \transSigma{\sigma}{e_3}{e_3'}}
{ \transSigma {\sigma} {\ifelseexpr{e_1}{e_2}{e_3}} {\ifelseexpr{e_1'}{e_2'}{e_3'}} }
\end{mathpar}

\begin{mathpar}
\inferrule[Eq]
{ \transSigmaT{T_1}{e_1}{T_1}{e_1'} \\ \transSigma{T_1}{e_2}{e_2'}}
{ \transThree {\eqexpr{e_1}{e_2}} {\eqexpr{e_1'}{e_2'}} {T} } 
\end{mathpar}

% \begin{mathpar}
% \inferrule[Match]
% {}
% { \transTypeExprE {\matchexpr{e_1}{b}{c}{d}{e}} {T}  }
% \end{mathpar}

\caption{Stage 3 - Expression transformation rules}
\label{fig:trans-fp}
\end{figure}

\begin{figure}[t]

\begin{mathpar}
\inferrule[CreateRef]
{\sigma = \& T}
{ \transTypeExpr{}{\&}}

\inferrule[CreatePointer]
{\sigma = \sim T}
{ \transTypeExpr{}{\sim}} \\

\inferrule[DerefRef]
{\sigma = T}
{ \transTypeExpr{\&}{*}}

\inferrule[DerefPointer]
{\sigma = T}
{ \transTypeExpr{\sim}{*}} \\

\inferrule[Pointer2Ref]
{ \sigma = \& T}
{ \transTypeExpr{\sim}{\&*}}
\end{mathpar}

\caption{Expression type transformation rules}
\label{fig:trans-type}
\end{figure}


Having collected the global context, the next stage of the transformation is to transform the user defined types, and their associated functions. These follow the stage 2 rules outlined in \autoref{fig:stage2-trans-oop} and \autoref{fig:stage2-trans-fp}. The implementation of this stage is split into two sections, the transformations of the types themselves, and then the transformation of the functions.

\subsection{Transformation of Type Definitions}
The first stage of this process is to transform the types themselves.  These transformations follow the \verb|IT2DT|, \verb|DT2IT|, \verb|GEN2CTR| and \verb|CTR2GEN| rules which are heavily based on the matching rules from \cite{food}. However these rules have been adapted to support the Rust type translations. With this step the types are converted to the new decomposition styles and the types of the attributes are updated.

% During this stage, for OOP the only type transformation rule applied is \verb|Pointer2Dyn|. This is because once traits have been converted to datatypes, they will no longer require the dynamic reference. For FP there are two cases which are transformed, datatype is used in one of the types, either directly or as a reference it is converted to a dynamic reference.

Note although not included in the formalization the visibility of the transformed types and functions are also translated in this stage. 

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustsnippet{type_def_transfom_oop}
    \caption{OOP decomposition style}
    % \label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustsnippet{type_def_transfom_fp}
    \caption{FP decomposition style}
    % \label{fig:dec-ex-fp}
\end{subfigure}
\caption{Outputs of the type definition transformation during the second stage of the transformer, on the examples from \autoref{fig:ex-exp-stage2p1}}
\label{fig:ex-exp-stage1}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustsnippet{type_fn_transfom_oop}
    \caption{OOP decomposition style}
    % \label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustsnippet{type_fn_transfom_fp}
    \caption{FP decomposition style}
    % \label{fig:dec-ex-fp}
\end{subfigure}
\caption{Outputs of the second stage of the transformer from examples in \autoref{fig:ex-exp}}
\label{fig:ex-exp-stage2}
\end{figure}

For OOP to FP the following steps are followed for each trait as outline in \verb|IT2DT| and \verb|GEN2CTR|:
\begin{enumerate}
    \item Create an enum with the same identifier and visibility as the trait
    \item For each generator of the trait (a struct which only implements that trait), add an enum variant to the enum. The enum variant should have the same identifier and fields as the struct. However if the struct references any other traits (including it self) then the types must be updated. Each reference to a trait will be a dynamic pointer and should be converted to a pointer as shown in \autoref{fig:thing}. This is because, once the trait is converted to an enum, it will no longer have methods associated with it. Therefore the pointer for dynamic dispatch (as outlined in \autoref{sec:dyn}) is no longer required. Note if this token is left in a compile-time error will occur. 
    Additionaly in Rust, enum variants and their fields do not have visibility separate from the Enum it self and therefore all visibility tokens (\rust{pub}) must be removed from the structs and their fields. This transformation will therefore result in a loss of information as if the program is transformed back to the original, the original visibility of the structs and fields will be unknown.
\end{enumerate}

For FP to OOP the following steps are followed for each enum:
\begin{enumerate}
    \item Create a (for now empty) trait with the same identifier and publicity as the enum  
    \item For each variant of the enum create a struct. The structs should have the same identifier and contain the same fields as the enum variant.
    However, as above, when an enum is transformed any datatypes which are used as types within attributes of the enum, will require a dyn pointer. For FP it is possible the type is either the datatype directly or a pointer to the trait. In both cases the type should be updated to become a dynamic pointer as reflected in the type transformation rules in \autoref{fig:trans-type-fp}.
    Given the visibility of enums is only set on the enum itself (rather than on any variant or fields), the visibility of the structs and their fields are copied from the enum.
\end{enumerate}

\subsection{Transformation of Associated Functions}

Having transformed the definitions of each type, the next stage of the transformation is to convert the associated functions. For OOP this involves transforming the implementations of the trait methods into consumers, whilst for FP, it involves transforming the consumers into the trait methods and associated implementations for the structs. The rules for these transformations also require alteration in order to accommodate the Rust typing system.

Similarly to the previous stage, the fundamental difference between these transformations and those from \cite{food} are the types in the signatures are required to change. In this case however a different set of type transformation rules are used. 

When transforming a datatype type or pointer to a datatype from FP, it must be transformed to a dynamic reference of the trait. As discussed in \autoref{sec:dyn} this is because a trait both does not have a fixed sized and must container a pointer to a vtable. However when transforming a dynamic trait from OOP, it is possible to convert it to either to a datatype type directly or to a pointer of the datatype. Both approaches are valid and have advantages. Converting to a pointer keeps an analogous type as dynamic pointers are treated much the same as pointers when writing in Pure OOP style. Where as converting to the datatype without the pointer, has the potential to reduce the complexity as a pointer may not be required. For datatypes the first style was selected as it prevents issues arising as a result of recursive types. In other cases this first style was also selected as it reduces the impact of the transformer on other aspects of the code. However for destructor implementations, the latter style was selected. This creates relational symmetry between the simpler FP style, where datatypes are used as types directly and the required dynamic pointer style in OOP.

In both directions references of interfaces/datatypes are unchanged. This is possible as references in OOP require an instance of a trait, this is inherently fixed size and of a concrete (struct) type.

In order to distinguish these type transformations, the $\transType{D}{}{}$ symbol is used to indicate a type transformation of type within the signature of a destructor/consumer of a given datatype, $D$, whilst all other type transformation are indicated by $\transType{}{}{}$.

Another consideration of destructor methods is the \verb|Self| type. In trait definitions this is a reference to the (unsized) trait, whilst in implementation block it is analogous to the struct type. In both cases this does not require a dynamic reference. For the purposes of the transformer, when a reference to Self is used this is analogous to a reference to the trait, where as when the Self type is used directly, this is analogous to a dynamic reference to the trait. For clarity rules for the transformation of Self have been included explicitly in \autoref{fig:}.

Using these typing rules the steps to transform the associated functions in OOP are as follows.

\begin{enumerate}
    \item For every destructor defined in the trait, a consumer is created. The consumer is a top level function with the same signature as the destructor, however with the type translations, discussed above, applied to the parameters and return type. The \verb|self| parameter is also renamed to avoid the use of the Rust keyword. The consumer function has the same name as the destructor and the publicity is copied from the trait.
    \item The body of the consumer is set as a match statement, which matches on the transformed \verb|self| parameter. A match arm is then created for each of the variants of the datatype, with all of the fields of the datatype included in the pattern match.
    \item For each generator, the implementation of the consumer method is taken and placed as the body of the match arm. Note at this stage it is possible the return types of these expressions do not match the consumers return type from the signature. These discrepancies are addressed in the third stage.
    \item For the body of each of the match arms, any reference to "self" fields are replaced with simply accessing the field directly from the pattern match attributes. All other references to self are replaced with a reference to the translated "self" type. This is encapsulated in the rules with the rename symbol $[self \mapsto d]$. 
\end{enumerate}

For FP  the following steps are followed

\begin{enumerate}
    \item A destructor is added to the trait for every consumer of the datatype. The types in the signature of the consumer are translated with the rules from \autoref{sig:} and the first attributes is coverted to a self parameter. % TODO try harder
    \item For each struct an implementation block for the trait is added. Each method added to the trait is implemented, copying the signature from the trait declaration. The body of the implementation is copied from the match arm in the equivalent consumer with the pattern of the equivalent constructor. Any reference to the old "self" parameter is renamed to the literal self values and any reference to values which were exposed through the match pattern are updated to use \verb|self|.
\end{enumerate}



\subsection{Other Type Transformations}

There are a many other places where type transformations are required. Firstly all function signatures (excluding the consumers which have already been translated). If let expression have explicit types then these may also need updating. Note there are many other expression types, where explicit types can be assigned in Rust (for example in closures) however these have not be addressed for now.
For these cases the standard typing translation rules, from \autoref{fig} and \autoref{fig} are used.

\subsection{Output}

Throughout the second stage, as the definitions are transformed, gamma is also updated. For example after a datatype is transformed into an interface, the interface name is added to the interface set and removed from the datatype set. Similarly the signatures are updated as required. By the end of this stage, the gamma context is switched. This means the gamma from OOP matches the FP gamma from \autoref{fig:ex-exp-gamma}.

\autoref{fig:ex-exp-stage2} shows the outputs of the second stage of the transformer on the examples from \autoref{fig:ex-exp}. There are two notable issues with this output. Note at the end of this stage the code is invalid. This is because the eval functions do not return the correct type and the function calls to the destructors/consumers have not been updated. This is handled in the third stage of the transformer.

% Add in transformation rules for type signature arguments and return types

% \subsection{OOP to FP}

% The top level function for transforming from OOP to FP is \verb|transform_trait|. This does the following:

% \begin{enumerate}
%     \item  Create an enum variant for each generator of the trait (structs that implements the trait). The enum variants will have the same names as the structs and the same fields. The type of the fields will be transformed such that any \verb|dyn| reference, the dyn is removed. Structs can also expose fields as public with the \verb|pub| token, this is automatically the case for all fields of an enum variant so this is removed.
%     \item Create an enum with the same name as the original trait and the variants collected in step 1.
%     \item Transform each destructor of the trait into a consumer.
% \end{enumerate}

% The first stage to transforming a destructor is transforming its signature. This is done by...

% Assuming each struct impl has an implementation for this destructor (see \autoref{sec:inheritance} for why this is not always the case) the next stage of the transformation is to create a match statement with an arm for each generator of the trait. The arms consist of two key elements, the match pattern and the body. The match pattern is the enum which the generator is transformed to. The current implementation shows all values in the enum. An improvement would be to ignore (using the \verb|..| syntax) fields which are not used the in body.

% The body is a transformed version of the body from the generator implementation of the method. The body is transformed by replacing any reference to self with the new consumer function argument.
% Additional transformations, such as fixing the return type and use of transformed types are handle by the \verb|trasnform_expr| function which is described in detail in \autoref{sec:item-trans}.

\section{Item Transformations}
%TODO Talk about this
\label{sec:item-trans}

The final stage of the transformation, performs two functions. It updates each use of the the transformed user types, including their associated functions. Given the types can become inconsistent as a result of the second stage type transformations, it also updates the type of each expression, ensuring the output program is valid. The transformations are outlined by the rules in \autoref{}.

% TODO EXTEND GAMMA IN THE RULES

During this stage, the whole AST is parsed. For each definition, the sub definitions and expressions are transformed. Equally for each expression the sub expressions are transformed. In each case when transforming a nested expression the expected return type is determined. For example, when transforming the body of a function, the final expression has an expected return type, equal to the return type of the function signature. Note in some case expressions are expected to return nothing, represented as $\bot$ in the syntax, such as a call to the print macro. Equally in cases any type can be returned from an expression. An example is a let expression without a type signature. In the implementation this is handled with an enum with three variants, Any, None and DeltaType. In the formalization this is instead represented by the $EXPLAIN THE TRANS SYMBOL$.

Each time an expression is transformed, after the transformation is complete the type of the output of the expression is checked using the context. If this type does not match the required type, the rules from \autoref{fig:trans-type} are applied. These use the $\sim$, $*$ and $\&$ expressions to update the type of the expression. Applying these type transformations as the final stage of transforming the expression, allow nested expression to also fix the type of the expression. For example if a return expression contains an if statement each arm of the if statement will update its return type, rather than the top level return type doing the transformation. This, in conjunction with type simplifying, reduce the total number of type translation throughout the code, and keeps the calls to the type transformer is a single place in the implementation.

% TODO talk about reference and box expressions and add rules

\subsubsection{Constructors and Generators}

\verb|Thing| and \verb|Thing2| outline the transformations for generators and constructors respectively. In each case fields of the types are assigned to expressions. Each of these are transformed, with the required types obtained from the $\sig$ map in the $\Delta$ context. The call is also updated to used the correct syntax. In Rust structs (generators) are in the global scope, where as by default enum variants (constructors) are nested within enums. For this reason when transforming generators into constructors, the path is extended with the datatype, for example \rust{Dimmer} to \rust{Light::Dimmer}. For FP this transformation is done in reverse, removing the datatype from the path.

Note an alternative approach to this is including enums in the global context, \rust{use Ligth::*}. This brings each enum variant into scope. However this increases the possibility of name collision and is generally considered bad practice. For this reason this syntax was not supported. With some restrictions this syntax could also trivially be supported by the transformer.

\subsubsection{Destructors and Consumers}

For both destructor and consumer transformations, the first step is to move the self parameter. For transforming from OOP to FP, the expression which the method is called on (the reciever) is moved to the first argument of a call to the consumer. Whilst for FP, the first argument is removed and the destructor method is called on that expression. All argument expressions are also transformed, with the return types from the signature in $\Delta$, which will have been updated after the stage 2 transformations.

% \subsection{Rust Type Transformations}

% % TODO how is the required return type determined for an expression?
% % TODO how are types of an expression determined - this should probably go in delta but it should also talk about blocks....
% % TODO how are these types encoded (EType and DeltaType) this should also probably be in Delta context
% % TODO how is the transformation applied
% % TODO type simplification

% Unlike in the Scala implementation of the transformer the signatures of types' associated functions can change during the transformation. In general this is not the type itself that changes, but rather the reference type (such as converting a type to a box of the type). This therefore requires additional steps in the transformation to ensure the types of expressions are correct. To achieve this the \verb|transform_expr| function is also passed an expected return type.

% When recursively transforming nested expression is it the required type of each sub expression must be determined. For each type this is determined as follows:

% \begin{itemize}
%     \item Method/Function call - The signature of the function states the expected type of each of the argument expressions. For a method call receiver, in the current implementation any type can be returned. %TODO evaluate this
%     \item Block - For block expressions, all expression should return None, except the last expression which should have the same return type requested for the whole block.
%     \item Generator/Constructor calls - The required types of each field can be determined by the field types (signature) from gamma.
%     \item Match - For match expression each arm is expected to return the same type as the required type for the whole match statement
%     \item Return - The inner expression should match the required type of the return expression
%     % TODO add support for if else
%     \item TODO add support for if else  
% \end{itemize}

% Given these expected return types, it is then the responsibility of the \verb|transform_expr| function to also transform the types. This is achieved using \verb|transform_expr_type|.  

% In the current implementation, reference types are encoded as a basic enum (either ref, box or none). Given, in general, only the reference type of types change, the \verb|trasnsform_expr_type| can only perform basic transformation on the reference type. The current supported transformations are:

% \begin{itemize}
%     \item Box/Reference $\xrightarrow{}$ None - Achieved by dereference
%     \item None $\xrightarrow{}$ Box - Uses \rust{Box::new} method to create box of provided expression
% \end{itemize}

% The return types express what the given expression should return after it has been transformed. These return types are expressed as one of the following:

% \begin{itemize}
%     \item None - No return value
%     \item DeltaType - Contains a DeltaType struct to encode the expected return type
%     \item Any - Any type can be returned (no transformation of the return type will be made)
% \end{itemize}



% \verb|transform_expr| is the core of this recusrsive transformation process. For each type of expression this transforms the expression it self, as well as any nested experssions.

% The main transformations performed by this function is updating the use of transformed types. For OOP to FP transformations this involves replacing method calls with function calls to the consumers, whilst for FP to OOP it does the reverse. 

% \begin{itemize}
%     \item A consumer/destructor is transformed and the return types changes. E.g. \verb|T| to \verb|Box<T>|. This can have knock on effect where this result is used later.
%     \item A consumer/destructor is transformed and the argument type changes
% \end{itemize}

% The transforming of types is handled in the \verb|transform_expr_type| function. With the PID controller, the robot is not traveling at a constant velocity, but rather accelerating and decelerating constantly to correct the measured velocity. This repeated acceleration and declaration makes this prediction unlikely


\section{Inheritance}
\label{sec:inheritance}

% TODO as metnitioned in \sec oop
Inheritance is one of the central concepts in object oriented programming \cite{cook_palsberg_1989}. It allows developer to share logic between linked classes, preventing duplicate logic. Rust, however, does not support traditional inheritance. Instead it opts for composition, which allows structs to contain other structs directly and default implementations in traits.

A limitation of default implementations is Rust cannot call methods on dynamic types, or use them to create structs.
% TODO explain this more
A clear example of where this is an issue, is the union of sets. Ideally a defualt implementation could be created, which returns a new Union struct with the two provided sets. However, given the Self type could be any set (and therefore it is not Sized), self cannot be used to create a new struct. For this reason the Rust implementation does not support default methods other than basic types.

The FP equivalent to these default trait implementations are wildcard matches in consumers. These allow the consumer to use the same implementation for any unmatched expression.

\paragraph{OOP to FP.} If any impls of a trait which is being transformed are missing any of the required methods, then a wildcard arm should be added for that consumer matching that method. The body of the consumer should be taken from the traits default implementation with the same transformations applied to the other arms. The only difference between the transformations is no additional delta collection is required as the pattern match patter is just the underscore token. 

\paragraph{FP to OOP.} If a consumer contains a wildcard match, then the return type of the arm must be checked. If it returns a simple (Sized) type, then a default implementation of the trait can be created and an implementation in the impl block is not required. However if the return type is not sized then a default implementation cannot be used. In this case a copy of the wildcard expression is created for each of the impl blocks. 
% TODO talk about shared function
% Although it may seem preferable to create a shared function to handle this logic, this is also not possible (withas the type of the 

\chapter{Extensions to the Transformer}

\weixin's transformer focuses on the transformations between Pure OOP style from \cite{cook} and FP style. For the transformer to be a truely useful tool for developers, it must support the majority of styles of programming. 
The requirement to use pure OOP style is extremely restrictive, and often requires large refactoring of code before it can be transformed.
This means the types of OOP programs that can be transformed are limited.

\section{Mutability}

A common style for object oriented programming is to include methods that mutate the state of the class it self. An example is the getter and setter pattern where one functions on a class is responsible for returning a value and another to mutates it, as shown in \autoref{fig:mutable-example}.

In rust, variables are immutable by default. To make a varaible mutable the \rust{mut} token must be used before variable assignment. This can be seen on line 3 of the example in \autoref{fig:mutable-example} where the \verb|mut| token allows the self object to be mutated. This is then used for the implementation of \rust{set_brightness} on line 12 to set the brightness of the Dimmer. 

% Horrible sentence
"Functional programming aims to minimise or eliminate side-effects" \cite{fp-uok}. This can have many benefits such as reducing complexity of programs and making them easier to test. For this reason when transforming this style of OOP programming to FP, the mutations should wrapped within the consumers. This means the consumers should parse in an instance, and a new instance should be returned by the consumer. When the consumer is used, the parsed in object should then be overwritten by the output of the function.  In simple cases this inherently make the transformed function pure by removing the mutation of the argument.

To achieve this kind of transformation, the transfomer check each destructor, checking if the self argument is mutable, if so the following additional steps are added:

\begin{itemize}
    \item Update the arguments of the signature of the destructor, updating the first argument to remove the mutability and borrow. Removing the borrow means the ownership of the passed in variable is taken and thus it will go out of scope after the consumer is called. This is desireable as the variable should be overwritten anyway using the result of the consumer. 
    \item Update the return type of the signature of the destructor. Currently only destructor with no returns are supported. Set the return to trait type that destructor is for.
    \item In the match statement, make each field mutable. This allows reassignment of these local variables.
    \item Replace all field assignments on self with field assignments on these new mutable varaibles. 
    \item Return a new instance of the matched enum with the values of the fields equal to the local variable from the match expression. 
    \item When transforming method calls to this object, ensure to also overwrite the receiver with the output of the method.
\end{itemize}

This has 2 key limitations. Firstly the method cannot return any other values. An extension to the existing transfomrer could be added to support returning tuples.



\begin{figure}
\centering
\rustexample{mutable/oop-basic}
\caption{Mutable class example}
\label{fig:mutable-example}
\end{figure}

\section{Macros}

In Rust macros are a form of "metaprogramming", meaning they allow developers to write code that writes other code. 

Although not unique to Rust, macros play a far greater role in Rust that most other programming languages, and for this reason it is an important topic to cover. 

In Rust there are 2 kinds of macros, declarative and procedural. Macros operate over token trees which contain the tokens parsed by the compiler. SAY SOMETHING BETTER HERE  https://reberhardt.com/cs110l/spring-2020/slides/lecture-17.pdf

Macros are an incredibly powerful tool and are very effective at reducing code duplication. However this flexibility creates many difficulties for the transformer. Given the macros inputs are parsed as tokens, it is not possible to predict what the macro will do to the input. For example, if a macro was created to strip method calls, it would not be possible for the transformer to predict this behaviour. In this case it could there transform a method call to a function and change the resultant meaning of the expression. 
Therefor it is not possible to reliably transform the inputs of macros. As a compromise the current implementation attempts to parse the inputs to any macros as comma separated set of expressions. If this is successful it will transform each expression as before using \verb|transform_expr|. After the transformations it will covert the transformed expressions back to the tokens format.

In many cases this limited approach is sufficient to maintain the correctness of the code and transform the required expressions. Critically this approach allows the use of basic macros included in rust such as \rust{println!} and \rust{vec!}. However as mentioned there is no guarantee of correctness when using these transformations with arbitrary macros.


\section{Generics}

% \subsection{Multiple Inheritance}

% \begin{figure}[t]
% \centering
% foo
% \caption{This is an example figure.}
% \label{fig}
% \end{figure}

% \begin{table}[t]
% \centering
% \begin{tabular}{|cc|c|}
% \hline
% foo      & bar      & baz      \\
% \hline
% $0     $ & $0     $ & $0     $ \\
% $1     $ & $1     $ & $1     $ \\
% $\vdots$ & $\vdots$ & $\vdots$ \\
% $9     $ & $9     $ & $9     $ \\
% \hline
% \end{tabular}
% \caption{This is an example table.}
% \label{tab}
% \end{table}

% \begin{algorithm}[t]
% \For{$i=0$ {\bf upto} $n$}{
%   $t_i \leftarrow 0$\;
% }
% \caption{This is an example algorithm.}
% \label{alg}
% \end{algorithm}
% 
% \begin{lstlisting}[float={t},caption={This is an example listing.},label={lst},language=C]
% for( i = 0; i < n; i++ ) {
%   t[ i ] = 0;
% }
% \end{lstlisting}

% -----------------------------------------------------------------------------

\chapter{Evaluation of the Transformer}
\label{chap:evaluation}

This section will explore some case studies of programs which the tansformer can and cannot transform. It will discuss why certain programs cannot be transformed by the current implementation and determine if possible, how difficult extending the transformer to support these feature would be. It will also compare the Rust implementation of the transformer with the Scala version from \cite{food}.

- First compare with scala
- Then talk about extra things and how wonderful they are
- Then talk about wider pictgure and how good the bad boi si and how to imporve./what cannotbe done  


% TODO talk about testing

\section{Comparison with Scala transformer} 

% TODO is this actually the set example from food?
% TODO explain what the xample is -> its a way to construct expressions
% TODO A subsection of the example form food has been included for the dicussion, however a transfomration of the full example can be found int he appendix.
\autoref{fig:trans-set-ex} demonstrates that the transformer successfully transforms the set example from \cite{food}. 
The outputted code passes a set of unit tests, which checks the functionality remains the same after the transformation.

The input, show in \autoref{fig:trans-set-ex-input} is written in the pure OOP style outlined in \autoref{sec:}. The trait (lines 1-3) is transformed into an enum and each struct is transformed into one of the enum variants. For each of the variants the fileds are successfully transformed with the dyn token removed. For each of the trait functions a consumer has been created with correspoding match expressions with the body as a transformed version from the struct implementation block.

Each use of the type is also transformed, meaning destructor calls are replaced with the \rust{eval} consumer, both in the match statements and in the demo function.

% TODO In reverse... FP to OOP

\subsection{Visibility}

% TODO summarise what visibility is
The expression example also demonstrates that the visibility on the inputs is maintained where possible. The visitiblity of the enum is taken directly from the trait. Enum variants inherently match the publicity of the enum, this means the visibility of the individual structs cannot be maintained. Equally in a trait the publicity of the items cannot be controller. Therefore visibility of the consumers is also taken directly from the trait. The visibility of other definitions (such as the \rust{demo} function) are unchanged.
% TODO include a not pub example somewhere so this actually makes sense

\subsection{Typing}

Similarly the typing of the outputted code is valid. The notible transfomrations are:

On line 20 the call to the eval consumer, the first argument is successfully converted from a box to a reference using firstly the \rust{*} operator, to dereference the box and then using the reference \rust{&} expression, to create a reference of the value.

In the eval consumer, the match expression is on a borrow of the expression. This means the fields exposed in the match pattern are also borrows. This is why the value (\rust{n}) must be dereferenced on line 8, and why they can be used directly for the calls to \rust{eval} on line 11.

Although the output typing is correct, it is clear in the demo function the typing could be simplified. On line 16 a Box of an expression is defined. This expression is then dereferenced (using the \rust{*} operator) before each use. This demo function could therefore be simplified by removing the box and the dereferences throughout. In this simple example this approach could work however if the box expression was later used as a box there is no guarantee that this would be simplest solution. Although not address in this paper, a metric could be determined for the "optimal" typing solution, for example based on the number of operations applied. This could then be used to find a simpler solution.

% TODO Talk about the wild card in the set example

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustexample{exp/oop}
    \caption{Input OOP program to transformer}
    \label{fig:trans-set-ex-input}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustoutput{exp/oop}
    \caption{Output FP program from transformer}
    \label{fig:trans-set-ex-output}
\end{subfigure}
\caption{Set transformation example}
\label{fig:trans-set-ex}
\end{figure}

\subsection{Macros}

The Expression example also demonstrates the successful transformation of macros arguments. However as mentioned in the implementation section \autoref{sec:}, this approach is very limited and relies on both the arguments to the macro being an expression and the result of the expression being used explicitly, rather than manipulating the expression it self.

Improvements to this approach could definitely be made, however a generalized solution for transforming macros is not possible given their flexibility. Going forward some macros would require custom transformations, which would have to be written by the developers before full transformations could be performed. 

\subsection{Inheritance}

% TODO add in example
% TODO xplain the example
In order to demonstrate the inheritance functionality, sections of the transformation of a Shape are included in \autoref{fig:}. The full example can be found in the \autoref{appx:}.

The trait includes two methods, one with a default implementation and one without. The method without the default implementation is handled as before, creating a consumer with a match for each struct with an implementation of the trait. The \rust{internal_angle} function, however is handled differently. Given the circle method overrides the default implementation, this is handled the same as before with a match arm created for it. In contrast, the triangle class, which does not override the method, does not have a match arm. This type is instead caught by the wildcard pattern which contains the transformed block from the trait default implementation.   

% TODO explian this waaay better
Note, the example default implementation returns an primative type, i32. Currently the transformer does not support returning the Self type of the trait as mentioned in \autoref{sec:}. This is beacuse the return type must be sized, and there is no guarantee Self type will be sized. 

Limited support could be added, life times or somethng...

\section{Additional features} 

\subsection{Mutability}

\autoref{fig:} shows the output of the transformation of the example discussed in \autoref{fig:}. This demonstrates how the mutability can be wrapped inside the constructor, such that the method returns a new instance of the class.

This demonstrates that the transformer can be extended to support multiple different styles of input. However currently the transformer only supports a single outputs style. This means if this style is transfomred from OOP to FP and back again, it will be transfomred in the pure OOP style outlined in \autoref{sec:}.
In the future it would be feasible for the transfomrer to support different outputs formats and styles. This would allow developers to not only select the paradigm by also the style within that paradigm to use.

Despite this the current implementation is very limited. The destructors cannot have ant return values, and the created implementations are not optimatal. \autoref{fig:} shows an ideal output, with the mutable variables removed. Although not in scope of this transformer these method bodies may be possible to simplify in the future.

\subsection{Generics}

\section{Wider picture}

Despite the improvements included in the Rust implementation of the transformer, there are still many limitations that prevent the tool from being widely useful in its current state. Expecting developers to transform sections of code before using the transformer removes the real benifits of the tool in a paractical setting.

One key limitation of the tool is the lack of support for different expressions. The current tool support a small subsect of possible expression in Rust which means many programs would be using unsupported feature. Examples of this include unnamed enums, and certain binary expressions. However, in most cases these missing feature can simply be added to the tool with minimal effort.

\subsection{Nested pattern matching and guards}

Similarly to \cite{food}, another key limitation is not supporting nested pattern matching. Nested pattern matching is a very useful tool in the functional decomposition style. Example

Nester pattern matching can also support matching on mutliple objects at the same time, as this is simply nested pattern matching on a tuple. Example 


% {\bf A topic-specific chapter} 
% \vspace{1cm} 

% \noindent
% This chapter is intended to evaluate what you did.  The content is highly 
% topic-specific, but for many projects will have flavours of the following:

% \begin{enumerate}
% \item functional  testing, including analysis and explanation of failure 
%       cases,
% \item behavioural testing, often including analysis of any results that 
%       draw some form of conclusion wrt. the aims and objectives,
%       and
% \item evaluation of options and decisions within the project, and/or a
%       comparison with alternatives.
% \end{enumerate}

% \noindent
% This chapter often acts to differentiate project quality: even if the work
% completed is of a high technical quality, critical yet objective evaluation 
% and comparison of the outcomes is crucial.  In essence, the reader wants to
% learn something, so the worst examples amount to simple statements of fact 
% (e.g., ``graph X shows the result is Y''); the best examples are analytical 
% and exploratory (e.g., ``graph X shows the result is Y, which means Z; this 
% contradicts [1], which may be because I use a different assumption'').  As 
% such, both positive {\em and} negative outcomes are valid {\em if} presented 
% in a suitable manner.

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

The Rust transformer demonstrates the feasibility of using the framework outlined by \weixin in \cite{food}, with a different languages. It shows that, the fundamental features from the transformer can be mapped to the rust language, with transformation of datattype, traits and their associated functions all possible. 

However to achieve this additional cosiderations are required. Primarily, Rust low level type system, introduces many additional challenges not addressed in the \cite{food} transformer. This requires the transformer to have a greater degree of type inference, maintaining the reference type as well as type name.

Rust was selected for this project as many multiparadigm languages exist with greater level of abstractions, whilst rust reamins low level and has a unique typing system. With this there are many differences from Scala. This allows Rust to demonstate the limitation in the generalization provided by 
\weixin. It is worth multi-paradigm langauges more similar to scala, may pose fewer issue in the implementation.

This paper also demostrates that the transfomrer can be extended to support addiitonal styles. Althrough limited in the current implemnetation, this could be extended in the future to support a variety of input and output formats.

There is still a large amount of work reuired for the tool to become a practical tool for developer. The basic mutability support and addiiton of generics in the paper is a step in the right direction to support more styles of input, however with the vast input spaces there are many feature which remain unsupported.

% \noindent
% The concluding chapter of a dissertation is often underutilised because it 
% is too often left too close to the deadline: it is important to allocate
% enough attention to it.  Ideally, the chapter will consist of three parts:

% \begin{enumerate}
% \item (Re)summarise the main contributions and achievements, in essence
%       summing up the content.
% \item Clearly state the current project status (e.g., ``X is working, Y 
%       is not'') and evaluate what has been achieved with respect to the 
%       initial aims and objectives (e.g., ``I completed aim X outlined 
%       previously, the evidence for this is within Chapter Y'').  There 
%       is no problem including aims which were not completed, but it is 
%       important to evaluate and/or justify why this is the case.
% \item Outline any open problems or future plans.  Rather than treat this
%       only as an exercise in what you {\em could} have done given more 
%       time, try to focus on any unexplored options or interesting outcomes
%       (e.g., ``my experiment for X gave counter-intuitive results, this 
%       could be because Y and would form an interesting area for further 
%       study'' or ``users found feature Z of my software difficult to use,
%       which is obvious in hindsight but not during at design stage; to 
%       resolve this, I could clearly apply the technique of Smith [7]'').
% \end{enumerate}

% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a databased) then inported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.
%
% We would recommend using BiBTeX, since it guarantees a consistent referencing style 
% and since many sites (such as dblp) provide references in BiBTeX format. 
% However, note that by default, BiBTeX will ignore capital letters in article titles 
% to ensure consistency of style. This can lead to e.g. "NP-completeness" becoming
% "np-completeness". To avoid this, make sure any capital letters you want to preserve
% are enclosed in braces in the .bib, e.g. "{NP}-completeness".

\backmatter

\bibliography{dissertation}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendicies; these are 
% the same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

% \chapter{An Example Appendix}
% \label{appx:example}

% Content which is not central to, but may enhance the dissertation can be 
% included in one or more appendices; examples include, but are not limited
% to

% \begin{itemize}
% \item lengthy mathematical proofs, numerical or graphical results which 
%       are summarised in the main body,
% \item sample or example calculations, 
%       and
% \item results of user studies or questionnaires.
% \end{itemize}

% \noindent
% Note that in line with most research conferences, the marking panel is not
% obliged to read such appendices. The point of including them is to serve as
% an additional reference if and only if the marker needs it in order to check
% something in the main text. For example, the marker might check a program listing 
% in an appendix if they think the description in the main dissertation is ambiguous.

% =============================================================================

\end{document}
