% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the academic year, say 2013/14 say).
                 

\documentclass[ oneside,% the name of the author
                    author={James Elgar},
                % the degree programme: BSc, MEng, MSci or MSc.
                    degree={MEng},
                % the dissertation    title (which cannot be blank)
                     title={Bidirectional transformer between functional and \\ object-oriented programming in Rust},
                % the dissertation subtitle (which can    be blank)
                  subtitle={}]{dissertation}
                  
% Commands
\usepackage{hyperref}
\newcommand{\weixin}{Zhang et al }

\usepackage{listings}
\usepackage{xcolor}

\usepackage{mathtools}

% \usepackage{minted}
\usepackage[outputdir=../]{minted}
\newcommand{\rust}[1]{\mintinline{rust}{#1}}

\newcommand{\codefile}[2]{\inputminted[xleftmargin=20pt,linenos, breaklines]{#1}{#2}}
\newcommand{\codefileLN}[4]{\inputminted[xleftmargin=20pt,linenos, breaklines, firstline=#3, lastline=#4]{#1}{#2}}
\newcommand{\rustsnippet}[1]{\codefile{rust}{snippets/#1.rs}}
\newcommand{\rustfile}[1]{\codefile{rust}{../#1.rs}}
\newcommand{\rustexample}[1]{\rustfile{examples/src/#1}}
\newcommand{\rustoutput}[1]{\rustfile{outputs/src/#1}}

\newcommand{\rustfileLN}[3]{\codefileLN{rust}{../#1.rs}{#2}{#3}}
\newcommand{\rustexampleLN}[3]{\rustfileLN{examples/src/#1}{#2}{#3}}

\usepackage{xcolor} % to access the named colour LightGray
\definecolor{LightGray}{gray}{0.9}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{soul}

\usepackage{xspace}
\usepackage{listings}
\usepackage{xypic}
\usepackage{mathpartir}
\usepackage{stmaryrd}
\usepackage{longtable}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{enumitem}
\usepackage{mathtools}

\input{report/syntax}

\begin{document}

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter


%\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Abstract}

% TODO I think is should really be in the intro, focus more on what actually happens
There are benefits to using both object orient and functional programming. With modern programming languages adopting traditional functional feature, developer often have the choice between the two paradigms. However once a paradigm is selected switching can be a time consuming process. In \cite{food} \weixin proposes a framework for transforming between these decomposition styles. This paper explorers the generality of this approach through an implementation of the transformer in rust. 

The key contributions of this paper are:

\begin{quote}
\noindent
\begin{itemize}
    \item Created an implementation of the \cite{food} transformer in rust
    \item Extend the \cite{food} transformer to support a basic implementation of the rust type system
    \item Explored extensions to the transformer including generics and mutability
\end{itemize}
\end{quote}

% {\bf A compulsory section, of at most 300 words} 
% \vspace{1cm} 

% ---- Reference/Example ---- %
% \noindent
% This section should pr\'{e}cis the project context, aims and objectives,
% and main contributions (e.g., deliverables) and achievements; the same 
% section may be called an abstract elsewhere.  The goal is to ensure the 
% reader is clear about what the topic is, what you have done within this 
% topic, {\em and} what your view of the outcome is.

% The former aspects should be guided by your specification: essentially 
% this section is a (very) short version of what is typically the first 
% chapter. If your project is experimental in nature, this should include 
% a clear research hypothesis.  This will obviously differ significantly
% for each project, but an example might be as follows:

% \begin{quote}
% My research hypothesis is that a suitable genetic algorithm will yield
% more accurate results (when applied to the standard ACME data set) than 
% the algorithm proposed by Jones and Smith, while also executing in less
% time.
% \end{quote}

% \noindent
% The latter aspects should (ideally) be presented as a concise, factual 
% bullet point list.  Again the points will differ for each project, but 
% an might be as follows:

% \begin{quote}
% \noindent
% \begin{itemize}
% \item I spent $120$ hours collecting material on and learning about the 
%       Java garbage-collection sub-system. 
% \item I wrote a total of $5000$ lines of source code, comprising a Linux 
%       device driver for a robot (in C) and a GUI (in Java) that is 
%       used to control it.
% \item I designed a new algorithm for computing the non-linear mapping 
%       from A-space to B-space using a genetic algorithm, see page $17$.
% \item I implemented a version of the algorithm proposed by Jones and 
%       Smith in [6], see page $12$, corrected a mistake in it, and 
%       compared the results with several alternatives.
% \end{itemize}
% \end{quote}

% -----------------------------------------------------------------------------


\chapter*{Dedication and Acknowledgements}

{\bf A compulsory section}
\vspace{1cm} 

\noindent
It is common practice (although totally optional) to acknowledge any
third-party advice, contribution or influence you have found useful
during your work.  Examples include support from friends or family, 
the input of your Supervisor and/or Advisor, external organisations 
or persons who  have supplied resources of some kind (e.g., funding, 
advice or time), and so on.

% -----------------------------------------------------------------------------

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl



% -----------------------------------------------------------------------------

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures and tables.  These are all compulsory parts of the dissertation.

\tableofcontents
\listoffigures
\listoftables

% -----------------------------------------------------------------------------



\chapter*{Ethics Statement}

This project did not require ethical review, as determined by my supervisor, [fill in name].

% -----------------------------------------------------------------------------

% \noindent
% This section should present a detailed summary, in bullet point form, 
% of any third-party resources (e.g., hardware and software components) 
% used during the project.  Use of such resources is always perfectly 
% acceptable: the goal of this section is simply to be clear about how
% and where they are used, so that a clear assessment of your work can
% result.  The content can focus on the project topic itself (rather,
% for example, than including ``I used \mbox{\LaTeX} to prepare my 
% dissertation''); an example is as follows:

\chapter*{Supporting Technologies}

\noindent
The following technologies were used in the implementation of the Rust transformer To implement the transformer I used the following technologies:

\begin{quote}
\noindent
\begin{itemize}
\item I used the Rust programming language to implement the transformer. \url{https://www.rust-lang.org/}
\item I used the \verb|syn| package to create and parse the AST. \url{https://docs.rs/syn/latest/syn/}
\item I used the \verb|quote| package to convert the transformed AST back to rust code. \url{https://docs.rs/quote/latest/quote/}
\item I used \verb|rustfmt| to format the transformed output code
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

% {\bf An optional section}
% \vspace{1cm} 

\noindent
% Any well written document will introduce notation and acronyms before
% their use, {\em even if} they are standard in some way: this ensures 
% any reader can understand the resulting self-contained content.  

% Said introduction can exist within the dissertation itself, wherever 
% that is appropriate.  For an acronym, this is typically achieved at 
% the first point of use via ``Advanced Encryption Standard (AES)'' or 
% similar, noting the capitalisation of relevant letters.  However, it 
% can be useful to include an additional, dedicated list at the start 
% of the dissertation; the advantage of doing so is that you cannot 
% mistakenly use an acronym before defining it.  A limited example is 
% as follows:

\begin{quote}
\noindent
\begin{tabular}{lcl}
AST                 &:     & Abstract Syntax Tree                                        \\
OOP                 &:     & Object Oriented Programming                                 \\
FP                  &:     & Functional Programming                                      \\
FOOD                &:     & Functional and Object Oriented Decomposition                \\
DST                 &:     & Dynamically Sized Type                                      \\
\end{tabular}
\end{quote}


% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter


\chapter{Introduction}
\label{chap:context}

% {\bf Unlike the frontmatter up to and including the Summary of Changes, which you should not deviate from, Chapters 1--5 represent a suggested outline only. This outline will only be appropriate for a specific type of project. You should talk with your supervisor about the best way to structure your own dissertation, but ultimately the choice is yours. However, almost every project will want to include the content discussed in these chapters in some way. For more advice on structuring your dissertation, see the unit handbook.}
% \vspace{1cm} 

% \noindent
% This chapter should introduce the project context and motivate each of the proposed aims and objectives.  Ideally, it is written at a fairly high-level, and easily understood by a reader who is technically competent but not an expert in the topic itself.

% In short, the goal is to answer three questions for the reader.  First, what is the project topic, or problem being investigated?  Second, why is the topic important, or rather why should the reader care about it?  For example, why there is a need for this project (e.g., lack of similar software or deficiency in existing software), who will benefit from the project and in what way (e.g., end-users, or software developers) what work does the project build on and why is the selected approach either important and/or interesting (e.g., fills a gap in literature, applies results from another field to a new problem).  Finally, what are the central challenges involved and why are they significant? 
 
% The chapter should conclude with a concise bullet point list that summarises the aims and objectives.  For example:

% \begin{quote}
% \noindent
% The high-level objective of this project is to reduce the performance 
% gap between hardware and software implementations of modular arithmetic.  
% More specifically, the concrete aims are:

% \begin{enumerate}
% \item Research and survey literature on public-key cryptography and
%       identify the state of the art in exponentiation algorithms.
% \item Improve the state of the art algorithm so that it can be used
%       in an effective and flexible way on constrained devices.
% \item Implement a framework for describing exponentiation algorithms
%       and populate it with suitable examples from the literature on 
%       an ARM7 platform.
% \item Use the framework to perform a study of algorithm performance
%       in terms of time and space, and show the proposed improvements
%       are worthwhile.
% \end{enumerate}
% \end{quote}

\noindent
With the rapid adoption of functional and object oriented features in modern programming languages, developers now have the choice of which paradigm to use in different situations. Both paradigms come with distinct advantages and disadvantages, and such the correct choice for a given feature can often be a tricky decision. In any case a "correct" decision may not be possible as both paradigms could have different values in the same feature. If a developer makes a choice they later regret, refactoring to another paradigm can be an extremely slow process.

Therefore, to truly harness the value of both paradigms, in \cite{food} \weixin explores the idea of a transformer, which allows developer to automatically transform between these two paradigms. This allows developers to instantly switch the style of there code depending on the task.

\weixin proposes a framework to transform between these two decomposition styles and create an implementation in Scala to demo its functionality.

The goal of this paper is to replicate this implementation in Rust. Rust was created by the Mozilla foundation with the intention of replacing C++ in their stack. It has a strong focus on performance and safety which it achieves partly through its borrow checker. With Rust being a low level language it exposes concepts to the developer which are not available in Scala, such as references and borrows. This requires extension to the existing transformer to allow it to transform with these concepts. 



% -----------------------------------------------------------------------------

% \noindent
% This chapter is intended to describe the background on which execution of the project depends. This may be a technical or a contextual background, or both. The goal is to provide a detailed explanation of the specific problem at hand, and existing work that is relevant (e.g., an existing algorithm that you use, alternative solutions proposed, supporting technologies).  

% Per the same advice in the handbook, note there is a subtly difference from this and a full-blown literature review (or survey).  The latter might try to capture and organise (e.g., categorise somehow) \emph{all} related work, potentially offering meta-analysis, whereas here the goal is simple to ensure the dissertation is self-contained.  Put another way, after reading this chapter a non-expert reader should have obtained enough background to understand what \emph{you} have done (by reading subsequent sections), then accurately assess your work against existing relevant related work.  You might view an additional goal as giving the reader confidence that you are able to absorb, understand and clearly communicate highly technical material and to situate your work within existing literature.

\chapter{Background}
\label{chap:technical}

In \cite{warburton}, Warburton discusses the on going evolution of modern languages focusing on their adoptions of functional and object oriented features. He states, object oriented and functional features were both introduced as improvements to more traditional procedural (imperative) languages in the 1980s. In these early days languages inherently had a combination of both paradigms. For example LISP, a functional language, had the CLOS (common LISP object system), and Smalltalk, an object oriented language, had lambda expression and operations equivalent to map, reduce and filter.

Between the 1990s and the 2000s this mulit-paradigm trend died down as Object-oriented programming became the dominant style with popular languages such as Java and C++.

However, since then the popularity of multi-paradigm features has risen again. Traditionally object oriented languages (including C++ and Java) are adding in functional features, whilst new languages such as Rust and Go are created with multi-paradigm features.

Warburton concludes with "The future is hybrid: pick the best features and ideas from both
functional and object-oriented approaches in order to solve the
problem at hand." \cite{warburton}. This statement is the motivation for this paper. Programmers are increasingly working within multi-paradigm projects and are expected to pick the "best" features from a paradigm depending on their task at hand. Selecting the "wrong" style or if requirements change, can be a costly as refactoring between styles is a time consuming process.

With this it is clear there is value in a tool which can automatically switch between features in these paradigms. The work of \weixin \cite{food}, demonstrates, that practical equivalence can be drawn between certain features and as such an automatic transformer, in some capacity, can be created. The goal of this paper, is to both replicate the functionality of that paper in a different language, but also to extend the transformer to support a wider range of input programs.

\section{Decomposition}

% "There are important design tradeoffs between functional and object-oriented decom-
% positions in terms of extensibility and expressiveness. As acknowledged by the notorious
% Expression Problem [18, 7, 23], these two decomposition styles are complementary in terms
% of extensibility."

Both object oriented and functional programming are approaches to decomposition, which is the process of breaking a complex problem down into less complex sub problems. The primary difference between these two paradigms, is how data abstraction is achieved.

\subsection{Object Oriented Decomposition}

Object oriented decomposition achieves data abstraction through the creation of interfaces. This is an "operation first" \cite{food} approach as these interfaces describe the operations that can be applied to the type but not the data within the types.
Throughout this paper these operations are referred to as "destructors" as they "tear down" an object into another type.
Throughout \cite{food} \weixin uses Pure OOP from \cite{cook} as the style of object oriented decomposition. In Pure OOP, these interfaces are types rather than classes.
In this style, classes are "object generators" \cite{food} meaning they create objects which satisfy a specified interface.

Although Rust's OOP features do not map directly to Pure OOP style, equivalence between the styles can be drawn. In Rust interfaces are defined as \rust{trait}s. Lines 3-6 of \autoref{fig:dec-ex-oop} demonstrate the definition of a trait called \rust{Shape}. \rust{Shape} defines two destructors, \rust{area} and \rust{perimeter}. 

In Rust the concept of classes are split into two components, \rust{struct} and \rust{impl}. A \rust{struct} defines a collection of attributes (the data), whilst the \rust{impl} block can implement a given trait on the struct (the operations). In Rust a \rust{struct} can implement multiple traits. \rust{struct}s are also treated a types them selves, meaning methods can be defined directly on them (using an \rust{impl} block with no \rust{for} trait). Despite this a \rust{struct} which implements a single \rust{trait} is equivalent to a Pure OOP class. 
Lines 8-19 demonstrte how a Pure OOP style class, which implements the \rust{Shape} interface, can be created in Rust. 
Lines 8-10 create the \rust{Circle} \rust{struct} with a single attribute \rust{radius}. Lines 11-19, then implement the \rust{Circle} \rust{trait}. In the implementation a definition for each of the required methods in included. 
Lines 21-32 demonstrate another implementation of a Pure OOP class. Therefore in this example the shape interface has two generators, \rust{Circle} and \rust{Square}. These generators can then be used to create an instance of a \rust{Shape} with the following syntax \rust{Circle{radius: 12.0}}.

% This section will describe the key components of this style and their equivalence in Rust. \autoref{fig:dec-ex-oop} will be used throughout as an example of a basic object oriented program in Rust. 

\subsubsection{Inheritance}
\label{sec:background-inheritance}

Inheritance is one of the central concepts in object oriented programming \cite{cook_palsberg_1989}. It allows developers to share logic between linked classes, preventing duplication. 
However Rust has very limited support for traditional inheritance. Instead Rust opts for composition, which enables structs to reference other objects (structs and traits).
Rust also has support for default implementations. These are definitions of destructors directly in the trait, meaning structs that then implement the method do not have to provide a definition. In this case if a definition is provided in an impl block, that definition is used other wise the default implementation is used.
An example is provided in \autoref{fig:inheritance-example-oop}. In this example the \rust{Shape} trait, defines two operations \rust{side_count} with no default implementation and \rust{internal_angle}, which provides a default implementation.
Two structs then implement this trait, \rust{Circle} and \rust{Square}. Both provide an implementation for the \rust{side_count}, method as required (as there is no default), whilst only \rust{Circle} provides an implementation for the \rust{internal_angle} method.
As a result, calling \rust{internal_angle} on a \rust{Circle} object uses the overridden definition (returning 0) whilst calling that method on \rust{Triangle} will use the default implementation.

\subsubsection{Self Reference}
\label{sec:self-reference}

When defining a destructors on a trait, an optional argument \rust{self} can be included as the first argument in the signature. 
This argument has type \rust{Self} and allows self-reference within the destructor implementation.
With in an \rust{impl} block for a \rust{struct}, the \rust{Self} type is equivalent to the struct, this therefore allows the method to access both attributes and operations on the object the method is called on.
Note, in default implementations (on traits) the concrete type is unknown. Therefore, the default implementation will not be able to access the fields of \rust{self} and instead only be able to access the destructors.  

In Rust if the self argument is not included in the method it referred to as a "associated function" as it no longer has access to an instance of the type, and is equivalent to a top level function.

\subsection{Functional Decomposition}

In contrast to object oriented decomposition, the first stage of data abstraction for functional decomposition is the creation of datatypes.
These data types specify the attributes of each variant, hence functional decomposition is described as "data first" \cite{food}. 
Similarly to the OOP style, \weixin uses a very restricted form of functional programming for defining the transformer.  
In this style consumers of datatypes are defined with pattern matches on the type.

In Rust \rust{enum}s are equivalent to datatypes. Lines 3-6 of \autoref{fig:dec-ex-fp} demonstrate the creation of an \rust{enum} with two variants \rust{Circle} and \rust{Square}. Note how the attributes of these variants (radius for circle and side for square) are defined directly in the datatype.

In Rust consumers are top level functions, which take a datatype as the first argument. They then contain a single match statement on the datatype, with an arm for each of its variants. In \autoref{fig:dec-ex-fp} both \rust{area} and \rust{perimeter} are consumers of the \rust{Shape} datatype. 
They both pattern match on the parameter of type shape, defining responses for each variant. Each arm of the match statement, such \rust{Shape::Square { side } => side * side}, contains a pattern which destruct the type of the parameter, giving access to the attributes of the provided \rust{Shape} variant.

\subsubsection{Wildcard}

In a similar vein to inheritance in OOP, match statements support the wildcard pattern, \rust{_ =>}. This will match any pattern that has not yet been matched by a match arm. This is demonstrated on line 11 in \autoref{fig:inheritance-example-fp}. Here the shape will match circles for the first arm of the match statement, but for any other shape variant will use the wildcard expression.
Similarly if all variants of the datatype have the same implementation, (i.e. the match statement only requires a single wild card match) no match statement is required. 

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustexample{shape/oop}
    \caption{Object Oriented Programming - Implementation of trait for structs}
    \label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustexample{shape/fp}
    \caption{Functional Programming - Datatype with two consumers}
    \label{fig:dec-ex-fp}
\end{subfigure}
\caption{Rust Decomposition Examples}
\label{fig:dec-ex}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustexample{shape2/oop}
    \caption{Object Oriented Programming - Default implementation}
    \label{fig:inheritance-example-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustexample{shape2/fp}
    \caption{Functional Programming - Wildcard pattern}
    \label{fig:inheritance-example-fp}
\end{subfigure}
\caption{Rust Decomposition Examples}
\label{fig:inheritance-example}
\end{figure}

\section{Decomposition without Regrets}
% Following on from this dudes work with rust

In "Decomposition without Regrets" \cite{food}, \weixin proposes a framework for a bidirectional transformer between these two decomposition styles. \weixin introduces a syntax called "FOOD calculus", which is heavily based on Scala. Using this syntax, \weixin defines a set of type-directed transformation rules, to formalise transformations between Cook's pure OOP decomposition style, outlined in \cite{food} and functional decomposition style.

These rules outline the fundamental transformations required to convert between the two styles within FOOD calculus, including transforming the top level types (datatype and traits) and their associated functions (generators, constructors, destructors and consumers).

% \weixin uses type directed rules to formalize the transformation to perform.

\weixin creates an implementation of this transformer in Scala and demonstrates the mapping between the theoretical rules in FOOD, and practical examples in Scala.
\weixin claims that the transformation rules are not Scala specific and can therefore be applied to other multi-paradigm languages. The primary goal of this paper is to replicate the work of \weixin's Scala transformer in another programming language, Rust, in order to validate this claim.



\section{Rust}

% Both Rust and Scala offer high level safety guarantees using the typing system.
% Rust 
Unlike Scala, Rust's API offers direct control over resource management, much like low-level languages such as C. However in order to maintain the safety guarantees, Rust approaches this control using a type system based on the principles of ownership and borrowing. For features that cannot be implemented with in Rust's strict type system, rust uses the concept of \rust{unsafe} code. This allows developer to directly use raw pointers and thus make use of shared mutable state. For these cases Rust's embraces the concept of "unsafe code encapsulated within safe APIs" \cite{rustsafesystems}.
The unique and strong type system of Rust presents additional challenges when creating a transformer for the rust programming language.

\subsection{Ownership, Borrows and References}

In system level programming languages such as C, developers have direct control over memory management. With this developers can create shared mutable state, using pointers to reference objects in memory. With this control, there is an inherently lack of safety. Developers are able to create references to objects which are then later mutated and such the pointer becomes invalid.

In order to maintain safety guarantees, programming languages have developed ways to obfuscate this memory management. High level languages such as Scala, often implement a garbage collector. This is a separate process which periodically frees any memory that can no longer be accessed by the program. This approach, however, comes at a performance cost as the garbage collector is repeatedly having to run through the program in order to detect memory which can be freed.

\subsubsection{Ownership}

In order to maintain control for the developers, instead of using a garbage collector, Rust opts for the principal of ownership. Rust wraps the use of raw pointers in \rust{unsafe} blocks. Outside of these blocks Rust enforces the strict "ownership" system.

The key principal of ownership is every variable in Rust has a single owner. Once the owner of a variable goes out of scope, the drop operation is run, which generally means the memory is freed. Ownership of a variable can be transferred by either, reassigning to a new variable, returning from a function or passing directly into a function. Note for types which implement the Copy trait (there are type which have a fixed size such as primitive types) these operations actually copy the values instead of changing ownership.

\subsubsection{References}

References provide a way to refer to a value without taking ownership of that value. A reference can be created with the \rust{&} symbol. For example:

\begin{minted}{rust}
let a = String::from("Hello world");
let b = &a;
\end{minted}

By default references are immutable, so in this example \rust{b} is an immutable reference of the variable \rust{a}.
There is no limit on the number of immutable references which can be created for a value. Mutable references however allow direct mutation of the underlying value and can be created with \rust{&mut}. Therefore for a value, only a single mutable reference can be created at one time. Moreover if a mutable reference exits, no immutable references can exits. This approach allows Rust to prevent the creation of dangling references as compile time and thus without impact on performance.

\subsubsection{Smart Pointers}

Rust also has a set of Smart pointers, these are data structures which "act like pointers" but include additional functionality. This paper currently only support the Box smart pointer. This pointer, allows the developer to assign a value on the heap. The Box is stored on the stack and stores a pointer to the value on the heap. Note the box is the owner of the data on the heap.
% Talk aobut smart pointers

\subsubsection{Dereferencing}

In Rust the "indirection", \rust{*}, operator can be used to access the value within a reference or pointer. 

A convenient feature in rust is the \rust{.} operator will automatically reference or dereference a value where possible. For example when using method with signature \rust{fn double(&self)} or \rust{fn double(self: Box<Self>)} an owned values of type Self can be called and the reference/box will be created.
% Talk about dereferencing
% Talk about clone 
% Talk about how the dot operator auto references or dereferences a thing

% DYN%

\subsection{Sized types}
\label{sec:dyn}

% In Rust types must have fixed size, this is both in signatures and 

In Rust there are two kinds of types, static and dynamic. In the majority of cases statically sized types are required, so that Rust can know how to assign and use the type. However these restrictions have some significant side effects. Firstly when defining a type (enum or datatype) in Rust, if it contains a reference to itself, it inherently cannot be sized. This is because an instance of the type could theoretically have infinite nesting of itself, and hence have unbound sized. To overcome this, recursive types, (a type which contains a value which is/has a value of itself), must be defined using a pointer. Pointers have a fixed sized, thus wrapping any nested value of the parent type with a pointer enforces the sized constraint.

This creates a significant deviation from Scala, where recursive types can be implemented without explicit indirection (pointers). To demonstrate this \autoref{fig:recursive-typedef}, shows the implementation of a recursive type in both Rust and Scala. In this example the box smart pointer has been used, although other pointer types should be analogous (with some restrictions).

Traits are one of the few examples in Rust where a dynamic types are required. In Pure OOP interfaces (which are mapped to traits in Rust) are types. However using a trait as a type directly in Rust poses two issues. Firstly traits are inherently unsized, as any number of differently sized structs can implement them. Secondly each implementation of the trait can have different implementations of the required methods. When using a trait as a type directly, the concrete type (generator) is unknown and hence the associated implementations of the methods are also unknown.

In Rust, any dynamically sized type must exist behind a pointer. Moreover to address the unknown concrete type of traits, Rust has "trait objects" \cite{rustonomicon}. Trait objects, represent types which implement the trait and where the concrete type is unknown. However they also store a reference to a "vtable", which stores additional information on how to use the object, most notably references to the implementations of each trait method. Combining these two factors, means whenever a trait is used as a type, it must be behind as a dynamic pointer or reference.

Note, as mentioned in \autoref{sec:self-reference}, the \rust{Self} type in destructors, refers to the concrete type (the struct) rather than the trait. For this reason in trait definitions, \rust{self} does not require a dynamic pointer.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \codefile{scala}{../scala/examples/src/main/scala/RecursiveType.scala}
    \caption{Scala recursive class}
    \label{fig:recursive-typedef-scala}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustexample{list/oop}
    \caption{Rust recursive struct}
    \label{fig:recursive-typedef-rust}
\end{subfigure}
\caption{Recursive Type Definitions}
\label{fig:recursive-typedef}
\end{figure}


% -----------------------------------------------------------------------------

% \noindent
% This chapter is intended to describe what you did: the goal is to explain
% the main activity or activities, of any type, which constituted your work 
% during the project.  The content is highly topic-specific, but for many 
% projects it will make sense to split the chapter into two sections: one 
% will discuss the design of something (e.g., some hardware or software, or 
% an algorithm, or experiment), including any rationale or decisions made, 
% and the other will discuss how this design was realised via some form of 
% implementation.

% This is, of course, far from ideal for {\em many} project topics.  Some
% situations which clearly require a different approach include:

% \begin{itemize}
% \item In a project where asymptotic analysis of some algorithm is the goal,
%       there is no real ``design and implementation'' in a traditional sense
%       even though the activity of analysis is clearly within the remit of
%       this chapter.
% \item In a project where analysis of some results is as major, or a more
%       major goal than the implementation that produced them, it might be
%       sensible to merge this chapter with the next one: the main activity 
%       is such that discussion of the results cannot be viewed separately.
% \end{itemize}

% \noindent
% Note that it is common to include evidence of ``best practice'' project 
% management (e.g., use of version control, choice of programming language 
% and so on).  Rather than simply a rote list, make sure any such content 
% is useful and/or informative in some way: for example, if there was a 
% decision to be made then explain the trade-offs and implications 
% involved.


\chapter{Project Execution}
\label{chap:execution}


This sections outlines the development of the transformer. It begins by extending the work of \weixin from \cite{food} by providing a formalization of the transformer in Rust. It provides this formalization in an extension of the FOOD syntax \cite{food}, called RFOOD. Throughout, it discuses the details of the implementation, outlining aspects of the transformations which are not encapsulate by the provided rules. It touches on the implementation of the transformer and outlines how the transformer can be used by developers. Finally it discuses a few extensions to the transformer which enable it to go beyond some of the restrictions on the input style.

% Before any transformations take place, the provided code is transformed into an abstract syntax tree (AST). This provides a concrete format to represent the code syntax, keeping the transformer separate from the code parsing stage. Rust provides access to the \verb|rustc| package, which is its compiler. This contains the \verb|rustc_ast| and \verb|rustc_parse| package which are responsible for this parsing.

% However as Rust is a relatively new language many of these features have unstable APIs. This means they are only available with the nightly compiler and the interface is likely to change. To avoid this, the syn package is used for parsing the rust code into a syntax tree \cite{syn}which provides a stable interface.
% The main issue with Syn, is it geared toward helping parse code in Rust procedural macros. This means it has a stronger focus on understanding an AST rather than manually manipulating it and creating items. For this reason a fork of the Syn project was created to allow access to an additional library, which made creating more AST objects possible. The fork can be found at .

% Another requirement, is once the AST has be transformed, it must be converted back into Rust code. To achieve this the \verb|quote| and \verb|rustfmt|. The \verb|quote|  package converts the AST back into tokens of source code, whilst the \verb|rustfmt| package formats the outputted code in a consistent way.

% Finally a CLI was created to allow developers to parse in files they want to transform and to specity the direction (OOP to FP or FP to OOP).

\section{High level overview}
\label{sec:trans-overview}

In Rust and Scala datatypes (enums) can be used as types directly. However, unlike in Scala, a Rust interface (trait) cannot. 

As discussed in \autoref{sec:dyn}, when using a trait as a type either the concrete type can be used, which is the struct or the trait must be behind a dynamic reference/pointer. Using the struct as the type violates Pure OOP principals as in this style, interfaces are types (not generators). As such, when a datatype is converted into a trait, if it is used as a type directly, the type must be changed. These changes are required every time a variable, attribute or parameter is explicitly typed as one of these transformed traits. An example of this is if a function returns a datatype called \rust{Set} in FP, it will have a signature of the form \rust{fn foo() -> Set}, however once transformed it must take the form of \rust{fn foo -> Box<dyn Set>}. In order to accommodate these type translations the transformer has been split into 2 stages. 

% The first stage, matches the pre-processing stage of the transformer outlined in \cite{food}. In this stage the "global context" is collected which stores items that are regularly used throughout the transformation. This prevents repeated look ups of the same items, both improving efficiency and reducing the complexity of the transformer.

% A high level algorithm for each of the transform types is given in \autoref{alg:high-level}. In each case the transformer operates by performing three distinct passes of the AST.

The first stage is responsible for the transformations of the user defined types and their associated functions. This primarily matches the transformations outline by \weixin in \cite{food} however also includes the type translations. During this stage the only transformation applied to the bodies of functions is a rename operation, whilst other transformations are handled in the second stage. This means at the end of this stage the program may be invalid as types of expression may not match assigned types. For example, if a function's return type is updated, the function's body will still be an expression of the original return type. These discrepancies are addressed in the second stage of the transformer.

The second stage is responsible for updating any references to the user defined types and their functions. Again this closely aligns with the work from \cite{food}, however this stage also updates the types of each expression. It does this by recursively parsing the expected return type to each expression which is transformed. This outlines the type which the expression should return after the transformation is completed. Expressions which do not match the expected return types are updated using the reference ($\&$), pointer ($\sim$) and dereference ($*$) operators. 

This approach ensures all type transformations occur in the first stage. This allows the second stage of the transformer to update the types of expression under the guarantee that existing types are fixed. 

Throughout the following sections, the output of the discussed stages will be shown for the example input programs from \autoref{fig:ex-exp}. Both examples implement the same Exp type in OOP and FP decomposition style respectively. The \rust{Exp} type models an expression which can be evaluated to an integer using \rust{expr}. Two types of expression are defined, a literal, \rust{Lit}, and substitution, \rust{Sub}.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustexampleLN{exp/oop}{1}{22}
    \caption{OOP decomposition style}
    %\label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustexampleLN{exp/fp}{1}{11}
    \caption{FP decomposition style}
    %\label{fig:dec-ex-fp}
\end{subfigure}
\caption{Example input to transformer}
\label{fig:ex-exp}
\end{figure}

% for enujm in gamma.enums...
% \begin{figure}
% \begin{minipage}{0.46\textwidth}
%     \begin{algorithm}[H]
%     delta $\leftarrow$ collect delta \\
%     \For{int {\bf in} delta.interfaces}{
%         create enum\\
%         \For{gen {\bf in} int.generators}{
%             create enum variant\\
%         }
        
%         \For{dest {\bf in} int.destructors} {
%             create consumer function\\
%             \For{gen {\bf in} int.generators}{
%                 create match arm\\
%             }
%             \uIf{required} {
%                 create wildcard arm\\
%             }
%         }
%     }
%     \caption{OOP Transformation}
%     \label{alg:high-level-oop}
%     \end{algorithm}
% \end{minipage}
% \hfill
% \begin{minipage}{0.46\textwidth}
%     \begin{algorithm}[H]
%     \For{$i=0$ {\bf upto} $n$}{
%       $t_i \leftarrow 0$\;
%     }
%     \caption{FP Transformations}
%     \label{alg:high-level-fp}
%     \end{algorithm}
% \end{minipage}
% \caption{High level overview of transformations}
% \label{alg:high-level}
% \end{figure}

% In each case the transformations consist of three distinct stages. The first stage is to collect the global context, delta. This is used throughout the transformations to prevent repeated looks ups for items.

% The seconds stage is the transformation of the user defined types (either datatypes or traits). This stage closely maps to the transformation rules outline by \weixin in \cite{food}.
% Using these the user type as well as their associated destructors/consumers transformed. At this stage the  Implementations of destructors are converted into a match arm in a consumers, whilst consumers' match arms are converted into  destructor implementations.

% The final stage is to transform all expressions, including the output of the type transformations. This transforms any references to transformed function and methods, and updates the types of expressions which require updating as a result of the transformations.

\section{Syntax}

A minimal calculus, RFOOD, is introduced in \autoref{fig:syntax} and is used to formalize the Rust transformations. This syntax is based on the FOOD calculus created by \weixin in \cite{food} however, with modifications to align closer with Rust's syntax and support its typing system. 

In RFOOD, a program consist of a set of definitions. These definitions consist of datatypes (Dt), interfaces (It), generators (Gen) and functions (Fun). A datatype, defines a set of constructors (Ctr) within, whilst an interface defines a set of destructors (Dst). Generators (Gen) are represented by $\class{\genn}{x}{T}{\itn}{\overline{Fun}}$, which in Rust maps to a struct, G, with a single implementation block for the trait $\itn$. This implementation block contains a set of functions (Fun).

Top level functions have also been added to the syntax in order to demonstrate the effect of the type transformations. If a top level function's first argument is datatype and its body is a match statement on that datatype, it is deemed a consumer (Csm), of the datatype.

In Rust, enums can be defined in a named or unnamed style. In the implementation they have been mapped to the named style as this is closer to the style of structs. Extending to support both styles would be trivial.

A minimal set of expressions are included in this syntax. Firstly $x$ denotes a variable. $e_1.f(\overline{e_2})$ denotes a call to the destructor $f$ on the expression $e_1$ with the set of expression $\overline{e_2}$ as the arguments. $f(\overline{e})$ denotes a call to function $f$, with arguments $\overline{e}$. $\ctrCall{\dtn}{\ctrn}{e}$ denotes a call to the constructor $\ctrn$ of datatype $\dtn$ and $\genn(\overline{e})$ denotes a call to the generator G both with arguments $\overline{e}$.

The primarily difference between FOOD and RFOOD are the four additional types $\sim T$, $\sim_{dyn} T$, $\& T$ and $\&_{dyn} T$. These represent a pointer, a dynamic pointer, a reference and a dynamic reference to type T respectively. Note for this syntax a dynamic pointer/reference can only be created for a interface, whilst a regular pointer/reference is only for datatypes. 
For this transformer only the Box pointer is considered, however most other smart pointer types should be analogous. Raw pointers (and all unsafe concepts) have not been included. The $\sim e$ and $\& e$ expression are added in order to create pointers and references of expressions. Both are overloaded to create the dynamic form as required.

The $\kwself$ type has been kept separate from the other types in the syntax as it can only be used in destructors. The match statement is also not included as a standalone expression as pattern matching is currently only supported in constructor match statements. Note there are some additional restrictions on expressions and definitions which are not represented in the syntax, these are outlined throughout this section.

% A fundamental difference between the transformer from \cite{food} and the transformer proposed in this paper, is that types are required to change in Rust. This comes about as in Rust, much like in Scala, datatypes (enums) can be used as types directly. However once transformed to a trait, as discussed in \autoref{sec:dyn} they can not longer be used as types. Instead a dynamic reference of the trait must be used. This fundamentally changes the type. This can have impacts across the whole program and therefore additional considerations are required throughout the transformations.

% The places types will change during the translation are: 
% 
% \begin{itemize}
    % \item Any dyn reference in OOP, this is handled differently in destructors to the rest of the code.
    % \item Anything which has the type of a Datatype directly (this does not explicitly include constructors). % TODO why, talk about how rules pretend a DT is returned from a constructro but its actually the struct type itself. Instead of transorming this directly we have left it (to not break other code) but results in more complicated translations
% \end{itemize}

% In \cite{food} \weixin denotes a type directed translation of the program $L$ of type $T$, under the context of $\Gamma$ and $\Delta$ with $\oldtrans L T L'$, however with the potential for types to change, this is no longer sufficient. Instead translations are represented as $\trans L T {L'} {T'}$ where $T'$ represents the output type after the translation. Moreover, the $\vdash^{\sigma}$ symbol is used to represent a translation which should result in an expression of type $\sigma$, so in full $\transSigma \sigma L T {L'}$.

% Note the changing in type, never effects the base type, but only the references/pointers. For example, the transformation may convert the result type of a function from \rust{Box<dyn T>} to \rust{T} but it may not change \rust{T} itself.

\begin{figure}[t]
  \setstcolor{red}
  \begin{displaymath}
    \begin{array}{l}
      \begin{array}{llcl}
        \text{Program}
        & L & \Coloneqq & Def ; L\\
        \text{Definition}
        % & Def & \Coloneqq & Dt \mid It \mid Gen \mid Csm \\
        & Def & \Coloneqq & Dt \mid It \mid Gen \mid Fun \\
        \text{Datatype}
         & Dt & \Coloneqq & \datatype {\dtn} {Ctr}\\
        \text{Interface}
         & It & \Coloneqq & \interface {\itn} {Dtr}\\
        \text{Destructor}
         & Dtr & \Coloneqq & \fnDstr {\fnArg{self}{\&\kwself}} {x: T} {T} \mid \fnDstr {\fnArg{self}{\sim \kwself}} {x: T} {T}\\
         &     &           &  \fnDstr {\fnArg{self}{\&\kwself}} {x: T} {T} = \overline{e} \mid \fnDstr {\fnArg{self}{\sim \kwself}} {x: T} {T} = \overline{e} \\
        \text{Constructor}
        & Ctr & \Coloneqq & \ctr {\ctrn} {\fnArgs{x}{T}} \\
        \text{Generator}
        %  & gen & \Coloneqq & \class C {C(\overline{e})} {fun}\\
         & Gen & \Coloneqq & \class{\genn}{x}{T}{Itn}{Fun}\\
         
        \text{Consumer}
        & Csm & \Coloneqq & \csmcase {d: \&Dtn} {x: T} T {d} {P} {e} \\
        &     &           & \mid \csmcase{d: Dtn}{x: T}{T}{d} {P} {e}\\
        &     &           & \mid \csmcase{d: \sim Dtn}{x: T}{T}{d} {P} {e}\\
        
         \text{Declaration}
         & Dec & \Coloneqq & \dec {x: T} T\\
         \text{Function}
         & \text{Fun} & \text{$\Coloneqq$} & \text{Dec} = \overline{e} \\
        \text{Expression}
         & e & \Coloneqq & x %\mid \overline{(x: T)} \Rightarrow t 
          \mid e_1.f(\overline{e_2})  \mid f(\overline{e}) \mid \genn(\overline{e}) \mid \ctrCall{\dtn}{\ctrn}{e} \mid \sim e\mid \&e \mid *e \mid\\
        %  &   &           & \ifelseexpr{e_1}{e_2}{e_3} \\
         \text{Pattern}
         & P & \Coloneqq & \genn(\overline{x}) \mid \ctrCall{\dtn}{\ctrn}{x} \mid \_ \\
        %  \text{Modifier}
        %  & m & \Coloneqq & \kwsealed \mid \kwabstract \mid \kwcase\\
        % \text{Boolean}
        % & bool & \Coloneqq & true \mid false  \\
        \text{Types}
        & T & \Coloneqq & \dtn \mid \overline{T} \rightarrow T \mid \sim  \dtn \mid \& \dtn \mid \sim_{dyn} \itn \mid \&_{dyn} \dtn \\ % \mid bool \\ %\mid \bot \\
        & \dtn & \Coloneqq & \text{Datatype name} \\
        & \itn & \Coloneqq & \text{Interface name} \\
        & \ctrn & \Coloneqq & \text{Constructor name} \\
        & \genn & \Coloneqq & \text{Generator name} \\
        & f & \Coloneqq & \text{Function or destructor name} \\
        & Self & \Coloneqq & \text{Alias for concrete type in trait impl and trait in trait dec} \\
        % & \blockO{e} & \Coloneqq & \text{Block of expressions} \\
        
        % & D & \Coloneqq & Dtn \mid Itn \\
        % & C & \Coloneqq & \text{constructor or generator names} \\
        % \text{Names}
        % & D & datatype or interface name \\
      \end{array}
    \end{array}
  \end{displaymath}
  \caption{Syntax of RFOOD}
  \label{fig:syntax}
\end{figure}

\section{Context}

\subsection{Global Context}

As outlined in \cite{food} a prepossessing step is performed by the transformer to collect commonly used attributes of the program. These are stored in the "global context", $\Delta$. It exposes a set of attributes that enable the following: checking if a type name is a datatype or interface; getting all constructors/generators and destructors/consumers for a user defined type; getting the signatures for both constructors/generators and destructors/consumers; getting the definition of any of the above. Note in the Rust implementation, top level functions are also collect in gamma and their signatures can be accessed using $\sig$. Given the Rust implementation is based on named fields in types, the $\attrs$ method has also been added, in order to access the names of attributes of a generator/constructor. 
The set of attributes and methods exposed by this global context in the formalization, are shown in \autoref{fig:global-context}.

The global contexts collected for the examples from \autoref{fig:ex-exp} are shown in \autoref{fig:ex-exp-gamma}. For the sake of space any empty attributes and $\defs$ have been excluded.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \begin{minted}[escapeinside=||,mathescape=true]{text}
    {
        IT: {Set}
        DTR: {
            Set: {eval}
        }
        GEN: {
            Set: {Lit, Sub}
        }
        SIG: {
            Lit: i32  -> Lit 
            Sub: |$\sim_{dyn}$ Set|, |$\sim_{dyn}$ Set|  -> Lit 
            eval: &Self -> i32 
        }
        ATTR: {
            Sub: [n]
            Lit: [l, r]
        }
    }
    \end{minted}
    \caption{OOP decomposition style}
    %\label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \begin{minted}[escapeinside=||,mathescape=true]{text}
    {
        DT: {Set}
        CSM: {
            Set: {eval}
        }
        CTR: {
            Set: {Lit, Sub}
        }
        SIG: {
            Lit: i32  -> Lit 
            Sub: |$\sim$ Set|, |$\sim$ Set|  -> Lit 
            eval: &Exp -> i32 
        }
        ATTR: {
            Sub: [n]
            Lit: [l, r]
        }
    }
    \end{minted}
    \caption{OOP decomposition style}
    %\label{fig:dec-ex-oop}
\end{subfigure}
\caption{Gamma collect for examples from \autoref{fig:ex-exp}. Empty attributes and $\defs$ excluded.}
\label{fig:ex-exp-gamma}
\end{figure}

\begin{figure}[t]
\begin{tabular}{ll}
\dt & Set of Datatypes\\
\ite & Set of Interfaces\\
\fn & Set of Functions \\
$\ctrs{\dtn}$ & Constructors of datatype $\dtn$\\ 
$\dtr{\itn}$ & Destructors of interface $\itn$\\  
$\generator{\itn}$ & Generators of interface $\itn$\\
$\consumer{\dtn}$ &  Consumers of datatype $\dtn$\\
$\sig(N)$ & Signature of constructor/generator/consumer indexed by name $N$ \\
% $\csmBody(f,C)$ & Case clause for constructor $C$ in consumer $f$\\
% $\dtrBody(f,C)$ & Body of destructor $f$ in generator $C$\\
$\dtrType(f,\itn)$ & Signature of destructor $f$ for interface $\itn$\\
$\dtrType(f,\itn)$ & Signature of destructor $f$ for interface $\itn$\\
$\defs(N)$ & Definition of function/datatype/interface/constructor/generator/consumer by name $N$\\
$\defs(f, N)$ & Definition of destructor $f$ for interface $\itn$\\
$\attrs(\genn)$ & Return list of attribute names for generator $\genn$\\
$\attrs(\ctrn, \dtn)$ & Return list of attribute names for constructor $\ctrn$ of datatype $\dtn$\\
% Caseof & $\caseof f C$ & = & if $\kwdef \ f(x : D)(\overline{x : T}): T = \overline{\kwcase\ C(\overline{x}) \Rightarrow e}\ \land$ \\
\end{tabular}
\caption{Global context, $\Delta$, attributes and methods. Based on \cite{food}.}
\label{fig:global-context}
\end{figure}


% In \cite{food} \weixin uses the name "global context", represented by $\Delta$, to represent the store with  the following attributes:

% \paragraph{Datatypes.} Datatypes are equivalent to Enums in Rust.
% \paragraph{Interfaces.} Interfaces are equivalent to Traits in Rust.
% \paragraph{Constructor.} Constructors create an instance of a given datatype. In Rust the equivalent is an Enum Variant.
% % TODO update this if it can handle multiple inheritance
% \paragraph{Generator.} Generators create an instance of a given interface. In Rust the equivalent is a struct which has an implementation of a trait (the interface). Note in this implementation a struct cannot be transformed if it implements multiple traits. 
% \paragraph{Consumer.} Consumers are functions that operate on a datatype. In Rust these are functions, with the first argument being a Datatype (or some reference to a datatype).
% \paragraph{Destructor.} A destructor is a function that operates on an interface. In Rust these are methods defined on the traits, which must either have a default implementation (in the trait) or an implementation in the generators impl of the trait.

% \vspace{10pt}

% In the current implementation of the transformer "global context" is an accurate name, as the Datatypes are collected at the beginning of the transformation. However, this does oversimplify the reality as it is possible to define Datatypes within scopes. In this case it would be more accurate to generate Gamma recursively (along side other transformations) to ensure all scoping is correct. Note, however, the top level types must be collected before any other types are transformed, as types defined after a function can still be referenced in that function.

% % TODO did the change?
% For the Rust implementation the Visitor pattern was used. Provided by the \verb|syn| package these traits (\verb|visit| and \verb|visit_mut|) allow the developer to view/mutate selected types of values in the AST. The default implementation of the visitor trait methods, recursively visits all expression. Using this technique ignores the scope of the current expression meaning all types and methods are collected and therefore assumes the developer has not renamed any types/function in a nested scope. This technique could be improved by a recursive approach to collecting this context along side other transformations, to ensure the correct versions of all the attributes are collected.

\subsection{Environment}

The other context variable is called the "Environment", $\Gamma$, and is used throughout the second stage of the transformation. The environment stores the types of variables currently on the stack. 
In the Scala implementation from \cite{food}, types are encoded as strings, however this is insufficient for Rust as reference types must also be kept track of. For this reason in the Rust implementation types are encoded as \verb|DeltaType| struct. 
This stores both the type name stored as an Ident (a simple wrapper around String provided by \verb|syn|) and the reference type.

The reference type is implemented as a recursive Enum called \verb|ReferenceType|, with a variant for \verb|Box| (pointer) and \verb|Reference| which both contain a nested \verb|ReferenceType| and a variant for \verb|None|. This allows the encoding of arbitrarily nested indirection. For example the following type signature in Rust \rust{&Box<&i32>}, is represented as a \verb|DeltaType| with the name "i32" and the reference type \verb|Ref(Box(Ref(None)))|.

% TODO sort this out
% The key limitation of the current implementation is these are not stored as recursive types. This means nested references such as reference of boxes cannot be encoded. 

% \subsection{Type of Expression}

% With these two contexts it is possible to perform basic type inference on expressions. The following rules are used to perform this inference:

% \begin{itemize}
%     \item For literals, for example \rust{1} or \rust{true}, a basic type of the literal is returned. In the current implementation only a subset of literals are supported (\rust{i32}, \rust{f32}, \rust{bool}) and they are assumed to be a specific type (i.e. \rust{i32} rather than a generic \verb|Int|).
%     \item For paths, which are expressions referring to some variable in the environment, the type of the variable from the environment is returned 
%     \item For struct/enum instatioation, the struct or enum type is returned.
%     \item For method or function calls, the return type of the function is returned
%     \item For field expressions, for example \rust{self.a} or \rust{item.field}, the type of receiver (the thing the field is being accessed from) is determined using gamma. The type of the field which is being accessed is then returned.
% \end{itemize}

% % TODO talk about casting
% There are a few key limitations to the current type inference:

% \begin{itemize}
%     \item  Currently the transformer only support single file transformations. This means it cannot perform type inference on imported or built in functions. Supported has been added for a small subset of built in functions such as clone but this is currently a manual process rather than the importing definitions.
%     \item  Currently a limited subset of expressions are supported. With time the types of expressions that are supported could be extended.
%     \item Currently type names are encoded as strings and as such no casting is supported. For example when if a literal is marked as an \rust{i8} the transformer is unable to cast that to an \rust{i32} if required.
% \end{itemize}

% One approach to resolving these issues is to use the built in type inference in the rust compiler. The rust compiler parses the source code (including all dependencies) and converts it to an AST, then to a HIR (High-Level Intermediate Representation) and then finally a THIR (Typed High-Level Intermediate Representation), where the typing information is added. It collects (and checks) these types using the \verb|rustc_typecheck| package which can be exposed using the nightly compiler. Currently the documentation for this package is very limited and the API for all of rustc remains unstable. For this reason this approach was not developed for this version of the compiler, but could create some tangible improvements to the type checking in the future. Another improvement of using this crate is the type checking could automatically track with the version of rust the developer is using. This means as rust add or changes features in the langauge the type inference of the transformer would not have to be updated directly.

\section{User Defined Type Transformations}

% Stage 2 OOP Trans Rules
\begin{figure}[t]

\begin{mathpar}
\inferrule[It2Dt]
{  \overline{Gen} = \generator{\itn} \\ \overline{\transtwoNT{\itn}{Gen}{Ctr}} \\ \overline{\transtwoNT{}{Dtr}{Csm}} \\ \overline{} \\ \transtwoNT {} L {L'} \\ \dtn = \itn}
{ \transtwoNT {} {\interface{\itn}{Dtr}; L} {\datatype {\dtn} {Ctr}; \overline{Csm}; L'}}
\end{mathpar}

\begin{mathpar}
\inferrule[Gen2Ctr]
{ \overline{\transType {\itn} T {T'}} \\  } %  
{ \transtwoNT {}{\class {C} {x} {T} {\itn} {\overline{Fun}}} {\ctr{C}{\overline{x: T'}}}}
\end{mathpar}

\begin{mathpar}
\inferrule[Dec2Csm]
{\fresh{d} \\ \overline{\genn} = \generator{\itn} \\ \transType{\itn}{T_{s}}{T_{s}'} \\ \overline{\transType{\itn}{T_{x}}{T_{x}'}} \\ \transType{\itn}{T}{T'} \\ \sig(G) = T_{g}
}
{\transtwoNTB{\itn}{\decD{self: T_{s}}{x: T_{x}}{T}}{
    \csmcase{\fnArg{d}{T_{s}'}} {\fnArg{x}{T_{x}'}} {T'} {d}  {\ctrPat{\itn}{\genn}(\attrs(G))} {[self \mapsto d]\gdefinition{f}{\genn}}}
}
\end{mathpar}

\begin{mathpar}
\inferrule[Fun2Csm]
{\fresh{d} \\ \transtwoNT{}{\decD{self: T_{s}}{x: T_{x}}{T}}{
    \csmcase{\fnArg{d}{T_{s}'}} {\fnArg{x}{T_{x}'}} {T'} {d}  {P} {e}}
}
{\transtwoNTB{\itn}{\fnDec{self: T_{s}, \overline{x: T_{x}}}{T}{\overline{e}}}{
    \csmcase{\fnArg{d}{T_{s}'}} {\fnArg{x}{T_{x}'}} {T'} {d}  {P} {e}}; \matchcaseNO{\_}{[self \mapsto d]\overline{e}}
}
\end{mathpar}

\caption{Stage 1 OOP - User defined types OOP transformation rules}
\label{fig:stage2-trans-oop}
\end{figure}

% Stage 2 OOP Type Trans Rules
\begin{figure}[t]

\begin{mathpar}

\inferrule[CreateRef]
{}
{ \transType{Dtn}{\& Self}{\&Dtn}}

\inferrule[CreateRef]
{}
{ \transType{Dtn}{\sim Self}{Dtn}}

\inferrule[CreateRef]
{}
{ \transType{Dtn}{\sim_{dyn}Dtn}{Dtn}}

\inferrule[CreateRef]
{}
{ \transType{Dtn}{\sim_{dyn}D}{\sim D}}

\end{mathpar}

\begin{mathpar}

\inferrule[CreateRef]
{}
{ \transType{}{\sim_{dyn} Dtn}{\sim Dtn}}

\end{mathpar}
\caption{Stage 1 OOP - Type transformation rules}
\label{fig:trans-type-oop}
\end{figure}

% Stage 2 FP Trans Rules
\begin{figure}[t]

\begin{mathpar}
\inferrule[Dt2It]
{  \overline{Csm} = \consumer{\dtn} \\ \overline{\transtwoNT{D}{Ctr}{Gen}} \\ \overline{\transtwoNT{D}{\defs(Csm)}{Dtr}} \\ \overline{} \\ \transtwoNT {} L {L'} \\ \itn = \dtn}
{ \transtwoNT {} {\datatype {\dtn} {Ctr}; L} {\interface{\itn}{Dtr};\overline{Gen}; L}}
\end{mathpar}

\begin{mathpar}
\inferrule[Ctr2Gen]
{  \overline{Csm} = \consumer{\dtn} \\ \overline{\transtwoNT{D, C}{\defs({Csm})}{Fun} } \\ \overline{\transType {\itn} T {T'}} \\  } %  
{ \transtwoNT {D} {\ctr{C}{\overline{x: T}}} {\class {C} {x} {T'} {\itn} {\overline{Fun}}}}
\end{mathpar}

% \begin{mathpar}
% \inferrule[Dec2Csm]
% {\fresh{d} \\ \overline{\genn} = \generator{\itn} \\ \transType{\itn}{T_{s}}{T_{s}'} \\ \overline{\transType{\itn}{T_{x}}{T_{x}'}} \\ \transType{\itn}{T}{T'} \\ \sig(G) = T_{g}
% }
% {\transtwoNTB{\itn}{\decD{self: T_{s}}{x: T_{x}}{T}}{
%     \csmcase{\fnArg{d}{T_{s}'}} {\fnArg{x}{T_{x}'}} {T} {d}  {\ctrPat{\itn}{\genn}(\attrs(G))} {[self \mapsto d]\gdefinition{f}{\genn}}}
% }
% \end{mathpar}

\begin{mathpar}
\inferrule[Case2Fun]
{\matchcase{\dtn::C(\overline{y})}{e} \in \overline{\matchcase{P}{e}} \\ \transtwoNT{D}{\fnDstr{d: T_{s}}{x: T}{T}} {\fnDstr{self: T_{s}'}{x: T'}{T'}}}
{\transtwoNT{D,C}{\csmcase{d: T_{s}}{x: T_{x}}{T}{d}{P}{e}} {\fnDec{\overline{self: T_{s}', x: T_{x}'}}{T'}{[d \mapsto self]\overline{e}}}}
\end{mathpar}

\begin{mathpar}
\inferrule[Csm2Dec]
{\matchcase{\_}{e} \notin \overline{\matchcase{P}{e}} \\ \transType{\dtn}{T_{s}}{T_{s}'} \\ \overline{\transType{\dtn}{T_{x}}{T_{x}'}} \\ \transType{\dtn}{T}{T'}}
{\transtwoNT{D}{\csmcase{d: T_{s}}{x: T}{T}{d}{P}{e}} {\fnDstr{self: T_{s}'}{x: T'}{T'}} }
\end{mathpar}

\begin{mathpar}
\inferrule[Csm2Fun]
{\matchcase{\_}{e} \in \overline{\matchcase{P}{e}} \\ \transType{\dtn}{T_{s}}{T_{s}'} \\ \overline{\transType{\dtn}{T_{x}}{T_{x}'}} \\ \transType{\dtn}{T}{T'}}
{\transtwoNT{D}{\csmcase{d: T_{s}}{x: T}{T}{d}{P}{e}} {\fnDec{self: T_{s}', \overline{x: T'}}{T'}{[d \mapsto self]\overline{e}}} }
\end{mathpar}

\caption{Stage 1 FP - User defined types FP transformation rules}
\label{fig:stage2-trans-fp}
\end{figure}

% Stage 2 FP Type Trans Rules
\begin{figure}[t]

\begin{mathpar}

\inferrule[2DynPnt]
{}
{ \transType{}{\dtn}{\sim_{dyn} D}}

\inferrule[Pnt2DynPnt]
{}
{ \transType{}{\sim \dtn}{\sim_{dyn} D}}

\inferrule[Ref2DynRef]
{}
{ \transType{}{\& \dtn}{\&_{dyn} D}}

\end{mathpar}

\begin{mathpar}

\inferrule[2SelfPnt]
{}
{\transType{s}{\dtn}{\sim \kwself}}

\inferrule[Pnt2SelfPnt]
{}
{\transType{s}{\sim \dtn}{\sim \kwself}}

\inferrule[Ref2SelfRef]
{}
{\transType{s}{\&\dtn}{\& \kwself}}

\end{mathpar}
\caption{Stage 1 FP - Type transformation rules}
\label{fig:trans-type-fp}
\end{figure}



\begin{figure}

\begin{mathpar}
\inferrule[Def]
{\transThreeStmt{Def}{Def'} \\ \transThreeStmt{L}{L'}} %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transThreeStmt{Def; L} {Def'; L'}}
\end{mathpar}

\begin{mathpar}
\inferrule[DecClass]
{\overline{\transThreeStmt{Fun}{Fun'}}} %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transThreeStmt{\class{\genn}{x}{T}{\itn}{\overline{Fun}}} {\class{\genn}{x}{T}{\itn}{\overline{Fun'}}}}
\end{mathpar}

\begin{mathpar}
\inferrule[DecFn]
{\transSigmaCtx{, \overline{x: T}}{T}{\overline{e}}{}{\overline{e'}}{}} %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transThreeStmt{\fnDec{\overline{x: T}}{T}{\overline{e}}} {\fnDec{\overline{x: T}}{T}{\overline{e'}}}}
\end{mathpar}

\begin{mathpar}
\inferrule[DecCsm]
{\overline{\transSigmaCtx{, \overline{x: T}}{T}{\overline{e}}{}{\overline{e'}}{}}} %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transThreeStmtB{\csmcase{d: T_{s}}{\overline{x: T}}{T}{d}{P}{e}} {\csmcase{d: T_{s}}{\overline{x: T}}{T}{d}{P}{e'}}}
\end{mathpar}

% \begin{mathpar}
% \inferrule[Eq]
% { \transSigmaT{T_1}{e_1}{T_1}{e_1'} \\ \transSigma{T_1}{e_2}{e_2'}}
% { \transThree {\eqexpr{e_1}{e_2}} {\eqexpr{e_1'}{e_2'}} {T} } 
% \end{mathpar}

% \begin{mathpar}
% \inferrule[Match]
% {}
% { \transTypeExprE {\matchexpr{e_1}{b}{c}{d}{e}} {T}  }
% \end{mathpar}

\caption{Stage 3 - Program and definition transformation rules}
\label{fig:trans-stage3-prog}
\end{figure}

\begin{figure}

\begin{mathpar}
\inferrule[Exp]
{\transSigmaInner{\sigma}{e}{e'} \\ \transSigmaType{\sigma}{e'}{e''}}
{ \transSigma{\sigma}{e}{e''}}
\end{mathpar}

\begin{mathpar}
\inferrule[Gen2Ctr]
{ C \in \constructor{D} \\ e = D::C (\overline{e'}) \\ \transSigma{\sigma}{e}{e'}{} }%\\ {\trans t {\overline{T} \arrow T} {t'}} } 
{ \transSigmaTInner{\sigma}{C(\overline{e})}{D}{e'}}
\end{mathpar}

\begin{mathpar}
\inferrule[Ctr2Gen]
{ C \in \generator{D} \\ e = C (\overline{e'}) \\ \transSigma{\sigma}{e}{e'}{} }%\\ {\trans t {\overline{T} \arrow T} {t'}} } 
{ \transSigmaTInner{\sigma}{D::C(\overline{e})}{D}{e'}}
\end{mathpar}

\begin{mathpar}
\inferrule[Gen]
{ C \in \generator{D} \\ \sig(C) = \overline{T} \rightarrow D  \\  \overline{\transSigma {T} e {e'}} }%\\ {\trans t {\overline{T} \arrow T} {t'}} } 
{ \transSigmaTInner{\sigma}{C(\overline{e})}{D}{C(\overline{e'})}}
\end{mathpar}

\begin{mathpar}
\inferrule[Ctr]
{ C \in \constructor{D} \\ \sig(C) = \overline{T} \rightarrow D  \\  \overline{\transSigma {T} e {e'}} }%\\ {\trans t {\overline{T} \arrow T} {t'}} } 
{ \transSigmaTInner{\sigma}{D::C (\overline{e})}{D}{D::C(\overline{e'})}}
\end{mathpar}


\begin{mathpar}
\inferrule[Sel2App]
{ \transSigma{\dtn}{e_1}{e_1'} \\ f \in \consumer{D} \\ e = \fncall{e_1, \overline{e_2}} \\ \transSigma{\sigma}{e}{e'} } %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transSigmaInner {\sigma} {\methodcall{e_1}{\overline{e_2}}} {e'}}
\end{mathpar}

\begin{mathpar}
\inferrule[Sel]
{ \transSigma{\itn}{e_1}{e_1'} \\ f \in \destructor{\itn} \\ \sig(f) = \fntype{\overline{T_{s}}}{T} \\ \transThreeStmt{e_1}{e_1'} \\ \overline{\transSigma{T_s}{e_2}{e_2'}}} %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transSigmaInner {\sigma} {\methodcall{e_1}{\overline{e_2}}} {\methodcall{e_1'}{\overline{e_2'}}}}
\end{mathpar}

\begin{mathpar}
\inferrule[App2Sel]
{ \transSigma{\itn}{e_1}{e_1'} \\ f \in \destructor{I} \\ e = \methodcall{e_1}{\overline{e_2}} \\ \transSigma{\sigma}{e}{e'} } %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transSigmaInner {\sigma} {\fncall{e_1, ...\overline{e_2}}} {e'}}
\end{mathpar}

% \begin{mathpar}
% \inferrule[App]
% { \transSigma{\itn}{e_1}{e_1'} \\ f \in \destructor{I} \\ e = \methodcall{e_1}{\overline{e_2}} \\ \transSigma{\sigma}{e}{e'} } %\\ {\trans t {\overline{T} \arrow T} {t'}} }
% {\transSigma {\sigma} {\fncall{\overline{e}}} {e'}}
% \end{mathpar}
\begin{mathpar}
\inferrule[App]
{ \transSigma{T}{e_1}{e_1'} \\ T \in \fn \cup \consumer{T} \\ \sig(f) = \overline{T} \rightarrow T \\ \overline{\transSigma{T}{e}{e'}} } %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transSigmaInner {\sigma} {\fncall{\overline{e}}} {e'}}
\end{mathpar}

% \begin{mathpar}
% \inferrule[Sel2App]
% { d \in Dtr } %\\ {\trans t {\overline{T} \arrow T} {t'}} }
% { \transTypeExprE {\fnDstr{d: T_d}{b}{c}} {} {}}
% \end{mathpar}

\begin{mathpar}
\inferrule[Block]
{\transSigma{\sigma}{e}{e'} \\ \overline{\transThreeStmt{e}{e'}}} %\\ {\trans t {\overline{T} \arrow T} {t'}} }
{\transSigmaInner {\sigma}{\overline{e}; e} {\overline{e'}; e'}}
\end{mathpar}

\caption{Stage 2 - Expression transformation rules}
\label{fig:trans-fp}
\end{figure}

\begin{figure}[t]

\begin{mathpar}
\inferrule[CreateRef]
{}
{ \transTypeExpr{T}{\&}{\&T} }

\inferrule[CreatePointer]
{}
{\transTypeExpr{T}{\sim}{\sim T}}

\inferrule[CreateDynPointer]
{}
{\transTypeExpr{T}{\sim}{\sim_{dyn} T}} \\

\inferrule[DerefRef]
{}
{\transTypeExpr{\& T}{*} {T}}

\inferrule[DerefPointer]
{}
{\transTypeExpr{\sim T}{*} {T}} \\

\inferrule[Pointer2Ref]
{}
{ \transTypeExpr{\sim T}{\&*} {\&T}}
\end{mathpar}

\caption{Expression type transformation rules}
\label{fig:trans-type}
\end{figure}

Having collected the global context, the first stage of the translation is to transform the user defined types, and their associated functions. These follow the rules outlined in \autoref{fig:stage2-trans-oop} and \autoref{fig:stage2-trans-fp}. In these rules, $\transtwoNT{}{e}{e'}$ represents the transformation of $e$ to $e'$ under the context of $\Gamma$. In some cases additional context is required to be passed between rules, which is represented with $\vdash_{c}$ where c is set of additional context variables. Specifically for FP, \verb|CTR2GEN|, \verb|CSM2DEC| and \verb|CSM2FUN| take a datatype, whilst \verb|CASE2FUN| takes both a datatype and constructor. In OOP, \verb|DEC2CSM| and \verb|FUN2CSM| take an interface.

The explanation of this stage is split into three sections. Firstly an overview of the type transformation is provided. Then separately, explanations of the translations of user defined types, and the translations of their associated functions. 

\subsection{Type Transformations}

As outlined in \autoref{sec:trans-overview} throughout this stage, type signature are required to change. These translation are required anywhere types are explicitly assigned, such as function signatures and in field definitions for generator and constructors.
These type transformation are required because, as discussed in \autoref{sec:dyn}, dynamically sized types must be behind a dynamic pointer/reference.
Note types are only required to change, if they are of a user defined type which is being transformed. All other types are kept the same.

An import consideration during these translations, is maintaining ownership. For example if a function previous took ownership of a argument, it should continue to do so. If it instead took a reference to a variable, it should also continue to do so. Changing the ownership of variables will inevitably result in issues, as variable may go out of scope when they are still required, or they may require cloning to be used. Note as 

For this reason references of datatypes are always transformed to dynamic references of traits and vice versa. This maintains the ownership principals and creates an analogous type.

However in FP there are two styles for types which take ownership of datatype variables, yet there is only one style for traits in OOP. In the FP datatypes can be used as a type directly or behind a pointer, whilst in OOP traits can only be used behind dynamic pointers. Thus when converting from FP to OOP both styles are converted to dynamic pointers as shown in \verb|PNT2DYNPNT| and \verb|2DYNPNT| rules in \autoref{fig:trans-type-fp}, where as for OOP to FP a decision has to be made of which style to used.

In this implementation, we selected a different style for destructor signatures than for all other translations. For destructor signatures, dynamic pointers of a trait are converted into the direct datatype. This often reduce the total use of indirection within the transformed code and creates transnational symmetry between the simplest implementations types. For example when defining the union operation for a \rust{Set}, the signature is transformed from \rust{fn union(self: Box<Self>, other: Box<dyn Set>) -> Box<dyn Set>} in OOP, to \rust{fn union(set: Set, other: Set) -> Set} in FP. Notice both the other parameter and return type are updated accordingly. These rules, given in \autoref{fig:trans-type-oop} and in each case provide the trait, $\itn$, which the destructor is defined on with the following syntax $\transType{\itn}{}{}$.

In all other cases the other style was selected, where dynamic pointers of traits are converted to pointers of datatypes. Given (in cases) dynamic pointer can be used analogously to pointer this reduces the overall impact of the transformer, reducing the changes it makes outside of the functions it is required to transform. Moreover this approach avoids issues with recursive type definitions, as types inherently keep pointers where they may be required. For example when converting the following struct defined as follows, where \rust{Set} is a trait which is being transformed to a datatype:

\begin{minted}{rust}
pub struct Insert{
    pub set1: Box<dyn Set>,
    pub value: i32,
}
\end{minted}

the dynamic pointer to the trait is converted to a pointer to the trait, as shown:

\begin{minted}{rust}
pub struct Insert{
    pub set1: Box<Set>,
    pub value: i32,
}
\end{minted}

Note as there are two styles in FP, which are converted to a single style in OOP, transformations are no longer injective. This means converting from one style to the other and then back to the original style, will no guarantee the output matches the original input. The rules for these translations are differentiated from the destructor type translation rules, by not providing an interface to the translation. Hence, $\transType{}{T}{T'}$, denotes the translation of $T$ to $T'$ outside of a destructor definition.

Another consideration of destructor methods is the \verb|Self| type. In trait definitions this is a reference to the (unsized) trait, whilst in implementation block it is analogous to the struct type. In both cases this does not require a dynamic reference. For the purposes of the transformer, when a reference to Self is used this is analogous to a reference to the trait, where as when the Self type is used directly, this is analogous to a dynamic reference to the trait. For clarity rules for the transformation of Self have been included explicitly in \autoref{fig:trans-type-oop} and \autoref{fig:trans-type-fp}. In FP a special syntax $\transType{s}{}{}$ is used to indicate the first argument of the consumer is being transformed, and thus it should be converted to the Self type.


\subsection{Transformation of Type Definitions}

The first stage of this process is to transform the user defined types themselves. These transformations follow the \verb|IT2DT|, \verb|DT2IT|, \verb|GEN2CTR| and \verb|CTR2GEN| rules which are heavily based on the matching rules from \cite{food}. However, these rules have been adapted to support the Rust type translations. With this step the types are converted to the new decomposition styles and the types of the attributes are updated.

% During this stage, for OOP the only type transformation rule applied is \verb|Pointer2Dyn|. This is because once traits have been converted to datatypes, they will no longer require the dynamic reference. For FP there are two cases which are transformed, datatype is used in one of the types, either directly or as a reference it is converted to a dynamic reference.

Note although not included in the formalization the visibility of the transformed types and functions are also translated in this stage. 

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustsnippet{type_def_transfom_oop}
    \caption{OOP decomposition style}
    % \label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustsnippet{type_def_transfom_fp}
    \caption{FP decomposition style}
    % \label{fig:dec-ex-fp}
\end{subfigure}
\caption{Outputs of the type definition transformation during the second stage of the transformer, on the examples from \autoref{fig:ex-exp}}
\label{fig:ex-exp-stage1}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustsnippet{type_fn_transfom_oop}
    \caption{OOP decomposition style}
    % \label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustsnippet{type_fn_transfom_fp}
    \caption{FP decomposition style}
    % \label{fig:dec-ex-fp}
\end{subfigure}
\caption{Outputs of the second stage of the transformer from examples in \autoref{fig:ex-exp}}
\label{fig:ex-exp-stage2}
\end{figure}

For OOP to FP the following steps are followed for each trait as outlined in \verb|IT2DT| and \verb|GEN2CTR| from \autoref{fig:stage2-trans-oop}:
\begin{enumerate}
    \item Create an enum with the same identifier and visibility as the trait
    \item For each generator of the trait (a struct which only implements that trait), add an enum variant to the enum. The enum variant should have the same identifier and fields as the struct, however the type of each field should be updated as outlined in the typing rules from \autoref{fig:trans-type-oop}. 
    Additionally in Rust, enum variants and their fields do not have visibility separate from the Enum it self and therefore all visibility tokens (\rust{pub}) must be removed from the structs and their fields. This transformation will therefore result in a loss of information as if the program is transformed back to the original, the original visibility of the structs and fields will be unknown.
\end{enumerate}

For FP to OOP the following steps are followed for each enum, as outlined in \verb|DT2IT| and \verb|CTR2GEN| from \autoref{fig:stage2-trans-fp}:
\begin{enumerate}
    \item Create a (for now empty) trait with the same identifier and publicity as the enum  
    \item For each variant of the enum create a struct. The structs should have the same identifier and contain the same fields as the enum variant.
    However, as above, the types of the attributes should be updated as outline by the type transformation rules in \autoref{fig:trans-type-fp}. Note at this stage the non datatype rules are used.
    Given the visibility of enums is only set on the enum itself (rather than on any variant or fields), the visibility of the structs and their fields are copied from the enum.
\end{enumerate}

\subsection{Transformation of Associated Functions}

Having transformed the definitions of each type, the next stage of the transformation is to convert the associated functions. For OOP this involves transforming the implementations of the trait methods into consumers, whilst for FP, it involves transforming the consumers into the trait methods and associated implementations for the structs. The rules for these transformations also require alteration in order to accommodate the type translations.

% When transforming a datatype type or pointer to a datatype from FP, it must be transformed to a dynamic reference of the trait. As discussed in \autoref{sec:dyn} this is because a trait both does not have a fixed sized and must contain a pointer to a vtable. However when transforming a dynamic trait from OOP, it is possible to convert it to either to a datatype type directly or to a pointer of the datatype. Both approaches are valid and have advantages. Converting to a pointer keeps an analogous type as dynamic pointers are treated much the same as pointers when writing in Pure OOP style. Where as converting to the datatype without the pointer, has the potential to reduce the complexity as a pointer may not be required. For datatypes the first style was selected as it prevents issues arising as a result of recursive types. In other cases this first style was also selected as it reduces the impact of the transformer on other aspects of the code. However for destructor implementations, the latter style was selected. This creates relational symmetry between the simpler FP style, where datatypes are used as types directly and the required dynamic pointer style in OOP.

% In both directions references of interfaces/datatypes are unchanged. This is possible as references in OOP require an instance of a trait, this is inherently fixed size and of a concrete (struct) type.

% In order to distinguish these type transformations, the $\transType{D}{}{}$ symbol is used to indicate a type transformation of type within the signature of a destructor/consumer of a given datatype, $D$, whilst all other type transformation are indicated by $\transType{}{}{}$.

% Another consideration of destructor methods is the \verb|Self| type. In trait definitions this is a reference to the (unsized) trait, whilst in implementation block it is analogous to the struct type. In both cases this does not require a dynamic reference. For the purposes of the transformer, when a reference to Self is used this is analogous to a reference to the trait, where as when the Self type is used directly, this is analogous to a dynamic reference to the trait. For clarity rules for the transformation of Self have been included explicitly in \autoref{fig:}.

The steps for transforming the associated functions in OOP, as outlined in \verb|DEC2CSM| and \verb|FN2CSM| from \autoref{fig:stage2-trans-oop}, are as follows.

\begin{enumerate}
    \item For every destructor defined within a trait, a consumer is created. The consumer is a top level function with the same signature as the destructor, however with the type translations, discussed above, applied to the parameters and return type. The \verb|self| parameter is also renamed to a fresh variable name, in order to avoid the use of the \verb|self| keyword. The consumer function has the same name as the destructor and the visibility is copied from the trait.
    \item The body of the consumer is set as a match statement, which matches on a borrow of the transformed \verb|self| parameter. A borrow must be used as otherwise the pattern cannot expose all the attributes, without breaking ownership principles. A match arm is then created for each of the variants of the datatype, with all of the fields of the datatype included in the pattern match.
    \item For each generator, the implementation of the destructor method method is taken and placed as the body of the match arm. Note at this stage it is possible that the return types of these expressions do not match the return type of the consumer signature. These discrepancies are addressed in the second stage of the transformer.
    \item For the body of each of the match arms, any reference to "self" fields are replaced with simply accessing the field directly from the pattern match attributes. All other references to self are replaced with a reference to the translated "self" type. This is encapsulated in the rules with the rename symbol $[self \mapsto d]$. 
    \item For each trait method, if a default implementation is provided, then a wildcard arm is added to the consumer, with the body of the default implementation. The body undergoes the same renaming encapsulated by $[self \mapsto d]$.
\end{enumerate}

For FP  the following steps are followed, as outline by \verb|DECFN| and \verb|DECCSM| from \autoref{fig:stage2-trans-fp}.

\begin{enumerate}
    \item A destructor is added to the trait for every consumer of the datatype. The types in the signature of the consumer are translated with the rules from \autoref{fig:trans-type-fp} and the first attributes name is converted to a self parameter.
    \item For each struct an implementation block for the trait is added. Each method added to the trait is implemented, copying the signature from the trait declaration. The body of the implementation is copied from the match arm in the equivalent consumer with the pattern of the equivalent constructor. Any reference to the old "self" parameter is update to use the self keyword and any reference to values which were exposed through the match pattern are updated to use access the attribute from \verb|self|. In the rules this renaming is indicated with $[d \mapsto self]$, where $d$ is the name of the first argument to the consumer.
    \item If a consumers contains a wildcard match then the implementation is added as the trait's default implementation of the method. The body undergoes the same renaming encapsulated by $[d \mapsto self]$. Note, as discussed in \autoref{sec:background-inheritance}, Rust has limited support for inheritance and thus this translation is only supported for methods which return basic types. Although not outlined in the formalization, a top level function is also supported as a consumer if no match is provided. This body is simply treated analogously to a match statement with a single wildcard arm.
\end{enumerate}

\subsection{Other Type Transformations}

For both transformation direction, any expressions which are not transformed during the first stage by the provided rules, still require the types to be updated. For example if a top level function (which is not a consumer) is included the types of all the parameters and the return type should be updated as outlined in the type transformation rules. This also required for any destructors and their implementations which are not transformed. Note beyond the current implementation there are many other places explicit types can be assigned, such as let expressions and closures. These are currently not supported.

\subsection{Output}

\autoref{fig:ex-exp-stage2} shows the outputs of the second stage of the transformer on the examples from \autoref{fig:ex-exp}. There are two notable issues with this output which make the output code from this stage invalid. Firstly the eval functions do not return the correct type and secondaly the function calls to the destructors/consumers have not been updated. These issue are addressed in the third stage of the transformer.

% Add in transformation rules for type signature arguments and return types

% \subsection{OOP to FP}

% The top level function for transforming from OOP to FP is \verb|transform_trait|. This does the following:

% \begin{enumerate}
%     \item  Create an enum variant for each generator of the trait (structs that implements the trait). The enum variants will have the same names as the structs and the same fields. The type of the fields will be transformed such that any \verb|dyn| reference, the dyn is removed. Structs can also expose fields as public with the \verb|pub| token, this is automatically the case for all fields of an enum variant so this is removed.
%     \item Create an enum with the same name as the original trait and the variants collected in step 1.
%     \item Transform each destructor of the trait into a consumer.
% \end{enumerate}

% The first stage to transforming a destructor is transforming its signature. This is done by...

% Assuming each struct impl has an implementation for this destructor (see \autoref{sec:inheritance} for why this is not always the case) the next stage of the transformation is to create a match statement with an arm for each generator of the trait. The arms consist of two key elements, the match pattern and the body. The match pattern is the enum which the generator is transformed to. The current implementation shows all values in the enum. An improvement would be to ignore (using the \verb|..| syntax) fields which are not used the in body.

% The body is a transformed version of the body from the generator implementation of the method. The body is transformed by replacing any reference to self with the new consumer function argument.
% Additional transformations, such as fixing the return type and use of transformed types are handle by the \verb|trasnform_expr| function which is described in detail in \autoref{sec:item-trans}.

\section{Item Transformations}
%TODO Talk about this
\label{sec:item-trans}

The second stage of the transformation, performs two functions. Firstly, it updates each use of the the transformed user types and their associated functions. Secondly, given that types can become inconsistent as a result of the first transformation stage, it updates the type of each expression, ensuring the output program is valid.

% TODO EXTEND GAMMA IN THE RULES

Prior to this stage, the global context, $\Delta$, is reevaluated. This updates the $\Delta$ such that it contains the values for the newly transformed types. In the example from \autoref{fig:ex-exp}, the context is switched, such that the context collected for the OOP example at this stage is equal to the context collected for the FP example prior to the first stage and vice versa.

During this stage, the whole AST is parsed. For each definition and expression, the sub definitions and expressions are transformed. In each case when transforming a nested expression the expected return type is determined. For example, when transforming the body of a function, the final expression has an expected return type, equal to the return type of the function signature. 

% TODO MOST OF THIS FEEL ITS OUT OF SCOPE FOR THE TRASFORMER
% Note in some case expressions are expected to return nothing, represented as $\bot$ in the syntax, such as a call to the print macro. Equally in cases any type can be returned from an expression. An example is a let expression without a type signature. In the implementation this is handled with an enum with three variants, Any, None and DeltaType. In the formalization this is instead represented by the $EXPLAIN THE TRANS SYMBOL$.

Each time an expression is transformed, after the transformation is complete the type of the output expression is inferred using the context. If this type does not match the required type, the rules from \autoref{fig:trans-type} are applied. These use the $\sim$, $*$ and $\&$ expressions to update the type of the expression. Applying these type transformations as the final stage of transforming the expression, allow nested expression to also fix the type of the expression. For example if a return expression contains an if statement each arm of the if statement will update its return type, rather than the top level return type doing the transformation. This, in conjunction with type simplifying, reduce the total number of type translation throughout the code, and keeps the calls to the type transformer in a single place in the implementation.

% TODO talk about reference and box expressions and add rules

\subsubsection{Type Inference}

Type inference is required throughout this stage in order to determine the current type of expressions, and thus determine if they need updating. The rules for this type inference are provided in \autoref{fig:type-inference}, where $\typeInf{}{e}{T}$ implies expression $e$ has type $T$ under the context $\Gamma$ and $\Delta$.

\begin{figure}

\begin{mathpar}
\inferrule[Var]
{}
{\typeInf{, x: T}{x}{T}}

\inferrule[Ctr]
{}
{\typeInf{}{\ctrCall{D}{C}{e}}{D}}

\inferrule[Gen]
{\exists D \in \dt : \genn \in \generator{D}}
{\typeInf{}{\genn(\overline{e})}{D}}

\inferrule[Ref]
{\typeInf{}{e}{T} \\ T \notin \ite}
{\typeInf{}{\& e}{\& T}}

\inferrule[RefIt]
{\typeInf{}{e}{T} \\ T \in \ite}
{\typeInf{}{\& e}{\&_{dyn} T}}

\inferrule[Pnt]
{\typeInf{}{e}{T} \\ T \notin \ite}
{\typeInf{}{\sim e}{\sim T}}

\inferrule[PntIt]
{\typeInf{}{e}{T} \\ T \in \ite}
{\typeInf{}{\sim e}{\sim_{dyn} T}}

\inferrule[DerefPnt]
{\typeInf{}{e}{\sim T}}
{\typeInf{}{*e}{T}}

\inferrule[DerefDynPnt]
{\typeInf{}{e}{\sim_{dyn} T}}
{\typeInf{}{*e}{T}}

\inferrule[DerefRef]
{\typeInf{}{e}{\& T}}
{\typeInf{}{*e}{T}}

\inferrule[DerefDynRef]
{\typeInf{}{e}{\&_{dyn} T}}
{\typeInf{}{*e}{T}}

\inferrule[Fn]
{\sig(f) = \fnsig{\overline{T}}{T}}
{\typeInf{}{f(\overline{e})}{T}}

\inferrule[Dtr]
{\typeInf{}{e_1}{\itn} \\ \sig(f, \itn) = \fnsig{\overline{T}}{T}}
{\typeInf{}{f(\overline{e})}{T}}

\end{mathpar}

\caption{Type inference rules}
\label{fig:type-inference}
\end{figure}

\subsubsection{Constructors and Generators}

\verb|GEN2CTR| and \verb|CTR2GEN| outline the transformations between generators and constructors. In Rust structs (generators) are in the global scope, where as by default enum variants (constructors) are nested within enums. For this reason when transforming generators into constructors, the path is extended with the datatype, for example \rust{Dimmer} to \rust{Light::Dimmer}. For FP this transformation is done in reverse, removing the datatype from the path.

Note an alternative approach to this is including enums in the global context, \rust{use Ligth::*}. This brings each enum variant into scope. However this increases the possibility of name collision and is generally considered bad practice. With some restrictions this syntax could also trivially be supported by the transformer.

In each case the result of the transformation is passed back to the transformer with the same required return type, which updates the nested expressions. 
\subsubsection{Destructors and Consumers}

The transformations between destructor and consumer are outline in \verb|SEL2APP|, \verb|APP2SEL|. For both, the first step is to move the self parameter. For transforming from OOP to FP, the expression which the method is called on (the receiver) is moved to the first argument of a call to the consumer. Whilst for FP, the first argument is removed and the destructor method is called on that expression. Again, as above, in each case the result of the transformation is passed back to the transformer with the same required return type.

\subsubsection{Expressions}

All other expressions, trivially transform the nested expressions. For example a function call transforms each of the parameter arguments with the required type from the signature.

\section{Beyond the formalization}

The formalization's provided in this paper, present a framework for transformations within the minimal RFOOD calculus. However, for the transformer to be a practical tool there are many additional constructs that must be supported.

\subsection{Additional Expression Transformations}

The implemented Rust transformer goes some way in extending the supported inputs by supporting additional constructs beyond those outlined in formalizations. In most cases these extension are trivial, where the expression simply transform each sub expression. For example for an if expression no change is required for the first stage transformer, whilst for the second stage the following rule can be added where an if expression is represented by $\ifelseexpr{e_1}{e_2}{e_3}$. 

\begin{mathpar}
\inferrule[If]
{ \transSigma{bool}{e_1}{e_1'} \\ \transSigma{\sigma}{e_2}{e_2'} \\ \transSigma{\sigma}{e_3}{e_3'}}
{ \transSigmaInner {\sigma} {\ifelseexpr{e_1}{e_2}{e_3}} {\ifelseexpr{e_1'}{e_2'}{e_3'}} }
\end{mathpar}

Note the expected return type of the condition is a boolean whilst the expected return types of the \verb|if| and \verb|else| block match that of the required type of the \verb|if| expression it self. When adding an expression expression, the rules for type inference also required extention. For the \verb|if| statement the type can be obtained from either of the \verb|if| or \verb|else| block expression. Note if these do not match the program will not compile. Trivially this logic is replicated for other expression types such as match statements. 

\subsubsection{Boolean Expressions}

In many cases, Rust requires that the types of sub-expressions within a boolean expression match. For example it is not possible to directly compare a string and a reference to a string with the \verb|==| operator. To accommodate this when transforming boolean expressions, the required type of the right expression is given as the type of the left expression. These expression also have implementations for type inference based on the operator. For example the \verb|==| operator is inferred as a boolean whilst the \verb|+| operator is inferred as the type of the left expression.     

\subsubsection{Let and Constants}

In the current implementation let expressions are implemented, such that no required type is provided. This means \verb|let| expressions are not unnecessarily transformed and instead type transformations occur when variables are used. Once the \verb|let| expression is transformed, type inference is used on the result to determine the type. This variable is then added to environment, $\Gamma$, within the current scope.

Similarly throughout the transformations constants must be added to the environment. An additional challenge, is when constants are defined globally, as they must be parsed first as otherwise the type may be unknown during the transformations. This is achieved with a additional parse of the items in the syntax prior to the second stage to collect all the types of global constants.

% \subsubsection{Reference and Dereference}

% TODO this should live somehwere else

% Throughout the third stage of the transformer, if a reference, deference or box expression (\rust{Box::new(e)}) is transformed, rather than repeating typing logic at this stage, simply the internal expression is returned. This removes the outer reference/deference/box from the expression as it will re-added if required by the expression type transformation. This allows the type transformation function to handle all the typing and prevents any additional indirection expressions that are not required.

% For type inference, if a reference or box expression is parsed it will return the type of the inner expression with the \verb|ref_type| type equal to a reference/box of the inner type's \verb|ref_type|. For example when parsing an expression if a reference is parsed and the inner expression has type $\sim i32$, then the outer expression will have type $\& \sim i32$. For dereference expression, if the inner expression is a pointer or reference then this is removed and the inner reference type is returned.

\subsubsection{Macros}

In Rust macros are a form of "metaprogramming", meaning they allow developers to write code that writes other code. 

Although not unique to Rust, macros play a far greater role in Rust that most other programming languages, and for this reason it is an important topic to cover. 

In Rust there are 2 kinds of macros, declarative and procedural. Declarative macros allow developers to write a match style function which operates on the code provided as the inputs, whist prodcedural macros allow developers to operate directly on the AST of the input code. 

Macros are an incredibly powerful tool, yet this flexibility creates many difficulties for the transformer. Given macro inputs are parsed as tokens, it is not possible to predict what the macro will do to the input. For example, if a macro was created to strip method calls, it would not be possible for the transformer to predict this behaviour.

Therefore it is not possible to reliably transform the inputs of macros. As a compromise the current implementation attempts to parse the inputs to any macros as comma separated set of expressions. If this is successful it will transform each expression as before using \verb|transform_expr|. After the transformations it will covert the transformed expressions back to the tokens format.

In many cases this limited approach is sufficient to maintain the correctness of the code and transform the required expressions. Critically this approach allows the use of basic macros included in rust such as \rust{println!} and \rust{vec!}. However as mentioned there is no guarantee of correctness when using these transformations with arbitrary macros.

\section{Implementation}

The transformation rules are applied to an abstract syntax tree, AST. 
Although Rust exposes the native compiler parser through the \verb|rustc| crate, the APIs are unstable and there is limited documentation. Instead the \verb|syn| package was used to parse the provided source code file into an AST. 
The \verb|syn| package is designed for parsing macro inputs and therefore its support for creating AST objects is limited. A minor fork (\url{https://github.com/jelgar/syn}) of the package was created to expose an additional module which enabled the creation of a required set of types.

Once the transformations are completed, the \verb|quote| package is used to covert the AST back into Rust source code tokens. Finally, the \verb|rustfmt| package is used to format the output in a consistent style.

The transformer package is exposes as a command line interface, CLI, which takes the path to a file and the desired transformation type (OOP to FP / FP to OOP). 

The project is split into three modules, the core, examples and outputs. The examples and outputs have been separated from the core project to enable them to be tested independently. The core and examples have a limited set of unit tests. An end to end test has then been setup which runs the transformer on a set of examples, and runs a set of unit tests on the output files to validate the functionality of the output matches that of the input.

\subsection{VS Code Extension}

As well as packaging the transformer as a CLI tool, a VS Code plugin has also been developed. VS Code is free and opensource and has over 14 million active users \cite{tung_2021}. This plugin offers the transformer along side other popular refactoring tools, and thus drops the barrier of entry for using the tool. The extension allows developers to transform the file they are currently editing using the VS Code command pallet. A demo is available here \url{https://youtu.be/Z9bvl1Odcds}.

The tool uses the \verb|wasm-bindgen| crate and \verb|wasm-pack| to create web assembly bindings and build a web assembly package for the transformer called rfood. This is then imported into a VS Code extension project template generated by \verb|yo|. Two commands are added to the extension which call the bindings from the rfood library. The source of the file that is current open is parsed to the binding function which returns the transformed code. Formatting the code is handled by another VS Code extension called \verb|rust-analyzer|. With this developers can quickly switch their code as they work, integrating the tool directly into their workflow.

\section{Extensions to the transformer}

\weixin's transformer focuses on the transformations between Pure OOP style from \cite{cook} and FP style. For the transformer to be a truely useful tool for developers, it must support the majority of styles of programming. 
The requirement to use pure OOP style is extremely restrictive, and often requires large refactoring of code before it can be transformed.
This means the types of OOP programs that can be transformed are limited.

\subsection{Mutability}

A common style for object oriented programming is to include methods that mutate the state of the class it self. An example is the getter and setter pattern where one functions on a class is responsible for returning a value and another to mutates it, as shown in \autoref{fig:mutable-example}.

In rust, variables are immutable by default. To make a varaible mutable the \rust{mut} token must be used before variable assignment. This can be seen on line 3 of the example in \autoref{fig:mutable-example} where the \verb|mut| token allows the self object to be mutated. This is then used for the implementation of \rust{set_brightness} on line 12 to set the brightness of the Dimmer. 

% Horrible sentence
It is often claimed that "functional programming aims to minimise or eliminate side-effects" \cite{fp-uok}. This can have many benefits such as reducing complexity of programs and making them easier to test. For this reason when transforming this style of OOP programming to FP, the mutations should wrapped within the consumers. This means the consumers should parse in an instance, and a new instance should be returned by the consumer. When the consumer is used, the parsed in object should then be overwritten by the output of the function.  In simple cases this inherently make the transformed function pure by removing the mutation of the argument.

To achieve this kind of transformation, the transformer check each destructor, checking if the self argument is mutable, if so the following additional steps are added:

\begin{itemize}
    \item Update the arguments of the signature of the destructor, updating the first argument to remove the mutability and borrow. Removing the borrow means the ownership of the passed in variable is taken and thus it will go out of scope after the consumer is called. This is required  as the variable should be overwritten by the result of the consumer. 
    \item Update the return type of the signature of the destructor. Currently only destructor with no returns are supported. Set the return to the trait type that destructor is defined within.
    \item In the match statement, make each field mutable. This allows reassignment of these local variables.
    \item Replace all field assignments on self with field assignments on these new mutable variables. 
    \item Return a new instance of the matched enum with the values of the fields equal to the local variable from the match expression. 
    \item When transforming method calls to this object, ensure to also overwrite the receiver with the output of the method.
\end{itemize}

\begin{figure}
\centering
\rustexample{mutable/oop_basic}
\caption{Mutable class example}
\label{fig:mutable-example}
\end{figure}

The result of transforming \autoref{fig:mutable-example} is shown in \autoref{fig:mutable-example-output}. 

\begin{figure}
\centering
\rustoutput{mutable/oop_basic}
\caption{Mutable class example output}
\label{fig:mutable-example-output}
\end{figure}

\subsection{Generics}

% \subsection{Rust Type Transformations}

% % TODO how is the required return type determined for an expression?
% % TODO how are types of an expression determined - this should probably go in delta but it should also talk about blocks....
% % TODO how are these types encoded (EType and DeltaType) this should also probably be in Delta context
% % TODO how is the transformation applied
% % TODO type simplification

% Unlike in the Scala implementation of the transformer the signatures of types' associated functions can change during the transformation. In general this is not the type itself that changes, but rather the reference type (such as converting a type to a box of the type). This therefore requires additional steps in the transformation to ensure the types of expressions are correct. To achieve this the \verb|transform_expr| function is also passed an expected return type.

% When recursively transforming nested expression is it the required type of each sub expression must be determined. For each type this is determined as follows:

% \begin{itemize}
%     \item Method/Function call - The signature of the function states the expected type of each of the argument expressions. For a method call receiver, in the current implementation any type can be returned. %TODO evaluate this
%     \item Block - For block expressions, all expression should return None, except the last expression which should have the same return type requested for the whole block.
%     \item Generator/Constructor calls - The required types of each field can be determined by the field types (signature) from gamma.
%     \item Match - For match expression each arm is expected to return the same type as the required type for the whole match statement
%     \item Return - The inner expression should match the required type of the return expression
%     % TODO add support for if else
%     \item TODO add support for if else  
% \end{itemize}

% Given these expected return types, it is then the responsibility of the \verb|transform_expr| function to also transform the types. This is achieved using \verb|transform_expr_type|.  

% In the current implementation, reference types are encoded as a basic enum (either ref, box or none). Given, in general, only the reference type of types change, the \verb|trasnsform_expr_type| can only perform basic transformation on the reference type. The current supported transformations are:

% \begin{itemize}
%     \item Box/Reference $\xrightarrow{}$ None - Achieved by dereference
%     \item None $\xrightarrow{}$ Box - Uses \rust{Box::new} method to create box of provided expression
% \end{itemize}

% The return types express what the given expression should return after it has been transformed. These return types are expressed as one of the following:

% \begin{itemize}
%     \item None - No return value
%     \item DeltaType - Contains a DeltaType struct to encode the expected return type
%     \item Any - Any type can be returned (no transformation of the return type will be made)
% \end{itemize}



% \verb|transform_expr| is the core of this recusrsive transformation process. For each type of expression this transforms the expression it self, as well as any nested experssions.

% The main transformations performed by this function is updating the use of transformed types. For OOP to FP transformations this involves replacing method calls with function calls to the consumers, whilst for FP to OOP it does the reverse. 

% \begin{itemize}
%     \item A consumer/destructor is transformed and the return types changes. E.g. \verb|T| to \verb|Box<T>|. This can have knock on effect where this result is used later.
%     \item A consumer/destructor is transformed and the argument type changes
% \end{itemize}

% The transforming of types is handled in the \verb|transform_expr_type| function. With the PID controller, the robot is not traveling at a constant velocity, but rather accelerating and decelerating constantly to correct the measured velocity. This repeated acceleration and declaration makes this prediction unlikely


% \section{Inheritance}
% \label{sec:inheritance}

% % TODO as metnitioned in \sec oop
% Inheritance is one of the central concepts in object oriented programming \cite{cook_palsberg_1989}. It allows developer to share logic between linked classes, preventing duplicate logic. Rust, however, does not support traditional inheritance. Instead it opts for composition, which allows structs to contain other structs directly and default implementations in traits.

% A limitation of default implementations is Rust cannot call methods on dynamic types, or use them to create structs.
% % TODO explain this more
% A clear example of where this is an issue, is the union of sets. Ideally a defualt implementation could be created, which returns a new Union struct with the two provided sets. However, given the Self type could be any set (and therefore it is not Sized), self cannot be used to create a new struct. For this reason the Rust implementation does not support default methods other than basic types.

% The FP equivalent to these default trait implementations are wildcard matches in consumers. These allow the consumer to use the same implementation for any unmatched expression.

% \paragraph{OOP to FP.} If any impls of a trait which is being transformed are missing any of the required methods, then a wildcard arm should be added for that consumer matching that method. The body of the consumer should be taken from the traits default implementation with the same transformations applied to the other arms. The only difference between the transformations is no additional delta collection is required as the pattern match patter is just the underscore token. 

% \paragraph{FP to OOP.} If a consumer contains a wildcard match, then the return type of the arm must be checked. If it returns a simple (Sized) type, then a default implementation of the trait can be created and an implementation in the impl block is not required. However if the return type is not sized then a default implementation cannot be used. In this case a copy of the wildcard expression is created for each of the impl blocks. 

% TODO talk about shared function
% Although it may seem preferable to create a shared function to handle this logic, this is also not possible (withas the type of the 

% \subsection{Multiple Inheritance}

% \begin{figure}[t]
% \centering
% foo
% \caption{This is an example figure.}
% \label{fig}
% \end{figure}

% \begin{table}[t]
% \centering
% \begin{tabular}{|cc|c|}
% \hline
% foo      & bar      & baz      \\
% \hline
% $0     $ & $0     $ & $0     $ \\
% $1     $ & $1     $ & $1     $ \\
% $\vdots$ & $\vdots$ & $\vdots$ \\
% $9     $ & $9     $ & $9     $ \\
% \hline
% \end{tabular}
% \caption{This is an example table.}
% \label{tab}
% \end{table}

% \begin{algorithm}[t]
% \For{$i=0$ {\bf upto} $n$}{
%   $t_i \leftarrow 0$\;
% }
% \caption{This is an example algorithm.}
% \label{alg}
% \end{algorithm}
% 
% \begin{lstlisting}[float={t},caption={This is an example listing.},label={lst},language=C]
% for( i = 0; i < n; i++ ) {
%   t[ i ] = 0;
% }
% \end{lstlisting}

% -----------------------------------------------------------------------------

\chapter{Evaluation of the Transformer}
\label{chap:evaluation}

This section will give a full evaluation of the transformer. We begin by evaluating the Rust transformer as a standalone tool. We then discuss the finding from implementing the FOOD transformer in a different language and use them to evaluate the generality of this approach. Finally we discuss how the transformer can be extended beyond the FOOD features and what limitations remain.

% TODO talk about testing

\section{The Rust Transformer}

A wide range of examples have been produced to demonstrate the functionality of transformer. The full set of examples can be here \url{https://github.com/JElgar/rfood}. A suite of units tests have been created to validate the examples and their outputs.

To evaluate the Rust transformer, we begin by exploring some case studies transformed by the tool. 

\subsubsection{Countdown Problem}

In \cite{hutton} Hutton outlines a solution for what he calls the "countdown problem". The objective of the implementation is to find an arithmetic equation which evaluates to a given target value. The arithmetic equation must be constructed from values from a provided list, which can only be used at most once. A simple example is, given the input list \rust{[1,2,3]} and the target value $7$, the equation $2 * 3 + 1$ is a valid solution. For this example we focus of the validation of a given expression. Hutton opts for functional decomposition in his solution and thus in this case study, we will discuss the required changes in order to support transforming the outlined solution to object oriented decomposition.

The first key limitation, is the lack of support for built in types. The current implementation of the transformer only operates on a single file and thus importing other definitions is not supported. As a result, currently, the vector and optional types from Rust cannot be used and instead a custom optional value and set type are implemented.

The list implementation 
 
\subsubsection{Shape}

One of the key values of the transformer is increasing the efficiency of developers. It can achieve this by allowing developers to extend code in the optimal decomposition style depending on the task. In order to demonstrate this we use the shape example, which is based on work from \cite{oopdesign}. This program begins with the following \rust{Shape} trait which outlines a set of destructors. 

\begin{minted}{rust}
trait Shape {
    fn perimeter(&self) -> f64;
    fn area(&self) -> f64;
}
\end{minted}

We begin with two destructors in the trait, \rust{perimeter} and \rust{area}. Two generators of \rust{Shape} are then defined, \rust{Circle} and \rust{Square}. In object oriented decomposition style, extending the program to add additional types is trivial. For example to implement the \rust{Rectangle} shape, we simply add a new class. In Rust this maps to the following 

\begin{minted}{rust}
struct Rectangle {
    length: f64,
    height: f64,
}
impl Shape for Rectangle {
    fn area(&self) -> f64 {
        self.length * self.height
    }

    fn perimeter(&self) -> f64 {
        2.0 * self.length + 2.0 * self.height
    }
}
\end{minted}

Notice the whole implementation of the type is contained in a single place, and no alteration to the original \rust{Shape} type is required. However, when extending the example to support additional destructors the complexity is increased. For example in order to support a side count method, firstly the method would have to be added to the trait, then each generator of the trait would have to be extended to implement this new method.

To simplify this process the transformer can be used. By first converting the program into functional decomposition style, the additional method can be added as a single constructor definition as follows

\begin{minted}{rust}
fn side_count(shape: &Shape) -> u8 {
    match &shape {
        Shape::Circle { radius } => 1,
        Shape::Square { side } => 4,
        Shape::Rectangle { length, height } => 4, 
    }
}
\end{minted}

Note much like when adding types in object oriented decomposition style, no changes outside of the consumer definition are required. With this example it becomes clear functional decomposition is the optimal style for adding functions to types, which object oriented is best for adding additional types. Without first performing these transformation, in more complex programs, it can be a time consuming process to find each location changes as a result of an extension.

\subsection{Type Transformations}

The primary difference between the FOOD transformer and the Rust transformer is its type transformations. These transformations as handled through two extension to the transformer. Firstly types are transformed using a set type transform rules, then each expression is transformed to all expressions conform to these new types.

The approach outlined in this paper splits the transformations into two stages. The first updates the types of all expression, whilst the second is responsible for transforming the types of expressions. An alternative approach to this, would to define concrete translations for constructor and destructor, updating each rule to update expression to use new types. 
For example a constructor of datatype would be updated to always be wrapped in a pointer when converted to a trait. Then any reference to the type could simply be updated to use this new type.
This approach would prevent the requirement for multiple stages in the formalization and ultimately result in fewer, less complex translations. 

The trade off however, is the type correction becomes closely tired to the transformation rules themselves. As outline in the implementation section, when transforming dynamic pointers to traits in OOP it is possible to convert them to either a pointer of, or directly to a datatype. Ultimately the chosen type is a design decision and each option has merits.
Combing the transformations into a single stage would require these types to be concrete as each rules would transform based on the results. Instead separating, the type translations from the expression transformation enables a relatively small set of rules to define the type translations, which can be updated independently of the expression rules.

Although the transformer creates transformations that are correct, it is not guaranteed that the transformations are optimal. Optimal in this sense refers to two things, minimising indirection operations and the speed of the transformed code. The type simplifications outlined in the implementation go so way to minimise the use of indirection where it is not required. However in some cases, it may be possible to improve the solution further. For example, the following extracts are taken from an expression example which is provided in full here TODO add to abstract \autoref{}. 

\sloppy This shows the input to the transformer, where a box to an expression in created, called empty. The \rust{insert} method is then called on this expression which has the following signature \rust{fn insert(self: Box<Self>, i: i32) -> Box<dyn Set>}.
\begin{minted}{rust}
    let empty = Box::new(Empty{});
    let set = empty.insert(1);
\end{minted}

\sloppy However once transformed the signature of the \rust{insert} function changes to the following \rust{fn insert(set: Set, i: i32) -> Set}, yet the transformed solution looks as follows:

\begin{minted}{rust}
    let empty = Box::new(Set::Empty {});
    let set = insert(*empty, 1);
\end{minted}

Once again, a box to a set is created. However it now must be dereference before it can be passed into the \rust{insert} function. Once passed in, the ownership is transferred to the function and therefore the variable is dropped. In this case the expression was never used as a box and therefore a more efficient solution would be to remove the Box from the let expression. A potential solution for this is for let expression to look ahead and determine how the expression is used before setting it type. This would allow the let expressions to assign the variables to the optimal type to reduce indirection where possible. 

In terms of speed of the outputted code, it is unlikely the transformation will have a significant impact. The transformer does not support the clone operation as borrowing introduced by the developer is maintained as a result of the translation. With this 



\subsection{Rust Limitations}

\section{FOOD Generality} 

A primary objective of this paper, is to evaluate the generality of the FOOD transformer from \cite{food}.
The transformation rules outlined in this paper are based on those in FOOD and therefore clear equivalences between them can be drawn. For example \autoref{fig} gives the \verb|GEN2CTR| rule from FOOD, which maps closely to \verb|GEN2CTR| created for the Rust transformer. These equivalences are evident throughout the transformer, suggesting the foundations of the transformer can be applied to other languages.

However significant change are required to support the Rust typing system. Most significantly, additional types have to be considered. Given Rust does not support traits as standalone types, translations between these types inherently become required. As a result a set of transformation rules are added to handle the transformation of these types in different scenarios. Additionally transformation rules are then required to update the types of expressions, in order to match any transformed types. With these changes the Rust transformer is required to significantly deviates from  that outline in \cite{food}. 

Ultimately, Rust is a very different language to Scala. Given the core concepts of the transformer can remain the same in Rust highlights that the FOOD transformer is, in some capacity, language agnostic. Clearly when it comes to implementation these concepts, language specific features have to be considers. Currently there are very few popular programming languages which, have multi-paradigm features and which provided such low level control of memory as Rust. It is more common to find the multi-paradigm features required for this transformer in higher level, systems programming languages. With this when apply the FOOD transformer to other languages, it is likely they will align more closely with Scala and thus required fewer changes.

\section{Beyond FOOD} 

For

\section{Comparison with Scala transformer} 

% TODO is this actually the set example from food?
% TODO explain what the xample is -> its a way to construct expressions
% TODO A subsection of the example form food has been included for the dicussion, however a transfomration of the full example can be found int he appendix.
\autoref{fig:trans-set-ex} demonstrates that the transformer successfully transforms the set example from \cite{food}. 
The outputted code passes a set of unit tests, which checks the functionality remains the same after the transformation.

The input, show in \autoref{fig:trans-set-ex-input} is written in the pure OOP style outlined in \autoref{sec:}. The trait (lines 1-3) is transformed into an enum and each struct is transformed into one of the enum variants. For each of the variants the fileds are successfully transformed with the dyn token removed. For each of the trait functions a consumer has been created with correspoding match expressions with the body as a transformed version from the struct implementation block.

Each use of the type is also transformed, meaning destructor calls are replaced with the \rust{eval} consumer, both in the match statements and in the demo function.

% TODO In reverse... FP to OOP

\subsection{Visibility}

% TODO summarise what visibility is
The expression example also demonstrates that the visibility on the inputs is maintained where possible. The visibility of the enum is taken directly from the trait. Enum variants inherently match the publicity of the enum, this means the visibility of the individual structs cannot be maintained. Equally in a trait the publicity of the items cannot be controller. Therefore visibility of the consumers is also taken directly from the trait. The visibility of other definitions (such as the \rust{demo} function) are unchanged.
% TODO include a not pub example somewhere so this actually makes sense

\subsection{Typing}

Similarly the typing of the outputted code is valid. The notible transfomrations are:

On line 20 the call to the eval consumer, the first argument is successfully converted from a box to a reference using firstly the \rust{*} operator, to dereference the box and then using the reference \rust{&} expression, to create a reference of the value.

In the eval consumer, the match expression is on a borrow of the expression. This means the fields exposed in the match pattern are also borrows. This is why the value (\rust{n}) must be dereferenced on line 8, and why they can be used directly for the calls to \rust{eval} on line 11.

Although the output typing is correct, it is clear in the demo function the typing could be simplified. On line 16 a Box of an expression is defined. This expression is then dereferenced (using the \rust{*} operator) before each use. This demo function could therefore be simplified by removing the box and the dereferences throughout. In this simple example this approach could work however if the box expression was later used as a box there is no guarantee that this would be simplest solution. Although not address in this paper, a metric could be determined for the "optimal" typing solution, for example based on the number of operations applied. This could then be used to find a simpler solution.

% TODO Talk about the wild card in the set example

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustexample{exp/oop}
    \caption{Input OOP program to transformer}
    \label{fig:trans-set-ex-input}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustoutput{exp/oop}
    \caption{Output FP program from transformer}
    \label{fig:trans-set-ex-output}
\end{subfigure}
\caption{Set transformation example}
\label{fig:trans-set-ex}
\end{figure}

\subsection{Macros}

The Expression example also demonstrates the successful transformation of macros arguments. However as mentioned in the implementation section \autoref{sec:}, this approach is very limited and relies on both the arguments to the macro being an expression and the result of the expression being used explicitly, rather than manipulating the expression it self.

Improvements to this approach could definitely be made, however a generalized solution for transforming macros is not possible given their flexibility. Going forward some macros would require custom transformations, which would have to be written by the developers before full transformations could be performed. 

\subsection{Inheritance}

% TODO add in example
% TODO xplain the example
In order to demonstrate the inheritance functionality, sections of the transformation of a Shape are included in \autoref{fig:}. The full example can be found in the \autoref{appx:}.

The trait includes two methods, one with a default implementation and one without. The method without the default implementation is handled as before, creating a consumer with a match for each struct with an implementation of the trait. The \rust{internal_angle} function, however is handled differently. Given the circle method overrides the default implementation, this is handled the same as before with a match arm created for it. In contrast, the triangle class, which does not override the method, does not have a match arm. This type is instead caught by the wildcard pattern which contains the transformed block from the trait default implementation.   

% TODO explian this waaay better
Note, the example default implementation returns an primative type, i32. Currently the transformer does not support returning the Self type of the trait as mentioned in \autoref{sec:}. This is beacuse the return type must be sized, and there is no guarantee Self type will be sized. 

Limited support could be added, life times or somethng...

\section{Additional features} 

\subsection{Mutability}

\autoref{fig:} shows the output of the transformation of the example discussed in \autoref{fig:}. This demonstrates how the mutability can be wrapped inside the constructor, such that the method returns a new instance of the class.

This demonstrates that the transformer can be extended to support multiple different styles of input. However currently the transformer only supports a single outputs style. This means if this style is transfomred from OOP to FP and back again, it will be transfomred in the pure OOP style outlined in \autoref{sec:}.
In the future it would be feasible for the transfomrer to support different outputs formats and styles. This would allow developers to not only select the paradigm by also the style within that paradigm to use.

Despite this the current implementation is very limited. The destructors cannot have ant return values, and the created implementations are not optimatal. \autoref{fig:} shows an ideal output, with the mutable variables removed. Although not in scope of this transformer these method bodies may be possible to simplify in the future.

\subsection{Generics}

\section{Wider picture}

Despite the improvements included in the Rust implementation of the transformer, there are still many limitations that prevent the tool from being widely useful in its current state. Expecting developers to transform sections of code before using the transformer removes the real benifits of the tool in a paractical setting.

One key limitation of the tool is the lack of support for different expressions. The current tool support a small subsect of possible expression in Rust which means many programs would be using unsupported feature. Examples of this include unnamed enums, and certain binary expressions. However, in most cases these missing feature can simply be added to the tool with minimal effort.

\subsection{Nested pattern matching and guards}

Similarly to \cite{food}, another key limitation is not supporting nested pattern matching. Nested pattern matching is a very useful tool in the functional decomposition style. Example

Nester pattern matching can also support matching on mutliple objects at the same time, as this is simply nested pattern matching on a tuple. Example 


% {\bf A topic-specific chapter} 
% \vspace{1cm} 

% \noindent
% This chapter is intended to evaluate what you did.  The content is highly 
% topic-specific, but for many projects will have flavours of the following:

% \begin{enumerate}
% \item functional  testing, including analysis and explanation of failure 
%       cases,
% \item behavioural testing, often including analysis of any results that 
%       draw some form of conclusion wrt. the aims and objectives,
%       and
% \item evaluation of options and decisions within the project, and/or a
%       comparison with alternatives.
% \end{enumerate}

% \noindent
% This chapter often acts to differentiate project quality: even if the work
% completed is of a high technical quality, critical yet objective evaluation 
% and comparison of the outcomes is crucial.  In essence, the reader wants to
% learn something, so the worst examples amount to simple statements of fact 
% (e.g., ``graph X shows the result is Y''); the best examples are analytical 
% and exploratory (e.g., ``graph X shows the result is Y, which means Z; this 
% contradicts [1], which may be because I use a different assumption'').  As 
% such, both positive {\em and} negative outcomes are valid {\em if} presented 
% in a suitable manner.

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

The Rust transformer demonstrates the feasibility of using the framework outlined by \weixin in \cite{food}, with a different languages. It shows that, the fundamental features from the transformer can be mapped to the rust language, with transformation of datattype, traits and their associated functions all possible. 

However to achieve this additional cosiderations are required. Primarily, Rust low level type system, introduces many additional challenges not addressed in the \cite{food} transformer. This requires the transformer to have a greater degree of type inference, maintaining the reference type as well as type name.

Rust was selected for this project as many multiparadigm languages exist with greater level of abstractions, whilst rust reamins low level and has a unique typing system. With this there are many differences from Scala. This allows Rust to demonstate the limitation in the generalization provided by 
\weixin. It is worth multi-paradigm langauges more similar to scala, may pose fewer issue in the implementation.

This paper also demostrates that the transfomrer can be extended to support addiitonal styles. Althrough limited in the current implemnetation, this could be extended in the future to support a variety of input and output formats.

There is still a large amount of work reuired for the tool to become a practical tool for developer. The basic mutability support and addiiton of generics in the paper is a step in the right direction to support more styles of input, however with the vast input spaces there are many feature which remain unsupported.

% \noindent
% The concluding chapter of a dissertation is often underutilised because it 
% is too often left too close to the deadline: it is important to allocate
% enough attention to it.  Ideally, the chapter will consist of three parts:

% \begin{enumerate}
% \item (Re)summarise the main contributions and achievements, in essence
%       summing up the content.
% \item Clearly state the current project status (e.g., ``X is working, Y 
%       is not'') and evaluate what has been achieved with respect to the 
%       initial aims and objectives (e.g., ``I completed aim X outlined 
%       previously, the evidence for this is within Chapter Y'').  There 
%       is no problem including aims which were not completed, but it is 
%       important to evaluate and/or justify why this is the case.
% \item Outline any open problems or future plans.  Rather than treat this
%       only as an exercise in what you {\em could} have done given more 
%       time, try to focus on any unexplored options or interesting outcomes
%       (e.g., ``my experiment for X gave counter-intuitive results, this 
%       could be because Y and would form an interesting area for further 
%       study'' or ``users found feature Z of my software difficult to use,
%       which is obvious in hindsight but not during at design stage; to 
%       resolve this, I could clearly apply the technique of Smith [7]'').
% \end{enumerate}

% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a databased) then inported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.
%
% We would recommend using BiBTeX, since it guarantees a consistent referencing style 
% and since many sites (such as dblp) provide references in BiBTeX format. 
% However, note that by default, BiBTeX will ignore capital letters in article titles 
% to ensure consistency of style. This can lead to e.g. "NP-completeness" becoming
% "np-completeness". To avoid this, make sure any capital letters you want to preserve
% are enclosed in braces in the .bib, e.g. "{NP}-completeness".

\backmatter

\bibliography{dissertation}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendicies; these are 
% the same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

% \chapter{An Example Appendix}
% \label{appx:example}

% Content which is not central to, but may enhance the dissertation can be 
% included in one or more appendices; examples include, but are not limited
% to

% \begin{itemize}
% \item lengthy mathematical proofs, numerical or graphical results which 
%       are summarised in the main body,
% \item sample or example calculations, 
%       and
% \item results of user studies or questionnaires.
% \end{itemize}

% \noindent
% Note that in line with most research conferences, the marking panel is not
% obliged to read such appendices. The point of including them is to serve as
% an additional reference if and only if the marker needs it in order to check
% something in the main text. For example, the marker might check a program listing 
% in an appendix if they think the description in the main dissertation is ambiguous.

% =============================================================================

\end{document}
