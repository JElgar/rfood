% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the academic year, say 2013/14 say).
                 

\documentclass[ oneside,% the name of the author
                    author={James Elgar},
                % the degree programme: BSc, MEng, MSci or MSc.
                    degree={MEng},
                % the dissertation    title (which cannot be blank)
                     title={Bidirectional transformer between functional and \\ object-oriented programming in Rust},
                % the dissertation subtitle (which can    be blank)
                  subtitle={}]{dissertation}
                  
% Commands
\usepackage{hyperref}
\newcommand{\weixin}{Anonymous }

\usepackage{listings}
\usepackage{xcolor}

% \usepackage{minted}
\usepackage[outputdir=../]{minted}
\newcommand{\rust}[1]{\mintinline{rust}{#1}}

\newcommand{\codefile}[2]{\inputminted[xleftmargin=20pt,linenos, breaklines]{#1}{#2}}
\newcommand{\rustfile}[1]{\codefile{rust}{../src/#1.rs}}
\newcommand{\rustexample}[1]{\rustfile{examples/#1}}

\usepackage{xcolor} % to access the named colour LightGray
\definecolor{LightGray}{gray}{0.9}

\usepackage{caption}
\usepackage{subcaption}

\begin{document}

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter


%\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Abstract}

% TODO I think is should really be in the intro, focus more on what actually happens
There are benefits to using both object orient and functional programming. With modern programming languages adopting traditional functional feature, developer often have the choice between the two paradigms. However once a paradigm is selected switching can be a time consuming process. In \cite{food} \weixin proposes a framework for transforming between these decomposition styles. This paper explorers the generality of this approach through an implementation of the transformer in rust. 

The key contributions of this paper are:

\begin{quote}
\noindent
\begin{itemize}
    \item Created an implementation of the \cite{food} transformer in rust
    \item Extend the \cite{food} transformer to support a basic implementation of the rust type system
    \item Explored extensions to the transformer including generics and mutability
\end{itemize}
\end{quote}

% {\bf A compulsory section, of at most 300 words} 
% \vspace{1cm} 

% ---- Reference/Example ---- %
% \noindent
% This section should pr\'{e}cis the project context, aims and objectives,
% and main contributions (e.g., deliverables) and achievements; the same 
% section may be called an abstract elsewhere.  The goal is to ensure the 
% reader is clear about what the topic is, what you have done within this 
% topic, {\em and} what your view of the outcome is.

% The former aspects should be guided by your specification: essentially 
% this section is a (very) short version of what is typically the first 
% chapter. If your project is experimental in nature, this should include 
% a clear research hypothesis.  This will obviously differ significantly
% for each project, but an example might be as follows:

% \begin{quote}
% My research hypothesis is that a suitable genetic algorithm will yield
% more accurate results (when applied to the standard ACME data set) than 
% the algorithm proposed by Jones and Smith, while also executing in less
% time.
% \end{quote}

% \noindent
% The latter aspects should (ideally) be presented as a concise, factual 
% bullet point list.  Again the points will differ for each project, but 
% an might be as follows:

% \begin{quote}
% \noindent
% \begin{itemize}
% \item I spent $120$ hours collecting material on and learning about the 
%       Java garbage-collection sub-system. 
% \item I wrote a total of $5000$ lines of source code, comprising a Linux 
%       device driver for a robot (in C) and a GUI (in Java) that is 
%       used to control it.
% \item I designed a new algorithm for computing the non-linear mapping 
%       from A-space to B-space using a genetic algorithm, see page $17$.
% \item I implemented a version of the algorithm proposed by Jones and 
%       Smith in [6], see page $12$, corrected a mistake in it, and 
%       compared the results with several alternatives.
% \end{itemize}
% \end{quote}

% -----------------------------------------------------------------------------


\chapter*{Dedication and Acknowledgements}

{\bf A compulsory section}
\vspace{1cm} 

\noindent
It is common practice (although totally optional) to acknowledge any
third-party advice, contribution or influence you have found useful
during your work.  Examples include support from friends or family, 
the input of your Supervisor and/or Advisor, external organisations 
or persons who  have supplied resources of some kind (e.g., funding, 
advice or time), and so on.

% -----------------------------------------------------------------------------

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl



% -----------------------------------------------------------------------------

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures and tables.  These are all compulsory parts of the dissertation.

\tableofcontents
\listoffigures
\listoftables

% -----------------------------------------------------------------------------



\chapter*{Ethics Statement}

This project did not require ethical review, as determined by my supervisor, [fill in name].

% -----------------------------------------------------------------------------

% \noindent
% This section should present a detailed summary, in bullet point form, 
% of any third-party resources (e.g., hardware and software components) 
% used during the project.  Use of such resources is always perfectly 
% acceptable: the goal of this section is simply to be clear about how
% and where they are used, so that a clear assessment of your work can
% result.  The content can focus on the project topic itself (rather,
% for example, than including ``I used \mbox{\LaTeX} to prepare my 
% dissertation''); an example is as follows:

\chapter*{Supporting Technologies}

\noindent
To implement the transformer I used the following technologies:

\begin{quote}
\noindent
\begin{itemize}
\item I used the Rust programming language to implement the transformer. \url{https://www.rust-lang.org/}
\item I used the syn package to create and parse the AST. \url{https://docs.rs/syn/latest/syn/}
\item I used the quote package to covert the transformed AST back to rust code. \url{https://docs.rs/quote/latest/quote/}
\item I used rustfmt (a component of the Rust language) to format the transformed output code.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

{\bf An optional section}
\vspace{1cm} 

\noindent
% Any well written document will introduce notation and acronyms before
% their use, {\em even if} they are standard in some way: this ensures 
% any reader can understand the resulting self-contained content.  

% Said introduction can exist within the dissertation itself, wherever 
% that is appropriate.  For an acronym, this is typically achieved at 
% the first point of use via ``Advanced Encryption Standard (AES)'' or 
% similar, noting the capitalisation of relevant letters.  However, it 
% can be useful to include an additional, dedicated list at the start 
% of the dissertation; the advantage of doing so is that you cannot 
% mistakenly use an acronym before defining it.  A limited example is 
% as follows:

\begin{quote}
\noindent
\begin{tabular}{lcl}
AST                 &:     & Abstract Syntax Tree                                        \\
OOP                 &:     & Object Oriented Programming                                 \\
FP                  &:     & Functional Programming                                      \\
FOOD                &:     & Functional and Object Oriented Decomposition                \\
\end{tabular}
\end{quote}


% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter


\chapter{Introduction}
\label{chap:context}

% {\bf Unlike the frontmatter up to and including the Summary of Changes, which you should not deviate from, Chapters 1--5 represent a suggested outline only. This outline will only be appropriate for a specific type of project. You should talk with your supervisor about the best way to structure your own dissertation, but ultimately the choice is yours. However, almost every project will want to include the content discussed in these chapters in some way. For more advice on structuring your dissertation, see the unit handbook.}
% \vspace{1cm} 

% \noindent
% This chapter should introduce the project context and motivate each of the proposed aims and objectives.  Ideally, it is written at a fairly high-level, and easily understood by a reader who is technically competent but not an expert in the topic itself.

% In short, the goal is to answer three questions for the reader.  First, what is the project topic, or problem being investigated?  Second, why is the topic important, or rather why should the reader care about it?  For example, why there is a need for this project (e.g., lack of similar software or deficiency in existing software), who will benefit from the project and in what way (e.g., end-users, or software developers) what work does the project build on and why is the selected approach either important and/or interesting (e.g., fills a gap in literature, applies results from another field to a new problem).  Finally, what are the central challenges involved and why are they significant? 
 
% The chapter should conclude with a concise bullet point list that summarises the aims and objectives.  For example:

% \begin{quote}
% \noindent
% The high-level objective of this project is to reduce the performance 
% gap between hardware and software implementations of modular arithmetic.  
% More specifically, the concrete aims are:

% \begin{enumerate}
% \item Research and survey literature on public-key cryptography and
%       identify the state of the art in exponentiation algorithms.
% \item Improve the state of the art algorithm so that it can be used
%       in an effective and flexible way on constrained devices.
% \item Implement a framework for describing exponentiation algorithms
%       and populate it with suitable examples from the literature on 
%       an ARM7 platform.
% \item Use the framework to perform a study of algorithm performance
%       in terms of time and space, and show the proposed improvements
%       are worthwhile.
% \end{enumerate}
% \end{quote}

\noindent
With the rapid adoption of functional and object oriented features in modern programming languages, developers now have the choice of which paradigm to use in different situations. Both paradigms come with distinct advantages and disadvantages, and such the correct choice for a given feature can often be a tricky decision. In any case a "correct" decision may not be possible as both paradigms could have different values in the same feature. If a developer makes a choice they later regret, refactoring to another paradigm can be an extremely slow process.

Therefore, to truly harness the value of both paradigms, in \cite{food} \weixin explores the idea of a transformer, which allows developer to automatically transform between these two paradigms. This allows developers to instantly switch the style of there code depending on the task.

\weixin proposes a framework to transform between these two decomposition styles and create an implementation in Scala to demo its functionality.

The goal of this paper is to replicate this implementation in Rust. Rust was created by the Mozilla foundation with the intention of replacing C++ in their stack. It has a strong focus on performance and safety which it achieves partly through its borrow checker. With Rust being a low level language it exposes concepts to the developer which are not available in Scala, such as references and borrows. This requires extension to the existing transformer to allow it to transform with these concepts. 

% -----------------------------------------------------------------------------

% \noindent
% This chapter is intended to describe the background on which execution of the project depends. This may be a technical or a contextual background, or both. The goal is to provide a detailed explanation of the specific problem at hand, and existing work that is relevant (e.g., an existing algorithm that you use, alternative solutions proposed, supporting technologies).  

% Per the same advice in the handbook, note there is a subtly difference from this and a full-blown literature review (or survey).  The latter might try to capture and organise (e.g., categorise somehow) \emph{all} related work, potentially offering meta-analysis, whereas here the goal is simple to ensure the dissertation is self-contained.  Put another way, after reading this chapter a non-expert reader should have obtained enough background to understand what \emph{you} have done (by reading subsequent sections), then accurately assess your work against existing relevant related work.  You might view an additional goal as giving the reader confidence that you are able to absorb, understand and clearly communicate highly technical material and to situate your work within existing literature.

\chapter{Background}
\label{chap:technical}

% Following on from this dudes work with rust

This paper extends and evaluates the work of \weixin in \cite{food}. \weixin proposes a framework for a sub set of possible transformations and creates an implementation of this framework in a single programming language Scala. 

The work of \weixin in \cite{food} focuses on the transformation of the

\section{Decomposition}

Decomposition in computer science, is how a problem is broken down into smaller parts. Object oriented programming uses classes or objects for this whilst functional programming uses functions and data types.

The pure OOP style described by Cook in \cite{cook} and used by \weixin to restrict the input to the transformer, maps closely to the object style in Rust. In pure OOP interfaces are used as types. Interfaces describe required operations which a class must implement to be of this type. These operations are referred to as destructors throughout this paper, as these operations "tear down" the object to another type. Classes which define these operations are then implementations of this interface. In Rust, interfaces are implemented as traits, whilst classes are implemented as structs. Structs can then explicitly implement traits with an impl statement as shown in \autoref{fig:dec-ex-oop}. 

With this style, destructors are used as method calls on an object that implements the interface. These methods can optionally have \verb|self| as the first argument, which allows self-reference within these methods, as shown in \autoref{fig:dec-ex-oop}. If required this argument is automatically passed in.

%TODO fix the 2nd sentence
In functional style types are algebraic datatypes with various constructors. Constructor are variation of this type. Consumers can then be defined, which are functions that take in a datatype and pattern match on its constructors. The equivalent in Rust, are Enums. Enums contain Enum variant which are the constructors of the datatypes. Consumers are then function which take in an instance of the Enum as the first argument.  \autoref{fig:dec-ex-fp} shows an equivalent example to above in functional style.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \rustexample{shape/oop}
    \caption{Object Oriented Programming - Implementation of trait for structs}
    \label{fig:dec-ex-oop}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustexample{shape/fp}
    \caption{Functional Programming - Datatype with two consumers}
    \label{fig:dec-ex-fp}
\end{subfigure}
\caption{Rust Decomposition Examples}
\label{fig:dec-ex}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \codefile{scala}{../scala/examples/src/main/scala/RecursiveType.scala}
    \caption{Scala recursive class}
    \label{fig:recursive-typedef-scala}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \rustexample{list/oop}
    \caption{Rust recursive struct}
    \label{fig:recursive-typedef-rust}
\end{subfigure}
\caption{Recursive Type Definitions}
\label{fig:recursive-typedef}
\end{figure}



\section{Rust Type System}
"Rust is the first industry-supported programming language to overcome the longstanding tradeoff between the safety guarantees of higher-level languages (like Java) and the control over resource management provided by lower-level “systems programming” languages (like C and C++)" \cite{rustsafesystems}. Rust achieves this by using an ownership type system. This gives developers the control to express low level concept, in a similar way to languages such as C, whilst maintaining the safety of higher level languages by catching potential errors at compile time with the strong type system. 

This unique and strong type system of Rust presents additional challenges when creating a transformer for the rust programming 

\subsection{References language}

In many high level languages, such as Scala, developer are not required to assign and free memory explicitly. In Scala a garbage collector is used to automatically free memory that can no longer be accessed by the program. This improves memory safety, as memory cannot be accessed after it has been freed. In contrast low level languages like C, requires developers to create and free memory as required. This gives developers greater control and has efficiency gains as the expensive garbage collection task is not required. The trade off is code can be unsafe as developers can try and access memory that has previously been freed.

\subsection{Recursive types}

For a compiler to create an instance of a type, it must know the exact size of the object. Recursive types are types which reference themselves. See \autoref{fig:recursive-typedef} for an example, where \verb|Cons| is a \verb|List| which contains another \verb|List|. At face value it would therefore seem impossible to know the size of a \verb|Cons| object. In Rust this is not allowed and therefore for recursive types to be defined, the developer must use a pointer. In Rust there are many kinds of pointers, but the simplest for this use case is the \verb|Box|. 
Moreover when creating a pointer to a trait the developer must use the \verb|dyn|, which tells the compiler the size of the reference object is not fixed (in this case could either be \verb|Empty| or \verb|Cons|). 

In Scala this concept is obfuscated by the high level API exposed to the developer and thus they are allowed to define recursive types directly (as shown in \autoref{fig:recursive-typedef-scala}).  

In a similar vein, return types of methods cannot be the trait type itself, without the use of a dynamic pointer. 

% TODO This needs to validation
Although traits are not a fixed size, enums are. This means in OOP style the dyn tokens must be removed. With these additional requirements of the rust typing system, additional steps are required to transform the code.



% -----------------------------------------------------------------------------

% \noindent
% This chapter is intended to describe what you did: the goal is to explain
% the main activity or activities, of any type, which constituted your work 
% during the project.  The content is highly topic-specific, but for many 
% projects it will make sense to split the chapter into two sections: one 
% will discuss the design of something (e.g., some hardware or software, or 
% an algorithm, or experiment), including any rationale or decisions made, 
% and the other will discuss how this design was realised via some form of 
% implementation.

% This is, of course, far from ideal for {\em many} project topics.  Some
% situations which clearly require a different approach include:

% \begin{itemize}
% \item In a project where asymptotic analysis of some algorithm is the goal,
%       there is no real ``design and implementation'' in a traditional sense
%       even though the activity of analysis is clearly within the remit of
%       this chapter.
% \item In a project where analysis of some results is as major, or a more
%       major goal than the implementation that produced them, it might be
%       sensible to merge this chapter with the next one: the main activity 
%       is such that discussion of the results cannot be viewed separately.
% \end{itemize}

% \noindent
% Note that it is common to include evidence of ``best practice'' project 
% management (e.g., use of version control, choice of programming language 
% and so on).  Rather than simply a rote list, make sure any such content 
% is useful and/or informative in some way: for example, if there was a 
% decision to be made then explain the trade-offs and implications 
% involved.


\chapter{Project Execution}
\label{chap:execution}

\section{Setup}

Before any transformations take place, the provided code is transformed into an abstract syntax tree (AST). This provides a concrete format to represent the code syntax, keeping the transformer separate from the code parsing stage. Rust provides access to the \verb|rustc| package, which is its compiler. This contains the \verb|rustc_ast| and \verb|rustc_parse| package which are responsible for this parsing.

However as Rust is a relatively new language many of these features are unstable. This means they are only available with the nightly compiler and the interface is likely to change. To avoid this, the syn package is used. "Syn is a parsing library for parsing a stream of Rust tokens into a syntax tree of Rust source code" \cite{syn}. This provides a stable interface for parsing and manipulating the AST.
The main issue with Syn, is it geared toward helping parse code in Rust procedural macros. This means it has a stronger focus on understanding an AST rather than manually manipulating it and creating items. For this reason a fork of the Syn project was created to allow access to an additional library, which made creating more AST objects possible. The fork can be found at \url{https://github.com/jelgar/syn}.

Another requirement, is once the AST has be transformed, it must be converted back into Rust code. To achieve this the \verb|quote| and \verb|rustfmt|. The \verb|quote|  package converts the AST back into tokens of source code, whilst the \verb|rustfmt| package formats the outputted code in a consistent way.

Finally a CLI was created to allow developers to parse in files they want to transform and to specity the direction (OOP to FP or FP to OOP).

\section{High level overview}

The transformation is split into 4 step:

\begin{enumerate}
    \item Collect global context
    \item Transform the types
    \item Recursively transform other items
\end{enumerate}


\section{Context}

Prior to transforming any expressions or statements, the transformer must know the context. This consists of two components, the global context, $\Delta$, and the environment, $\Gamma$.

\subsection{Global Context}

In \cite{food} \weixin uses the name "global context", represented by $\Delta$, to represent the store with  the following attributes:

\paragraph{Datatypes.} Datatypes are equivalent to Enums in Rust.
\paragraph{Interfaces.} Interfaces are equivalent to Traits in Rust.
\paragraph{Constructor.} Constructor create an instance of a given datatype. In Rust the equivalent is an Enum Variant.
% TODO update this if it can handle multiple inheritance
\paragraph{Generator.} Generators create an instance of a given interface. In Rust the equivalent is a struct which has an implementation of a trait (the interface). Note in this implementation a struct cannot be transformed if it implements multiple traits. 
\paragraph{Consumer.} Consumers are functions that operate on a datatype. In Rust these are functions, with the first argument being a Datatype (or some reference to a datatype).
\paragraph{Destructor.} A destructor is a function that operates on an interface. In Rust these are methods defined on the traits, which must either have a default implementation (in the trait) or an implementation in the generators impl of the trait.

\vspace{10pt}

In the current implementation of the transformer "global context" is an accurate name, as the Datatypes are collected at the beginning of the transformation. However, this does oversimplify the reality as it is possible to define Datatypes within scopes. In this case it would be more accurate to generate Gamma recursively (along side other transformations) to ensure all scoping is correct. Note, however, the top level types must be collected before any other types are transformed, as types defined after a function can still be referenced in that function.

% TODO did the change?
For the Rust implementation the Visitor pattern was used. Provided by the \verb|syn| package these traits (\verb|visit| and \verb|visit_mut|) allow the developer to view/mutate selected types of values in the AST. The default implementation of the visitor trait methods, recursively visits all expression. Using this technique ignores the scope of the current expression meaning all types and methods are collected and therefore assumes the developer has not renamed any types/function in a nested scope. This technique could be improved by a recursive approach to collecting this context along side other transformations, to ensure the correct versions of all the attributes are collected.

\subsection{Environment}

The environment stores the types of variables currently on the stack. 
In the Scala implementation a type is encoded as string, however this is insufficient for Rust as reference types must also be kept track of. For this reason in the Rust implementation types are encoded as \verb|DeltaType| struct. 
This stores both the type name stored as an Ident (a simple wrapper around String provided by \verb|syn|) and the reference type.

The reference type is currently implemented as an Enum, with a variant for \verb|Box|, \verb|Reference| and \verb|None|. These reference types are later used to transform the types of expressions as required.

% TODO sort this out
The key limitation of the current implementation is these are not stored as recursive types. This means nested references such as reference of boxes cannot be encoded. 

\subsection{Type of Expression}

With these two contexts it is possible to perform basic type inference on expressions. The following rules are used to perform this inference:

\begin{itemize}
    \item For literals, for example \rust{1} or \rust{true}, a basic type of the literal is returned. In the current implementation only a subset of literals are supported (\rust{i32}, \rust{f32}, \rust{bool}) and they are assumed to be a specific type (i.e. \rust{i32} rather than a generic \verb|Int|).
    \item For paths, which are expressions referring to some variable in the environment, the type of the variable from the environment is returned 
    \item For struct/enum instatioation, the struct or enum type is returned.
    \item For method or function calls, the return type of the function is returned
    \item For field expressions, for example \rust{self.a} or \rust{item.field}, the type of receiver (the thing the field is being accessed from) is determined using gamma. The type of the field which is being accessed is then returned.
\end{itemize}

% TODO talk about casting
There are a few key limitations to the current type inference:

\begin{itemize}
    \item  Currently the transformer only support single file transformations. This means it cannot perform type inference on imported or built in functions. Supported has been added for a small subset of built in functions such as clone but this is currently a manual process rather than the importing definitions.
    \item  Currently a limited subset of expressions are supported. With time the types of expressions that are supported could be extended.
    \item Currently type names are encoded as strings and as such no casting is supported. For example when if a literal is marked as an \rust{i8} the transformer is unable to cast that to an \rust{i32} if required.
\end{itemize}

One approach to resolving these issues is to use the built in type inference in the rust compiler. The rust compiler parses the source code (including all dependencies) and converts it to an AST, then to a HIR (High-Level Intermediate Representation) and then finally a THIR (Typed High-Level Intermediate Representation), where the typing information is added. It collects (and checks) these types using the \verb|rustc_typecheck| package which can be exposed using the nightly compiler. Currently the documentation for this package is very limited and the API for all of rustc remains unstable. For this reason this approach was not developed for this version of the compiler, but could create some tangible improvements to the type checking in the future. Another improvement of using this crate is the type checking could automatically track with the version of rust the developer is using. This means as rust add or changes features in the langauge the type inference of the transformer would not have to be updated directly.

\section{Type Transformations}

Having collected the global context, the next stage of the transformation is to transform the types, and their associated functions. For OOP to FP this means transforming the traits, structs and impls and for FP to OOP this means transforming the enums and consumers.

\subsection{OOP to FP}

The top level function for transforming from OOP to FP is \verb|transform_trait|. This does the following:

\begin{enumerate}
    \item  Create an enum variant for each generator of the trait (structs that implements the trait). The enum variants will have the same names as the structs and the same fields. The type of the fields will be transformed such that any \verb|dyn| reference, the dyn is removed. Structs can also expose fields as public with the \verb|pub| token, this is automatically the case for all fields of an enum variant so this is removed.
    \item Create an enum with the same name as the original trait and the variants collected in step 1.
    \item Transform each destructor of the trait into a consumer.
\end{enumerate}

The first stage to transforming a destructor is transforming its signature. This is done by...

Assuming each struct impl has an implementation for this destructor (see \autoref{sec:inheritance} for why this is not always the case) the next stage of the transformation is to create a match statement with an arm for each generator of the trait. The arms consist of two key elements, the match pattern and the body. The match pattern is the enum which the generator is transformed to. The current implementation shows all values in the enum. An improvement would be to ignore (using the \verb|..| syntax) fields which are not used the in body.

The body is a transformed version of the body from the generator implementation of the method. The body is transformed by replacing any reference to self with the new consumer function argument.
Additional transformations, such as fixing the return type and use of transformed types are handle by the \verb|trasnform_expr| function which is described in detail in \autoref{sec:item-trans}.

% TODO write some algos to show this
% \begin{algorithm}[t]
% \For{$i=0$ {\bf upto} $n$}{
%   $t_i \leftarrow 0$\;
% }
% \caption{This is an example algorithm.}
% \label{alg}
% \end{algorithm}

\subsection{FP to OOP}

When transforming from FP to OOP a similar structure is used. The top level function is \verb|transform_enum| and is called on each datatype in the global context. This creates a trait with the same fields, but adding in \verb|dyn| references if the type of the field is the enum type which is being transformed.

An implementation block for each of the created struct is then added. These implementation blocks contain a method for each of the consumers. The signature of these methods ares equivalent to those of the consumer but with the first argument (which must be the enum type) replaced with self. 
As the reverse to OOP to FP, any fields in the signature which are Box of the enum type, the dyn token is added.
% TODO Note if how we handle different borrow types of self
% TODO as required is garbage 
The body of these methods are then transformed. This adds self to method and field calls as required.


\section{Item Transformations}
%TODO Talk about this
\label{sec:item-trans}

After the type transformations are completed the next stage of the transformation is to updates uses of the transformed types. This includes instantiations of the type as well as any destructor/consumer calls.

This transformation is achieved using a recursive function called \verb|transform_expr|. This function takes an expressions and the current contexts and transforms both itself and any nested expressions. For example when transforming an \verb|if| expression both the conditional (the expression inside the if) and block expression (the expression run if the conditional expression is true) are transformed.

The key transformations that this function performs depends on the direction of the transformation. For OOP to FP, the function transforms any calls to destructor or generators. Whilst in FP to OOP it transforms any calls to consumers or constructors.

\subsection{OOP to FP}

\paragraph{Generators. } Generator calls are instations of structs. Gamma is used to determine if the struct is infact a generator. If so the expression is transformed into the instantiation of an enum using a constructor. In Rust this is also represented as a struct expression, however the path to the struct will change. This is because by default, enum variant are not in scope and can be reference by the enum it self. For example \rust{Light::Dimmer}. The alternative to this is to add the enum variant to the scope using \rust{use Ligth::*} however this increases the possibility of name collision and is generally considered bad practice. The other required transformation is ensuring the type of each field in the constructor is correct for the transformed signature. This is dicussed in more detail in \autoref{sec:type-transforms}.

\paragraph{Destructors. } Destructor calls in rust are method calls. Firstly delta is used to determine the type of the receiver, which is the expression the method is being called on. Gamma is then used to check if this type is a datatype (or a generator of that datatype) and that the method is a destructor of that type. If so the method call is replaced with a call to the new consumer. The receiver of the method call is transformed to the first argument of this function call. Similarly to above the nested expressions (the new function arguments and the ) are also recursively transformed.

\subsubsection{FP to OOP}

FP to OOP reflects the logic of OOP to FP.

\paragraph{Constructors. }  Gamma is also used to check it is a constructor, if so the enum prefix is removed (as the enum variant is transformed to a top level struct) and the nexted expressions are transformed.

\paragraph{Consumer. } For calls to consumers, gamma is used to check if a function call is a consumer. If so it is replaced by a method call with the first argument of the function call as the receiver of the method call. Equally the sub expressions are transformed.

\subsection{Rust Type Transformations}

% TODO how is the required return type determined for an expression?
% TODO how are types of an expression determined - this should probably go in delta but it should also talk about blocks....
% TODO how are these types encoded (EType and DeltaType) this should also probably be in Delta context
% TODO how is the transformation applied
% TODO type simplification

Unlike in the Scala implementation of the transformer the signatures of types' associated functions can change during the transformation. In general this is not the type itself that changes, but rather the reference type (such as converting a type to a box of the type). This therefore requires additional steps in the transformation to ensure the types of expressions are correct. To achieve this the \verb|transform_expr| function is also passed an expected return type.

When recursively transforming nested expression is it the required type of each sub expression must be determined. For each type this is determined as follows:

\begin{itemize}
    \item Method/Function call - The signature of the function states the expected type of each of the argument expressions. For a method call receiver, in the current implementation any type can be returned. %TODO evaluate this
    \item Block - For block expressions, all expression should return None, except the last expression which should have the same return type requested for the whole block.
    \item Generator/Constructor calls - The required types of each field can be determined by the field types (signature) from gamma.
    \item Match - For match expression each arm is expected to return the same type as the required type for the whole match statement
    \item Return - The inner expression should match the required type of the return expression
    % TODO add support for if else
    \item TODO add support for if else  
\end{itemize}

Given these expected return types, it is then the responsibility of the \verb|transform_expr| function to also transform the types. This is achieved using \verb|transform_expr_type|.  

In the current implementation, reference types are encoded as a basic enum (either ref, box or none). Given, in general, only the reference type of types change, the \verb|trasnsform_expr_type| can only perform basic transformation on the reference type. The current supported transformations are:

\begin{itemize}
    \item Box/Reference $\xrightarrow{}$ None - Achieved by dereference
    \item None $\xrightarrow{}$ Box - Uses \rust{Box::new} method to create box of provided expression
\end{itemize}

The return types express what the given expression should return after it has been transformed. These return types are expressed as one of the following:

\begin{itemize}
    \item None - No return value
    \item DeltaType - Contains a DeltaType struct to encode the expected return type
    \item Any - Any type can be returned (no transformation of the return type will be made)
\end{itemize}



\verb|transform_expr| is the core of this recusrsive transformation process. For each type of expression this transforms the expression it self, as well as any nested experssions.

The main transformations performed by this function is updating the use of transformed types. For OOP to FP transformations this involves replacing method calls with function calls to the consumers, whilst for FP to OOP it does the reverse. 

\begin{itemize}
    \item A consumer/destructor is transformed and the return types changes. E.g. \verb|T| to \verb|Box<T>|. This can have knock on effect where this result is used later.
    \item A consumer/destructor is transformed and the argument type changes
\end{itemize}

The transforming of types is handled in the \verb|transform_expr_type| function. This takes the current type of the expression and the 


\section{Inheritance}
\label{sec:inheritance}

Inheritance is one of the central concepts in object oriented programming \cite{cook_palsberg_1989}. It allows developer to share logic between linked classes, preventing duplicate logic. Rust, however, does not support traditional inheritance. Instead it opts for composition, which allows structs to contain other structs directly and default implementations in traits.

A limitation of default implementations is Rust cannot call methods on dynamic types, or use them to create structs.
% TODO explain this more
A clear example of where this is an issue, is the union of sets. Ideally a defualt implementation could be created, which returns a new Union struct with the two provided sets. However, given the Self type could be any set (and therefore it is not Sized), self cannot be used to create a new struct. For this reason the Rust implementation does not support default methods other than basic types.

The FP equivalent to these default trait implementations are wildcard matches in consumers. These allow the consumer to use the same implementation for any unmatched expression.

\paragraph{OOP to FP.} If any impls of a trait which is being transformed are missing any of the required methods, then a wildcard arm should be added for that consumer matching that method. The body of the consumer should be taken from the traits default implementation with the same transformations applied to the other arms. The only difference between the transformations is no additional delta collection is required as the pattern match patter is just the underscore token. 

\paragraph{FP to OOP.} If a consumer contains a wildcard match, then the return type of the arm must be checked. If it returns a simple (Sized) type, then a default implementation of the trait can be created and an implementation in the impl block is not required. However if the return type is not sized then a default implementation cannot be used. In this case a copy of the wildcard expression is created for each of the impl blocks. 
% TODO talk about shared function
% Although it may seem preferable to create a shared function to handle this logic, this is also not possible (withas the type of the 

\section{Macros}

In Rust macros are a form of "metaprogramming", meaning they allow developers to write code that writes other code. 

Although not unique to Rust, macros play a far greater role in Rust that most other programming languages, and for this reason it is an important topic to cover. 

In Rust there are 2 kinds of macros, declarative and procedural. Macros operate over token trees which contain the tokens parsed by the compiler. SAY SOMETHING BETTER HERE  https://reberhardt.com/cs110l/spring-2020/slides/lecture-17.pdf

Macros are an incredibly powerful tool and are very effective at reducing code duplication. However this flexibility creates many difficulties for the transformer. Given the macros inputs are parsed as tokens, it is not possible to predict what the macro will do to the input. For example, if a macro was created to strip method calls, it would not be possible for the transformer to predict this behaviour. In this case it could there transform a method call to a function and change the resultant meaning of the expression. 
Therefor it is not possible to reliably transform the inputs of macros. As a compromise the current implementation attempts to parse the inputs to any macros as comma separated set of expressions. If this is successful it will transform each expression as before using \verb|transform_expr|. After the transformations it will covert the transformed expressions back to the tokens format.

In many cases this limited approach is sufficient to maintain the correctness of the code and transform the required expressions. Critically this approach allows the use of basic macros included in rust such as \rust{println!} and \rust{vec!}. However as mentioned there is no guarantee of correctness when using these transformations with arbitrary macros.

\chapter{Extensions to the Transformer}

\weixin's transformer focuses on the transformations between Pure OOP style from \cite{cook} and FP style. For the transformer to be a truely useful tool for developers, it must support the majority of styles of programming. 
The requirement to use pure OOP style is extremely restrictive, and often requires large refactoring of code before it can be transformed.
This means the types of OOP programs that can be transformed are limited.

\section{Mutability}

A common style for object oriented programming is to include methods that mutate the state of the class it self. An example is the getter and setter pattern where one functions on a class is responsible for returning a value and another to mutates it, as shown in \autoref{fig:mutable-example}.

In rust, variables are immutable by default. To make a varaible mutable the \rust{mut} token must be used before variable assignment. This can be seen on line 3 of the example in \autoref{fig:mutable-example} where the \verb|mut| token allows the self object to be mutated. This is then used for the implementation of \rust{set_brightness} on line 12 to set the brightness of the Dimmer. 

% Horrible sentence
"Functional programming aims to minimise or eliminate side-effects" \cite{fp-uok}. This can have many benefits such as reducing complexity of programs and making them easier to test. For this reason when transforming this style of OOP programming to FP, the mutations should wrapped within the consumers. This means the consumers should parse in an instance, and a new instance should be returned by the consumer. When the consumer is used, the parsed in object should then be overwritten by the output of the function.  In simple cases this inherently make the transformed function pure by removing the mutation of the argument.

To achieve this kind of transformation, the transfomer check each destructor, checking if the self argument is mutable, if so the following additional steps are added:

\begin{itemize}
    \item Update the arguments of the signature of the destructor, updating the first argument to remove the mutability and borrow. Removing the borrow means the ownership of the passed in variable is taken and thus it will go out of scope after the consumer is called. This is desireable as the variable should be overwritten anyway using the result of the consumer. 
    \item Update the return type of the signature of the destructor. Currently only destructor with no returns are supported. Set the return to trait type that destructor is for.
    \item In the match statement, make each field mutable. This allows reassignment of these local variables.
    \item Replace all field assignments on self with field assignments on these new mutable varaibles. 
    \item Return a new instance of the matched enum with the values of the fields equal to the local variable from the match expression. 
    \item When transforming method calls to this object, ensure to also overwrite the receiver with the output of the method.
\end{itemize}

This has 2 key limitations. Firstly the method cannot return any other values. An extension to the existing transfomrer could be added to support returning tuples.



\begin{figure}
\centering
\rustexample{mutable/oop-basic}
\caption{Mutable class example}
\label{fig:mutable-example}
\end{figure}

\section{Generics}

% \subsection{Multiple Inheritance}

% \begin{figure}[t]
% \centering
% foo
% \caption{This is an example figure.}
% \label{fig}
% \end{figure}

% \begin{table}[t]
% \centering
% \begin{tabular}{|cc|c|}
% \hline
% foo      & bar      & baz      \\
% \hline
% $0     $ & $0     $ & $0     $ \\
% $1     $ & $1     $ & $1     $ \\
% $\vdots$ & $\vdots$ & $\vdots$ \\
% $9     $ & $9     $ & $9     $ \\
% \hline
% \end{tabular}
% \caption{This is an example table.}
% \label{tab}
% \end{table}

% \begin{algorithm}[t]
% \For{$i=0$ {\bf upto} $n$}{
%   $t_i \leftarrow 0$\;
% }
% \caption{This is an example algorithm.}
% \label{alg}
% \end{algorithm}
% 
% \begin{lstlisting}[float={t},caption={This is an example listing.},label={lst},language=C]
% for( i = 0; i < n; i++ ) {
%   t[ i ] = 0;
% }
% \end{lstlisting}

% -----------------------------------------------------------------------------

\chapter{Evaluation of the Transformer}
\label{chap:evaluation}

This section will explore some case studies of programs which the tansformer can and cannot transform. It will discuss why certain programs cannot be transformed by the current implementation and determine if possible, how difficult extending the transformer to support these feature would be. It will also compare the Rust implementation of the transformer with the Scala version from \cite{food}.

% TODO talk about testing

\section{Supported Transformations}

\section{Limitations}

\subsection{Nested pattern matching and guards}

\subsection{Macros}

% {\bf A topic-specific chapter} 
% \vspace{1cm} 

% \noindent
% This chapter is intended to evaluate what you did.  The content is highly 
% topic-specific, but for many projects will have flavours of the following:

% \begin{enumerate}
% \item functional  testing, including analysis and explanation of failure 
%       cases,
% \item behavioural testing, often including analysis of any results that 
%       draw some form of conclusion wrt. the aims and objectives,
%       and
% \item evaluation of options and decisions within the project, and/or a
%       comparison with alternatives.
% \end{enumerate}

% \noindent
% This chapter often acts to differentiate project quality: even if the work
% completed is of a high technical quality, critical yet objective evaluation 
% and comparison of the outcomes is crucial.  In essence, the reader wants to
% learn something, so the worst examples amount to simple statements of fact 
% (e.g., ``graph X shows the result is Y''); the best examples are analytical 
% and exploratory (e.g., ``graph X shows the result is Y, which means Z; this 
% contradicts [1], which may be because I use a different assumption'').  As 
% such, both positive {\em and} negative outcomes are valid {\em if} presented 
% in a suitable manner.

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

% \noindent
% The concluding chapter of a dissertation is often underutilised because it 
% is too often left too close to the deadline: it is important to allocate
% enough attention to it.  Ideally, the chapter will consist of three parts:

% \begin{enumerate}
% \item (Re)summarise the main contributions and achievements, in essence
%       summing up the content.
% \item Clearly state the current project status (e.g., ``X is working, Y 
%       is not'') and evaluate what has been achieved with respect to the 
%       initial aims and objectives (e.g., ``I completed aim X outlined 
%       previously, the evidence for this is within Chapter Y'').  There 
%       is no problem including aims which were not completed, but it is 
%       important to evaluate and/or justify why this is the case.
% \item Outline any open problems or future plans.  Rather than treat this
%       only as an exercise in what you {\em could} have done given more 
%       time, try to focus on any unexplored options or interesting outcomes
%       (e.g., ``my experiment for X gave counter-intuitive results, this 
%       could be because Y and would form an interesting area for further 
%       study'' or ``users found feature Z of my software difficult to use,
%       which is obvious in hindsight but not during at design stage; to 
%       resolve this, I could clearly apply the technique of Smith [7]'').
% \end{enumerate}

% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a databased) then inported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.
%
% We would recommend using BiBTeX, since it guarantees a consistent referencing style 
% and since many sites (such as dblp) provide references in BiBTeX format. 
% However, note that by default, BiBTeX will ignore capital letters in article titles 
% to ensure consistency of style. This can lead to e.g. "NP-completeness" becoming
% "np-completeness". To avoid this, make sure any capital letters you want to preserve
% are enclosed in braces in the .bib, e.g. "{NP}-completeness".

\backmatter

\bibliography{dissertation}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendicies; these are 
% the same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

% \chapter{An Example Appendix}
% \label{appx:example}

% Content which is not central to, but may enhance the dissertation can be 
% included in one or more appendices; examples include, but are not limited
% to

% \begin{itemize}
% \item lengthy mathematical proofs, numerical or graphical results which 
%       are summarised in the main body,
% \item sample or example calculations, 
%       and
% \item results of user studies or questionnaires.
% \end{itemize}

% \noindent
% Note that in line with most research conferences, the marking panel is not
% obliged to read such appendices. The point of including them is to serve as
% an additional reference if and only if the marker needs it in order to check
% something in the main text. For example, the marker might check a program listing 
% in an appendix if they think the description in the main dissertation is ambiguous.

% =============================================================================

\end{document}
